% !TEX root = D41-report.tex


\section{Task 4.1: Parallelization of structural learning}\label{sec:parallel}

Task 4.1 is devoted to the development of parallel algorithms for structural learning of Bayesian networks.
The task covers \emph{structure restricted models} as well as \emph{classic constraint-based methods}.

Structure restricted models are sub-classes of Bayesian networks where only some particular structures
are allowed. Typically, such kind of models are employed in specific tasks like \emph{classification and regression}
where one is interested in predicting the value of a target variable rather than in accurately modeling the dependencies
among the variables in the model. A classification model contains a set of variables $\{X_1,\ldots,X_n,C\}$
where $C$ is the \emph{class variable} and $X_1,\ldots,X_n$ are called \emph{features}. A Bayesian network can be used
for classification purposes by modeling the distribution $p(X_1,\ldots,X_n,C)$. Then, an item with observed
feature values $x_1,\ldots,x_n$ is classified as belonging to class $c^*$ given by

\begin{equation}
\label{eq:class}
c^* = \arg\max_{c\in\Omega_C} p(c|x_1,\ldots,x_n) = \frac{p(c,x_1,\ldots,x_n)}{p(x_1,\ldots,x_n)} ,
\end{equation}
where $\Omega_C$ denotes the set of possible classes (i.e. the state space of variable $C$).

By restricting the possible structures for representing $p(X_1,\ldots,X_n,C)$, it is possible to avoid the exponential
growth in the number of parameters to learn from data with respect to the number of variables. The extreme case
is when all the features are assumed to be independent given the class, which minimizes the number of parameters
to learn, as the joint distribution factorizes as 
\[
p(X_1,\ldots,X_n,C) = p(C) \prod_{i=1}^nP(X_i|C) .
\]
A classifier constructed in this way is called a naive Bayes (NB) classifier, and it corresponds to a Bayesian network
structure as depicted in Fig.~\ref{fig:NB}.

An improvement on the NB is the so-called \emph{Tree Augmented Naive Bayes} (TAN) classifier \cite{Fri97}. In a TAN, the
features are arranged as a tree, and all the feature variables have the class as a parent. Figure~\ref{fig:TAN}
shows a TAN structure with five feature variables, $X_1,\ldots,X_5$.

\begin{figure}[htb]
  \begin{center}
   \scalebox{0.8}{
    \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,
      node distance=2cm,semithick,
      every state/.style={fill=none,circle,draw}]
      \node[state] (X1) {$X_1$};
      \node[state] (X2) [right of=X1] {$X_2$};
      \node[state] (X3) [right of=X2] {$X_3$};
      \node[state] (X4) [right of=X3] {$X_4$};
      \node[state] (X5) [right of=X4] {$X_5$};
      \node[state] (C) [above of=X3] {$C$};  
      
      \draw (C) to (X1);  
      \draw (C) to (X2);  
      \draw (C) to (X3);  
      \draw (C) to (X4);  
      \draw (C) to (X5);  
      \end{tikzpicture}
    }
  \end{center}
  \caption{A Naive Bayes structure}
  \label{fig:NB}
\end{figure}


\begin{figure}[htb]
  \begin{center}
   \scalebox{0.8}{
    \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,
      node distance=2cm,semithick,
      every state/.style={fill=none,circle,draw}]
      \node[state] (X1) {$X_1$};
      \node[state] (X2) [right of=X1] {$X_2$};
      \node[state] (X3) [right of=X2] {$X_3$};
      \node[state] (X4) [right of=X3] {$X_4$};
      \node[state] (X5) [right of=X4] {$X_5$};
      \node[state] (C) [above of=X3] {$C$};  
      
      \draw (C) to (X1);  
      \draw (C) to (X2);  
      \draw (C) to (X3);  
      \draw (C) to (X4);  
      \draw (C) to (X5);  
      \draw (X2) to (X1);
      \draw (X2) to (X3);
      \draw (X3) to (X4);
      \draw (X4) to (X5);
      \end{tikzpicture}
    }
  \end{center}
  \caption{A TAN structure. Unlike in Fig.~\ref{fig:NB}, the features conform a tree rooted at $X_2$}
  \label{fig:TAN}
\end{figure}

The structure of a TAN model is obtained by computing a \emph{score} for each pair of features, namely
their \emph{conditional mutual information} given the class. Then, a maximum spanning tree of the features is constructed,
labeling the edges using the computed scores. 

Structural learning of TAN models has been approached in task 4.1 using two different methods.
One of them uses \emph{threads} to distribute the workload onto a number of cores. This method assumes that all data is 
available in main memory. The algorithm creates a number of threads that iterate through the scores to be computed. 
Each score is assigned to a thread and a thread only computes the scores assigned to it.
The software developed under this approach corresponds to task 5.1 of WP5, and is included in the AMIDST toolbox 
inside HUGIN AMIDST (see Deliverable D5.1). It can be accessed from the open source AMIDST toolbox using the 
open source AMIDST $\leftrightarrow$ HUGIN AMIDST interface, called HUGIN-Link, which is a functionality of the open
source AMIDST toolbox that  enables to use the HUGIN AMIDST API.

The other method uses \emph{MPI} \cite{For93} to distribute the workload onto a number of processors. This method 
does not assume that all data is available in main memory. The variables are distributed between the processors and each 
process only reads the data for variables assigned to it. The computation of scores is controlled using Balanced-Incomplete 
Block Designs \cite{Sti03} as described in \cite{Mad14}. This approach assumes that data on each variable is stored in a 
separate file. This method is not available through the HUGIN-Link interface of the open source AMIDST toolbox.
However, it can be accessed from it using file exchange, as the open source AMIDST toolbox is able to read and
write HUGIN objects and files, and hence the structure yielded by HUGIN AMIDST can be transferred to software developed
using the open source toolbox.

Constraint-based structural learning encompasses those algorithms for inducing unrestricted Bayesian network
structures according to the result of a series of conditional independence tests. A brute-force constraint based
algorithm could start off with a complete network (where each variable is linked to all the others) and remove edges
between variables for which a statistical test accepts the independence hypothesis. Perhaps the most popular
constraint-based structural learning algorithm is the so called PC \cite{Spi00}. Basically, the PC algorithm
makes use of the same scores as the TAN, i.e. the conditional mutual information scores, which distribution
is known to be of class $\chi^2$ under the hypothesis of independence. It allows the construction of a statistical
test for deciding about the independence relationships between the variables in the network at any given significance level.

Parallelization of the PC algorithm is still under development and the software will be implemented as part of task 5.1
of WP5. The functionality developed for implementing the parallel learning of TAN models will be used as a part of
the parallel PC implementation. It will be available in the AMIDST toolbox as a part of HUGIN AMIDST and its access from 
the open source AMIDST will be effectively done in a similar way as described for the parallel TAN.

%This section will give updates from Task 4.1, with focus on the actual tool-box implementation of parallel PC. We have two sources of information/ideas how to proceed: 
%\bit
%\item The slideset/poster we discussed some time ago.
%\item Hugin people are investigating lines for parallelizing the PC using  multi-thread and relying on the TAN-PGM paper.
%\eit
%
%We should also discuss the learning of TAN classifiers (the PGM paper). Anders has suggested to Antonio that we could include the PGM paper here (except maybe the experiments, that could be reported in D5.1).
%In this way there would be no rush for having it implemented in the toolbox.
%
%The setup of this section is that we show in practice how the initial procedures described in the design section is to be used.