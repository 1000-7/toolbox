% !TEX root = D41-report.tex
\section{Introduction}

%\comment{This is just the first  page from D3.1, then quickly terminated after learning is introduced. Just to say what a BN is.}

This report describes the progress on software development related to WP4, which is concerned with
learning AMIDST models from data. The AMIDST  model class (see Deliverable D2.1 \cite{AMIDST-D21}) is based on 
probabilistic graphical models (PGMs), which provide a well-founded and principled approach for performing inference 
in complex domains endowed with uncertainty. A PGM is a framework consisting 
of two parts: a qualitative component in the form of a graphical model encoding conditional independence 
assertions about the domain being modelled as well as a quantitative component consisting of a collection of local 
probability distributions adhering to the independence properties specified in the graphical model. Collectively, the 
two components provide a compact representation of the joint probability distribution over the domain being modelled. 

In order to put in context the progress on the software development, we summarize the main concepts involved in
learning AMIDST models from data. 
Bayesian networks (BNs) \cite{Pearl88} are a particular type of
PGM that has enjoyed widespread attention in
the last two decades. Figure~\ref{fig:sampleBN} shows a BN representing
the joint distribution of variables $X_1,\ldots,X_5$. Attached to each node, there is a conditional probability
distribution given its parents in the network, so that the joint distribution factorises as

\[
p(X_1,\ldots,X_5) = p(X_1) p(X_2|X_1) p(X_3|X_1) p(X_4|X_2,X_3) p(X_5|X_3).
\]

In general, for a BN with $n$ variables $\bX=\{X_1,\ldots,X_n\}$, the joint distribution factorises as

\begin{equation}
\label{equ:factorisation}
p(\bX) = \prod_{i=1}^n p(X_i|\pa{X_i}) ,
\end{equation}
where $\pa{X_i}$ denotes the set of parents of $X_i$ in the network. 


\newcommand{\simpleModel}{    
      \node[obs] (X1) {$X_1$};
      \node[obs] (X2) [below left of=X1, xshift=-1.2cm, yshift=-1.2cm] {$X_2$};
      \node[obs] (X3) [below right of=X1, xshift=+1.2cm, yshift=-1.2cm] {$X_3$};
      \node[obs] (X4) [below right of=X2, xshift=+1.2cm, yshift=-1.2cm] {$X_4$};
      \node[obs] (X5) [below right of=X3, xshift=+1.2cm, yshift=-1.2cm] {$X_5$};
      \edge{X1}{X2};
      \edge{X1}{X3};
      \edge{X2,X3}{X4};
      \edge{X3}{X5};
    }
    
\begin{figure}[htb]
  \begin{center}
   \scalebox{1}{
    \begin{tikzpicture}

    \simpleModel
    \end{tikzpicture}
    }
  \end{center}
  \caption{A Bayesian network with five variables.}
  \label{fig:sampleBN}
\end{figure}


We will use lowercase letters to refer to
values or configurations of values, so that $x$ denotes a value of $X$ and $\bx$ is a configuration of the
variables in $\bX$. 
Given a set of observed variables $\bX_E\subset \bX$ and a set of variables of interest $\bX_I\subset \bX \setminus \bX_E$,
\emph{probabilistic inference} is the calculation  of the posterior distribution
$p(x_i|\bx_E)$ for each $i\in I$. 
A thorough introduction to the state of the art for inference in BNs was given in Deliverable D3.1 \cite{AMIDST-D31}. 

In this document we assume that inference techniques for a given BN are available (inference will be treated in
WP3 and partly in WP5), and will consider how to 
\textit{learn} a BN from data, i.e. how to define a BN model that fits a data set as well as possible.



