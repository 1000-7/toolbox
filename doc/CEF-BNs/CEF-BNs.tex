\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeY format
\usepackage{geometry}                		% See geom\dagetry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   		% ... or a4paper or a5paper or ... 
%\geometrx{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an emptx line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatey; use eps in DVI ye
\usepackage{array}							% TeY will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsmath}
\usepackage{cite}
\usepackage[final]{fixme}
\usepackage{pdfpages}
\usepackage{tabulary}
\usepackage{fancyheadings}
\usepackage{lastpage}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage[titletoc,title]{appendix}


\parskip 6pt % 1pt = 0.351 mm
\parindent 0pt

%\title{Requirement Engineering Process in AMIDST}
%\author{The handsome AMIDST guys et. al.}
%\date{Latest version, \today}							% Activate to display a given date or no date


%\setcounter{page}{2}
\newcommand{\drop}[1]{}
\newcommand{\bm}{\mathbf}

\newcommand{\bu}[1]{\mathbf{#1}}
\newcommand{\bv}[1]{\bm{#1}}

\newcommand{\todo}[1]{{\bf [TODO: #1]}}

\DeclareMathOperator*{\E}{\mbox{\large E}}

\newcommand{\me}{\mathrm{e}}

\numberwithin{figure}{section}
\numberwithin{equation}{section}
\numberwithin{table}{section}

\newcommand{\e}[1]{E\left[ #1 \right]}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage{pdfpages}

\begin{document}
\title{ Representation, Inference and Learning of Bayesian Networks as Conjugate Exponential Family Models }

\maketitle
\begin{abstract}

\end{abstract}


\section{Introduction}

Defining the data structure of a Bayesian network is not a straightforward problem. The definition of the data structure of a DAG is not complex when compared to the definition of the data structure of the conditional probability distributions encoded in the BN. The DAG is an \textit{homogeneous} data structure, in the sense that it is only composed by nodes and directed edges. However, the set of different conditional distributions is not limited at all. For example, the data structure for representing a Multinomial distribution is, in a first look, quite different from the data structure needed to represent a Normal distribution. In the former case, we need to store the probability of each one of the cases of the multinomial variable, while in the latter case we need to store, for example, the mean and the variance of the Normal distribution. If we consider a Poisson, an Exponential, or a MoTBF, etc, the data structures needed to represent these distributions are completely different. 

But these are only examples of unidimensional distributions. When defining the data structure of the conditional distributions things become much more complex. For example, the data structure for representing the conditional distribution of a Normal distribution given a set of normally distributed variables is, in a first look, totally different from the data structure needed to represent a conditional distribution of a Multinomial variable given a set of Multinomial variables. In the former case, under the conditional linear Gaussian framework, we need to store the coefficients for the linear combination of the parents variables plus the variance of the main variable. While the data structure for the multinomial given multinomial case is usually defined using a big probability table. If we want to allow the combination of Normal, Multinomial, Poison, Exponential, etc, in the same framework the number of data structures needed to represent all the possible conditional distributions combinations quickly explode.

The problem of making inferences and learn from data with BNs with different kinds of conditional distributions becomes also quite challenging. For example, the maximum likelihood of a Normal distribution is obtained by computing the sample mean and variance, while the maximum likelihood of a Multinomial distribution is obtained by normalizing the sample \textit{counts} of each one of the sates. Alternative methods are required for the different possible conditional probabilities, which means that the addition of new family of variables, i.e. Normal, Poission, or Exponential, etc, implies to define, code and test from scratch new maximum likelihood methods.  

In the case of inference, things are even worse. For example, the combination and marginalization operations over probability potentials belonging to different distribution families is in general non-closed and, in principle, involves quite different approaches. I.e., the combination or multiplication of two multinomial potentials or distributions involve completely different methods than the combination or product of two Normal distributions. And similarly for the marginalization operation. So, defining and coding all of these operations for different family of distributions can become a daunting task. 

In this technical report, we show that if we restrict ourselves to the so-called conjugate exponential family models, we can avoid most of the above problems by using previously known results and algorithms. Firstly, all the conditional probability distributions inside this family can be represented using the same data structure, which is simply composed by two n-dimensional vectors (the so-called natural and moment parameters) and two n-dimensional functions (the so-called sufficient statistics and log-normalizer functions). Moreover, we describe previously proposed learning and inference algorithms than can be directly implemented on top of this general and unique representation. The result is a suitable framework for coding a toolbox which aims to deal with the problem of representing, making inference and learning general Bayesian networks from data.


\section{Background and notation}

\subsection*{Bayesian networks}
Let $\bm X = \{X_1,\ldots,X_N\}$ denote the set of stochastic random variables defining our domain problem and $\bm x$ an observation vector. A Bayesian network defines a joint distribution $P(\bm X)$ in the following form:

$$ p(\bm X) = \prod_{i=1}^N p(X_i|Pa(X_i))$$ 

\noindent where $Pa(X_i)\subset \bm X\setminus X_i$ represents the so-called \emph{parent variables} of $X_i$. Bayesian networks can be graphically represented by a directed acyclic graph (DAG). Each node, labelled $X_i$ in the graph, is associated with a factor or conditional probability $p(X_i|Pa(X_i))$. Additionally, for each parent $X_j \in Pa(X_i)$, the graph contains one directed edge pointing from $X_j$ to the \emph{child} variable $X_i$.

\subsection*{Exponential family models}

A Bayesian network defines a joint probability in the exponential family with a natural (or canonical) parametrization if the joint distribution can be functionally expressed as follows, 
\begin{equation}
\label{Equation:EFCanonical}
p(\bm x |\theta) = h(x) exp(\theta^Ts(\bm x) -
A(\theta) )
\end{equation}
\noindent where $\theta$ is the so-called natural parameter, which belongs to the so-called \emph{natural parameter space}
$\Theta \equiv \{ \theta \in\Re^K: \int_{\bm x} h(\bm x) exp(\theta^T s(\bm x) -
A(\theta) ) d\bm x < \infty \}$, $s(\bm x)$ is the vector of sufficient statistics belonging to $\mathcal{S} \subseteq\Re^K$, $A$ is the log partition function and $h(x)$ is the base measure. 

The so-called \emph{expectation or moment parameters} $\mu\in\mathcal{S}$ can also be used to parametrize probability
distributions of the exponential family. This \emph{expectation parameter} $\mu$ is defined as the expected vector of sufficient statistics with respect to $\theta$:

\begin{equation}
\label{Equation:NaturalToMoment}
\begin{array}{lll}
\bm \mu & \triangleq & \e{s(\bm x)|\theta}  = \int s(\bm x)p(\bm x|\theta)
d\bm x\\
\end{array}
\end{equation}

The \emph{natural parameter} $\theta$ associated to
an \emph{expectation parameter} $\mu$ is obtained by solving the following optimization 
problem.
 
\begin{eqnarray}
\label{Equation:MomentToNatural}
\theta(\mu) = arg\max_{\theta\in\Theta} \theta^T\mu
-A(\theta)
\end{eqnarray}

Given the natural parameters, the inverse step of updating the moment parameters is not trivial in the case of conditional distributions, since as we will see in Section \ref{sec:CondDist} the transformation requires the joint probability distribution of the children and the parents. 

The importance of being able to convert from one space of parameters to the other takes special relevance in the implementation of the variational message passing algorithm, where message passing and updates are carried out in the two different spaces. Another less explored alternative is the use of \textit{root} parameters, in order to prevent possible numerical imprecision. More information on this topic can be found in the following technical report \cite{HowJeb05}.

\subsection*{Regular Exponential Family}

A Bayesian network belongs to the linear exponential family if the natural parameter space $\Theta$ is an open and convex set. Additionally, a linear exponential family is said to be minimal if there is non-zero constant vector $\alpha$, such that $\alpha^Ts(\bm x)$ is equal to a constant for all $\bm x$.  We will consider the \textit{regular exponential family} to be the linear exponential one with minimal representation.


<<<<<<< HEAD
The regular exponential family has been widely studied in the literature. The distributions in this family enjoy many useful properties. The following two properties are some of the most relevant ones: i)  The transformation between $\theta$ and $\mu$ parameters is a one-to-one correspondence, i.e. $\mu$ is a dual set of the model parameter $\theta$; and ii) the moment parameters equal the gradient of the log-normalizer (the proof of this equality is (will be!) included in the appendix), 
=======
The regular exponential family with a minimal representation has been widely studied in the literature. The distributions in this family enjoy many useful properties. The following two properties are some of the most relevant ones: i)  The transformation between $\theta$ and $\mu$ parameters is a one-to-one correspondence, i.e. $\mu$ is a dual set of the model parameter $\theta$; and ii) the moment parameters equal the gradient of the log-normalizer (the proof of this equality is included in Appendix \ref{appendix:regularEFequality}), 
>>>>>>> b99102880c38141c272982e23df593c9288312cc

\begin{equation}
\label{Equation:RegularEFEquality}
\mu = \frac{\partial A(\theta)}{\partial \theta}
\end{equation}


However any BN which contains immoralities does not induce a linear exponential family (some parameters are restricted to (non)-linear constraints to ensure the conditional independence). So, some of the nice properties of the regular family are lost. In that case, we need to rely on a more general family called the \textit{curved exponential family}. In this case, the probability distribution can be more generally expressed as follows, 

\begin{equation*}
p(\bm x |\theta) = h(x) exp(\psi(\theta)^Ts(\bm x) -
A(\theta) )
\end{equation*}

\noindent where $\psi$ is a parameter transformation function. In our case, we restrict to cases where $\psi$ is invertible, so we can re-parametrize the distribution in a canonical form as in Equation \ref{Equation:EFCanonical}, where the natural parameter space is not any more an open convex set.


\begin{exmp}

A Bernoulli distribution can be represented as a curved exponential family as follows, 


$$
\ln p(x| \rho ) = 
\begin{pmatrix}
\ln \rho\\
\ln (1-\rho)
\end{pmatrix}^T
\begin{pmatrix}
I(x=0)\\
I(x=1)
\end{pmatrix}
$$

\noindent where the $\psi$ function is defined as follows, $\psi(\rho) = (\ln \rho, \ln 1-\rho)$. 

The above distribution can be reparametrized in cannonical form as follows, 

$$
\ln p(x| \theta ) = 
\begin{pmatrix}
\theta_1\\
\theta_2
\end{pmatrix}^T
\begin{pmatrix}
I(x=0)\\
I(x=1)
\end{pmatrix}
$$

\noindent where $\theta\in\Theta = \{ (\theta_1, \theta_2) \in \Re^2 : e^{\theta_1} + e^{\theta_2} = 1\}$.  As can be seen, $\Theta$ is not an open convex set in $\Re^2$. 

\end{exmp}

\subsection*{Conditional distributions as exponential family models} \label{sec:CondDist}

A conditional distribution $p(X|Pa(X))$ is in the exponential family if it can be written in the following functional form, 

\begin{equation}
p(x | Pa(X)) = h(x) exp( \theta(Pa(X))^Ts(x) -
A(\theta(Pa(X))) ) 
\end{equation}

\noindent where $\theta(Pa(X))$ denotes that the natural parameters are now a function of the parent variables of $X$.  %Equivalently, we can say that $p(X|Pa(X))$ is in the exponential family if for any assignment, $\bm \pi$  to the parents variables, $p(X|\bm\pi)$ is in exponential form. 

A conditional distribution $p(X|Y)$ is said to be \textit{conjugate} to a child distribution $p(W|X)$ if $p(X|Y)$ has the same functional form, with respect to $X$, as $p(W|X)$. An important property of the exponential conjugate conditional distributions is that they can also be expressed in the following functional form,
 
\begin{equation}
\label{Equation:EqCED}
p(x | \bm y) = h(x) exp(\theta^Ts(x,\bm y) -
B(\theta) ) 
\end{equation}
\noindent where $B(\theta)$ is the conditional log-normalizer. 

A Bayesian network is said to be a conjugate exponential (CEF) model if all its conditional distributions belong to the exponential family and are conjugate. Importantly enough, the use of conjugate distributions allows that the posterior for each distribution has the same form as the prior, which means that only the values of the parameters change, but not the functional form of the distribution.


\section{Bayesian networks as CEF models}
\label{Section:CEFBN}

\subsection{Representation}
\label{Section:CEFBN:Representation}

In this section we show why conjugate-exponential family Bayesian networks (CEF-BNs) are quite amenable to be represented (and coded) as exponential family models. 

By using Equation \ref{Equation:EqCED}, a conjugate-exponential BN can be represented in the following way, 

\begin{eqnarray}
\label{Equation:CEFSS}
\ln p(X_1,\ldots, X_n) &=& \sum_{i=1}^n \ln p(X_i|Pa(X_i))\nonumber\\
&=& \sum_{i=1}^n \theta_i(Pa(X_i))^T s_i(X_i) - B(\theta(Pa(X_i)))\nonumber\\
&=& \sum_{i=1}^n \theta_i^T s_i(X_i, Pa(X_i)) - B(\theta_i)\nonumber\\
&=&
\begin{pmatrix}
\theta_1\\
\ldots \\
\theta_n\\
\end{pmatrix}^T
\begin{pmatrix}
s_1(X_1,Pa(X_1)) \\
\ldots \\
s_n(X_n,Pa(X_n)) \\
\end{pmatrix}
- \sum_{i=1}^n B_i(\theta_i)
\end{eqnarray}

The above expression show us that we can represent a CEF-BN by using the local representations of the conditional distributions:

\begin{itemize}
\item The natural parameters are formed by the composition of the local natural parameters of each conditional distribution. 
\item The sufficient statistics are formed the composition of the local sufficient statistics of each conditional distribution. 
\item The log-normalizer is the sum of the local conditional log-normalizer of each conditional distribution. 
\end{itemize}

So, in order to represent a BN as an exponential family model we only have to worry about the local representation of each conditional distribution as in Equation \ref{Equation:EqCED}. The global representation is just obtained by composing these local representations. 

Let us notice that without the assumption of conjugacy  for the conditional exponential distributions, the above representation would have not been possible. 

\subsection{From Natural to Moment Parameters}
\label{Section:CEFBN:NaturalToMoment}

We now look at the transformation from to natural to moment parameters in a CEF-BN. Following Equation \ref{Equation:NaturalToMoment}., we have that the vector of moment parameters in a CEF-BN model decomposes as follows,

\begin{equation}
\begin{array}{lll}
\mu & = & \e{s(X_1,\ldots, X_n)|\theta} \\
&=& \int s(X_1,\ldots, X_n)p(X_1,\ldots, X_n|\theta) 
d\bm X = \\
&=& (\mu_1,\ldots,\mu_n) \\
\end{array}
\end{equation}

\noindent where $\mu_i= \int s(X_i,Pa(X_i)) p(X_i,Pa(X_i)) d\bm X$. I.e. the moments parameter of a CEF-BN locally decomposes in moment parameters associated to local marginal probabilities. Let us note that to compute the local marginal probability  $p(X_i,Pa(X_i))$ we need to perform inference over the whole BN.

\subsection{From Moment to Natural Parameters}
\label{Section:CEFBN:MomentToNatural}


The transformation from moment to natural parameters also decomposes. Let us start by expanding Equation \ref{Equation:MomentToNatural}, 

\begin{eqnarray}
\label{Equation:CEFBN_MomentToNatural}
\theta(\mu) &=& \arg\max_{\theta\in\Theta} \theta^T\mu
-A(\theta)\nonumber \\
&=& \arg\max_{(\theta_1,\ldots, \theta_n) \in\Theta} \sum_{i=1}^n \theta_i^T \mu_i - B(\theta_i) 
\end{eqnarray}

Because the $\theta_i$ parameters are independent, the above maximization problem fully decomposes in a local maximization problem for each conditional probability distribution,  

\begin{eqnarray}
\label{Equation:CEFBN_MomentToNaturalLocal}
\theta_i(\mu_i) = \arg\max_{\theta_i\in\Theta_i} \theta_i^T\mu_i - B(\theta_i)
\end{eqnarray}

\noindent and the global solution is just the composition of the local solutions, 

$$ \theta(\mu) = (\theta_1(\mu_1), \ldots, \theta_n(\mu_n))$$

Let us note that, as oppose to the previous case, the transformation from moment to natural parameters can be performed locally at each conditional distribution. The global solution is just an aggregation of the local solutions.

\section{Conditional distributions with multinomial parents in exponential form}
\label{Section:CD_With_MParents}

In this subsection we try to exploit the commonalities in terms of structure that are present in many conditional distributions in order to ease its representation in exponential form and, also, the associated parameter transformations.  The structure we try to exploit is the presence of multinomial variables in the parent set of a conditional distribution. The advantage of this representation is that if we know how to represent in a exponential form some distribution (ie. Poison distribution, a Normal distribution, a Multinomial distribution, a Normal distribution with Normal parents, etc), we can directly derive the corresponding distribution conditioned to multinomial parents (i.e. Poison given Multinomial parents , Normal given Multinomial parents, Multinomial given Multinomial parents, Normal given Normal and Multinomial parents, etc). Similarly, we  also show that for these conditional distributions the transformation from moment to natural parameters can be further decomposed. 


\subsection{Representation}
\label{Section:CD_With_MParents:Representation}

Let $(\bm Z, \bm Y)$ be the set of parents of a variable $X$, where $\mathbf{Y}$ is a set of multinomial variables and $\bm Z$ a set of non-multinomial variables\footnote{$\bm Z$ can be empty.}.  Let $q$ denote the total number of parental configurations for the variables in $\bm Y$, and let $\bm y$ denote the $l$-th parental configuration $1 \leq l \leq q$.

The log-conditional probability of $X$ given its parent-nodes $\bm Z$ and $\mathbf{Y}$ decomposes as  follows:

\begin{eqnarray*}
\ln p(X \mid \bm Z, \bm Y) &=&  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p(X | \bm Z, \mathbf{y}^l) \\
&=& \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{l}^T s_l(X, \bm Z)  -  B_l(\theta_{l}) \Big)\\
&=& \sum_{l=1}^q \theta_{l}^T  I(\mathbf{Y} =\mathbf{y}^l) s(X, \bm Z) - \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) B_l(\theta_{l})
\end{eqnarray*}

\noindent where $\theta_l$, $s_l(X,\bm Z)$ and $B_l(\theta_l)$ are provided when the local conditional distribution $p(X | \bm Z, \mathbf{y}^l)$ is expressed in exponential form. 

So, the conditional distribution $p(X \mid \bm Z, \bm Y)$ can be written in exponential form as follows, 

\begin{eqnarray}
\label{Equation:CD_With_MParents:Representation}
\ln p(X \mid \bm Z, \bm Y)  &=& \theta^T s(X,\mathbf{Y}) - B(\theta) \nonumber \\
&=&
\begin{pmatrix}
- B_1(\theta_{1}) \\
\vdots \\
- B_q(\theta_{q}) \\
\theta_{1} \\
\vdots \\
\theta_{q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s_1(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s_q(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray}

The corresponding proof can be found in Appendix \ref{appendix:CD_With_MParents:Representation}.

As can be seen, the exponential representation of a conditional probability with multinomial parents can be expressed as the composition of the exponential representation of the conditional distributions restricted to each one of the possible configurations of the multinomial parents. 

\subsection{From Natural to Moment Parameters}
\label{Section:CD_With_MParents:NaturalToMoment}


By using the decomposition of the sufficient statistics vector of this conditional probability distribution, the moment parameter vector associated to a natural parameter vector $\theta$ can be expressed as a combination of the moment parameters of the marginal probabilities $p(\bm Y|\theta)$ and $p(X, \bm Z)$ as follows:

$$ 
\begin{pmatrix}
\mu^{I}_1  \\
\vdots \\
\mu^{I}_q \\
\mu^{I}_1 \mu^{local}_1\\
\vdots \\
\mu^{I}_q \mu^{local}_q\\
\end{pmatrix}
$$

\noindent where  $\mu^{I}_l$ is the $l$-th component (i.e. a scalar) of the moment vector associated to the marginal distribution $p(\bm Y|\theta)$,  $\mu^{I}_l  = \int I(\bm Y = \bm y_l) p(\bm Y|\theta) d\bm Y= p(\bm y_l|\theta)$, and $\mu^{local}_l$  is the ``local'' moment parameter associated to the marginal distribution $p(X,\bm Z| \bm y = l)$, $\mu_l = \int s_l(X, \bm Z)p(X,\bm Z|\bm y = l, \theta) dX\bm Z$. 

Again, we can see that the moment parameters of this conditional distribution can be expressed as a composition of the local moment parameters of each of the distributions conditioned to each configuration of the multinomial parents variables. 

\subsection{From Moment to Natural Parameters}
\label{Section:CD_With_MParents:MomentToNatural}

We now look at how to transform from moment to natural parameters. Initially, the dimension of the optimization problem is $2q$ (i.e the dimension of the natural parameter vector), however we can exploit the structure present in the natural space and see that it boils down to a $q$ dimensional problem, 

\begin{eqnarray*}
\arg\max_{(\theta_1,\ldots, \theta_q) \in \Theta} \sum_{l=1}^q \theta_l^T \mu_{q+l}- \mu_l B_l(\theta_l)
\end{eqnarray*}

Furthermore, the above optimization problem decomposes in a set of $q$ independent optimization problems, 

\begin{eqnarray*}
\arg\max_{\theta_l \in \Theta_l} \theta_l^T \mu_{q+l}- \mu_l B_l(\theta_l)
\end{eqnarray*}

The solution of the above optimization problem is not affected if the optimized expression is divided by $\mu_l$, which is a scalar\footnote{If $\mu_l=0$ it would imply that $p(\bm Y= \bm y_t)=0$, so it does not make sense to solve the problem.}. So this problem can be transformed as follows, 

\begin{eqnarray}
\label{Equation:CD_With_MParents:MomentToNatural}
\arg\max_{\theta_l \in \Theta_l} \theta_l^T\mu'_l - B_l(\theta_l)
\end{eqnarray}

\noindent where $\mu'_l =\frac{1}{\mu_{l}}\mu_{q+l}$, i.e.the element-wise division of the vector $\mu_{q+l}$ by the scalar $\mu_l$.

The above problem simply corresponds to the transformation of the moment parameters $\mu'_l$ to their corresponding natural parameters $\theta_l$ for the conditional distribution $p(X|\bm Z, \bm y_l)$. So again, we see how the transformation from moment to natural decomposes in a series of local transformations. 


\section{Maximum Likelihood}

In this section we look at the maximum likelihood problem in CEF-BNs. Our aim is to show how the solution to this multi-dimensional optimization problem has a common characterization in terms of exponential family representation and, moreover, boils down to smaller local problems for each conditional distribution\footnote{We point out that the following derivation is made without assuming that our models belongs to the regular exponential family and, in consequence, not using the equality of Equation \ref{Equation:RegularEFEquality}.}.

Let us assume that we are given a set of $m$ i.i.d. data samples $D=\{\bm x^{(1)}, \ldots, \bm x^{(m)}\}$ indexed by $j$. The maximum likelihood problem can be stated as follows:

\begin{eqnarray*}
\theta^\star  &=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \ln p(\bm x^{(j)}|\theta) \\
&=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \theta^Ts(\bm x^{(j)})  - A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\sum_{j=1}^m s(\bm x^{(j)})\Big)  - m A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\frac{1}{m}\sum_{j=1}^m s(\bm x^{(j)})\Big)  - A(\theta) \\
\end{eqnarray*}

\noindent where the last part is achieved by diving the optimized equation by the number of samples $m$, what does not affect the result of the optimization. 

As widely known, the maximum likelihood is equivalent to a transformation form moment to natural parameters as stated in Equation \ref{Equation:MomentToNatural}, 

$$\theta^\star = \theta\Big(\frac{1}{m}\sum_{j=1}^m s(\bm x^{(j)})\Big)$$

As shown in Section \ref{Section:CEFBN:MomentToNatural}, this problem decomposes for each conditional probability distribution.  The above formation can be expressed in terms of local transformations defined in Equation \ref{Equation:CEFBN_MomentToNaturalLocal}, 

$$\theta_i^\star = \theta_i\Big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i)\Big)$$


To better understand the above decomposition, we should notice that $\theta_i(\mu_i)$ is directly related to maximum likelihood estimation of the conditional distribution $p(X_i|Pa(X_i))$, 

\begin{eqnarray*}
\theta^\star  &=& \arg\max_{\theta_i \in \Theta_i} \sum_{j=1}^m \ln p(x_i^{(j)}|\bm{pa}^{(j)}_i,\theta) \\
&=& \arg\max_{\theta_i \in \Theta_i} \sum_{j=1}^m \Big(\theta_i^Ts(x_i^{(j)},\bm{pa}^{(j)}_i)  - B_i(\theta_i) \Big)\\
&=& \arg\max_{\theta_i \in \Theta_i} \theta_i^T\Big(\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i) \Big)  - m B_i(\theta) \\
&=& \arg\max_{\theta_i \in \Theta_i} \theta_i^T\Big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i) \Big)  - B_i(\theta) \\
\end{eqnarray*}

\noindent where $\bm{pa}^{(j)}_i$ corresponds to the assignment to the parents of $X_i$ according the to the $j$-th data sample $\bm x^{(j)}$.

For those conditional distributions with multinomial parents, the problem further decomposes as shown in Section \ref{Section:CD_With_MParents:MomentToNatural}. 


%
%The maximum likelihood computation can be addressed using the following steps:
%
%\begin{enumerate}
%
%\item  Compute the sufficient statistics of a given data sample $\bm x^{(j)}$, which is the composition of the sufficient statistics of each conditional distribution (see Equation \ref{Equation:CEFSS}). If the conditional distribution has multinomial parents then we can use Equation \ref{Equation:CD_With_MParents:Representation} to compose the sufficient statistics of the conditional distribution. 
%
%\item Sum all the sufficient statistics and divide the sum vector by the total number of samples. 
%
%\item Obtain the natural parameters associated to this normalized sufficient statistics. According to Equation \ref{Equation:CEFBN_MomentToNatural}, this natural parameters can be obtain by aggregating the the natural parameters of the conditional distributions, which can be computed locally. If a conditional distribution has multinomial parents then we can use Equation \ref{Equation:CD_With_MParents:MomentToNatural} to compose the natural parameter of this conditional distribution. 
%\end{enumerate}


\section{EM algorithms in CEF-BNs}

\section{Variational Inference in CEF-BNs}

\section{Expectation  Propagation Inference in CEF-BNs}

\begin{appendices}


\section{Regular EF: moment parameters equal the gradient of the log-normalizer}\label{appendix:regularEFequality}
%\appendix
%\vspace{20mm}
%\begin{center}
%{\LARGE \textbf{APPENDIX}}
%\end{center}

\section{Conditional distributions with multinomial parents: proof of EF representation}\label{appendix:CD_With_MParents:Representation}

 %-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A binary child given a binary parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ and $Y$ be two binary variables. The log-conditional probability of the child-node $X$ given its parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(X= x^1) I(Y= y^1) \ln p_{x^1 \mid y^1} + I(X=x^2) I(Y= y^1) \ln p_{x^2 \mid y^1} \\
+ I(X=x^1) I(Y= y^2) \ln p_{x^1 \mid y^2} + I(X=x^2) I(Y= y^2) \ln p_{x^2 \mid y^2}
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:


\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
\ln p_{x^1 \mid y^1}\\
\ln p_{x^2 \mid y^1}\\
\ln p_{x^1 \mid y^2}\\
\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
\theta_{11}\\
\theta_{21}\\
\theta_{12}\\
\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(Y) \\
&=&
\begin{pmatrix}
I(Y=y^1)\ln p_{x^1 \mid y^1}  + I(Y=y^2)\ln p_{x^1 \mid y^2}\\
I(Y=y^1)\ln p_{x^2 \mid y^1}  + I(Y=y^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 \\
&=&
\begin{pmatrix}
m^Y_1\cdot\theta_{11}  + m^Y_2\cdot\theta_{12}\\
m^Y_1\cdot\theta_{21}  + m^Y_2\cdot\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
I(X=x^1)\ln p_{x^1 \mid y^1}  + I(X=x^2)\ln p_{x^2 \mid y^1}\\
I(X=x^1)\ln p_{x^1 \mid y^2}  + I(X=x^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  m^X_2\cdot \theta_{21}\\
m^X_1 \cdot \theta_{12}  + m^X_2 \cdot \theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A multinomial child given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a multinomial variable with $k$ possible values such that $k \geq 2$, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

$$ \ln p(X \mid \mathbf{Y}) = \sum_{j=1}^k \sum_{l=1}^q I(X=x^j) I(\mathbf{Y} =\mathbf{y}^l) \ln p_{x^j  \mid \mathbf{y}^l} $$

Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) \\\\
&=&
\begin{pmatrix}
\ln p_{x^1\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0 \\\\
&=&
\begin{pmatrix}
\theta_{11}\\
\vdots \\
\theta_{1q}\\
\vdots \\
\theta_{k1}\\
\vdots \\
\theta_{kq}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\vspace{0.5in}
\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(\mathbf{Y})^Ts(X) - A(\mathbf{Y}) \\ \\
&=&
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^1\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^k\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}
- 0 \\ \\
&=&
\begin{pmatrix}
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{11}  + m^{\mathbf{Y}}_q \cdot \theta_{1q} \\
\vdots \\
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{k1}  + m^{\mathbf{Y}}_q \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k)
\end{pmatrix}
- 0 
\end{eqnarray*}

\noindent such that $\mathbf{m}^{\mathbf{Y}}_1 = \prod_{i=1}^n I( Y_i = y_i^1) = \prod_{i=1}^n m^{Y_i}_1$ denotes the expected sufficient statistics for the first parental configuration, and $\mathbf{m}^{\mathbf{Y}}_q = \prod_{i=1}^n I( Y_i = y_i^{r_i})  = \prod_{i=1}^n m^{Y_i}_{r_i} $ denotes the expected sufficient statistics for the last parental configuration.

\vspace{0.5in}
\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\ \\
&=&
\begin{pmatrix}
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^1}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^1} \\
\vdots \\
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^q}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0\\ \\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  \ldots + m^X_k \cdot \theta_{k1}\\
\vdots \\
m^X_1 \cdot \theta_{1q}   + \ldots + m^X_k \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&= &
\begin{pmatrix}
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^1\mid \mathbf{y'}^1}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^k\mid \mathbf{y'}^1}  \! \\
\vdots \\
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^{q'})  \ln p_{x^1\mid \mathbf{y'}^{q'}}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^{q'}) \ln p_{x^k\mid \mathbf{y'}^{q'}}\! 
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
\! - 0 \! \\ \\
&=&
\begin{pmatrix}
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{11}  +  \ldots + m^X_k \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{k1}\\
\vdots \\
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{1q'}   + \ldots + m^X_k \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{kq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\noindent where $\mathbf{m}^{\mathbf{Y'}}_1 =  I(\mathbf{Y'} =\mathbf{y'}^1) = I( Y_1 = y_1^1) \cdot \ldots I( Y_{i-1} = y_{i-1}^1) \cdot I( Y_{i+1}  = y_{i+1}^1) \cdot \ldots I( Y_{n}  = y_{n}^1)$ denotes the expected sufficient statistics for the first configuration of the parent set $\mathbf{Y'} = \mathbf{Y} \setminus Y_i$, and $\mathbf{m}^{\mathbf{Y'}}_{q'} = I(\mathbf{Y'} =\mathbf{y'}^{q'}) = I( Y_1 = y_1^{q'}) \cdot \ldots I( Y_{i-1} = y_{i-1}^{q'}) \cdot I( Y_{i+1}  = y_{i+1}^{q'}) \cdot \ldots I( Y_{n}  = y_{n}^{q'})$ denotes the expected sufficient statistics for the last configuration of the parent set $\mathbf{Y'}$, with $q' = q / r_i$ denotes the total number of configurations of the parent set $\mathbf{Y'}$.




\end{itemize}


\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A normal child given a set of normal parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a normal variable and $ \mathbf{Y} = \{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are normal. 

The log-conditional probability of $X$ given its parents $\mathbf{Y}$ can be expressed as follows:

$$ \ln p(X|Y_1,\ldots,Y_n) = \ln \left(\frac{1}{\sigma \sqrt{2(\beta_0+\sum_i^n \beta_i Y_i )}} \me^{-\frac{(y-(\beta_0+\sum_i^n \beta_i Y_i))^2}{2\sigma^2}} \right)$$


Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}
\item \textbf{First form - Joint suff. stat. (Maxim. Likelihood)}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
\frac{-1}{2\sigma^2} &=& \theta_{\mbox{-}1} \\
\frac{\beta_0}{\sigma^2} &=& \theta_0 \\
\frac{\beta_1}{\sigma^2} &=& \theta_1 \\
\vdots\\
\frac{\beta_n}{\sigma^2} &=& \theta_n \\\\
\frac{-\beta_0\beta_1}{2\sigma^2} &=&\theta_{01} \\
\vdots\\
\frac{-\beta_0\beta_n}{2\sigma^2} &=& \theta_{0n} \\
\frac{-\beta_1^2}{2\sigma^2} &=& \theta_{1^2} \\
\vdots \\
\frac{-\beta_n^2}{2\sigma^2} &=& \theta_{n^2} \\
\frac{-\beta_1\beta_2}{2\sigma^2} &=& \theta_{12} \\
\vdots\\
\frac{-\beta_1\beta_n}{2\sigma^2} &=& \theta_{1n} \\
\vdots\\
\frac{-\beta_{n-1}\beta_n}{2\sigma^2} &=&\theta_{n\mbox{-}1n} \\
\end{pmatrix}^T
\begin{pmatrix}
X^2   &=& m_{X^2}\\
X     &=& m_{X}\\
XY_1  &=& m_{XY_1}\\
\vdots\\
XY_n  &=& m_{XY_n}\\
Y_1   &=& m_{Y_1}\\
\vdots\\
Y_n   &=& m_{Y_n}\\
Y_1^2 &=& m_{Y_1^2}\\
\vdots\\
Y_n^2 &=& m_{Y_n^2}\\
Y_1Y_2&=& m_{Y_1Y_2}\\
\vdots\\
Y_1Y_n &=& m_{Y_1Y_n}\\
\vdots\\
Y_{n\mbox{-}1}Y_n &=& m_{Y_{n\mbox{-}1}Y_{n}}
\end{pmatrix}
- \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right) + \frac{1}{\ln{\sqrt{2\mu_{X|Y}}}}
\end{eqnarray*}

where $\mu_{X|Y} = \beta_0+\sum_i^n{\beta_i Y_i}$

\begin{itemize}
\item \textbf{From moment to natural parameters: (matrix representation)}


\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
\beta_0 (\sigma^2)^{-1} &=& \theta_{0} \\
-\beta_0 \beta^T (2\sigma^2)^{-1} &=& \theta_{\beta_0 \beta^T} \\
-(2\sigma^2)^{-1} &=& \theta_{\mbox{-}1} \\
\beta (\sigma^2)^{-1} &=& \theta_{\beta} \\
-\beta' \beta^T (2\sigma^2)^{-1} &=& \theta_{\beta \beta^T} \\
\end{pmatrix}^T
\begin{pmatrix}
X   &=& \E(X)\\
Y   &=& \E(Y)\\
XX^T   &=& \E(XX^T)\\
YX^T   &=& \E(YX^T)\\
YY^T   &=& \E(YY^T)\\
\end{pmatrix}
\\
&-& \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right)  + \frac{1}{\ln{\sqrt{2\mu_{X|Y}}}}
\end{eqnarray*}

where

\begin{tabular}{p{4cm}p{4cm}}
\begin{eqnarray*}
\beta &=& 
\begin{pmatrix}
\beta_1\\
\vdots\\
\beta_n\\
\end{pmatrix}
\end{eqnarray*}
&
\begin{eqnarray*}
Y &=& 
\begin{pmatrix}
Y_1\\
\vdots\\
Y_n\\
\end{pmatrix}
\end{eqnarray*}
\\
\end{tabular}


\begin{itemize}


\item FIRST STEP: 
\begin{eqnarray*}
\mu_X &=& E(X)\\
\mu_\mathbf{Y} &=& \E(Y)\\
\Sigma_{XX} &=&  \E(XX^T) - \E(X)\E(X)^T\\
\Sigma_{\mathbf{YY}} &=&  \E(YY^T) - \E(Y)\E(Y)^T \\
\Sigma_{X\mathbf{Y}} &=&  \E(YX^T)^T - \E(X)\E(Y)^T\\
\Sigma_{\mathbf{Y}X} &=&  \E(YX^T) - \E(Y)\E(X)
\end{eqnarray*}

\item SECOND STEP (Theorem 7.4 in page 253, Koller \& Friedman):
\begin{eqnarray*}
\beta_0 &=& \mu_X - \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}}\mu_\mathbf{Y}\\
\beta   &=& \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}} \\
\sigma^2 &=& \Sigma_{XX} - \Sigma_{XY}\Sigma^{-1}_{YY}\Sigma_{YX}
\end{eqnarray*}

All natural parameters $\theta$ can now be calculated considering these equations.
\end{itemize}

\item \textbf{From natural to moment parameters:}
Via inference.

\end{itemize}

\vspace{0.5in}
\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(\mathbf{Y})^T s(X) - A \big(\theta(\mathbf{Y})\big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
\frac{\mu_{X|Y}}{\sigma^2}\\
\frac{-1}{2\sigma^2}\\
\end{pmatrix}^T
\begin{pmatrix}
X\\
X^2\\
\end{pmatrix}
- \left(\frac{\mu_{X|Y}^2}{2\sigma^2} + \ln{\sigma}\right) + \ln{\frac{1}{\sqrt{2\mu_{X|Y}}}} \\\\
&=&
\begin{pmatrix}
\theta_0+\sum_i^n\theta_i m^{Y_i}\\
\theta_{\mbox{-}1}\\
\end{pmatrix}^T
\begin{pmatrix}
X\\
X^2\\
\end{pmatrix}
- \left(\frac{\ln{(2\theta_{\mbox{-}1}})}{2}-\theta_{\mbox{-}1}\left(\theta_0+\sum_i^n\theta_i m^{Y_i}\right)^2 \right) \\
&+&
 \ln{\frac{1}{\sqrt{2(\theta_0+\sum_i^n\theta_i m^{Y_i})}}} 
\end{eqnarray*}



\vspace{0.2in}
\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A \big(\theta(X) \big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
-\frac{\beta_1^2}{2\sigma^2}\\
\cdots\\
-\frac{\beta_n^2}{2\sigma^2}\\
\frac{\beta_1(X-\beta_0)}{\sigma^2}\\
\cdots\\
\frac{\beta_n(X-\beta_0)}{\sigma^2}\\
-\frac{\beta_1\beta_2}{\sigma^2}\\
\cdots\\
-\frac{\beta_1\beta_n}{\sigma^2}\\
\cdots\\
-\frac{\beta_{n-1}\beta_n}{\sigma^2}
\end{pmatrix}^T
\begin{pmatrix}
Y_1^2\\
\cdots\\
Y_n^2\\
Y_1\\
\cdots\\
Y_n\\
Y_1 Y_2\\
\cdots\\
Y_1 Y_n\\
\cdots\\
Y_{n-1}Y_{n}\\
\end{pmatrix}
- \left( \frac{(X-\beta_0)^2}{\sigma^2} + \ln{\sigma} \right) + \frac{1}{\ln{\sqrt{2\mu_{X|Y}}}}\\\\
&=&
\begin{pmatrix}
\theta_{1^2}\\
\cdots\\
\theta_{n^2}\\
\theta_1 m^X+\theta_{01}\\
\cdots\\
\theta_n m^X+\theta_{0n}\\
\theta_{12}\\
\cdots\\
\theta_{1n}\\
\cdots\\
\theta_{n\mbox{-}1n}
\end{pmatrix}^T
\begin{pmatrix}
Y_1^2\\
\cdots\\
Y_n^2\\
Y_1\\
\cdots\\
Y_n\\
Y_1 Y_2\\
\cdots\\
Y_1 Y_n\\
\cdots\\
Y_{n-1}Y_{n}\\
\end{pmatrix}
- \left( \frac{X^2 -2X\beta_0 +\beta_0^2}{\sigma^2} + \ln{\sigma} \right) + \frac{1}{\ln{\sqrt{2\mu_{X|Y}}}}\\
&=&
\begin{pmatrix}
\theta_{1^2}\\
\cdots\\
\theta_{n^2}\\
\theta_1 m^X+\theta_{01}\\
\cdots\\
\theta_n m^X+\theta_{0n}\\
\theta_{12}\\
\cdots\\
\theta_{1n}\\
\cdots\\
\theta_{n\mbox{-}1n}
\end{pmatrix}^T
\begin{pmatrix}
Y_1^2\\
\cdots\\
Y_n^2\\
Y_1\\
\cdots\\
Y_n\\
Y_1 Y_2\\
\cdots\\
Y_1 Y_n\\
\cdots\\
Y_{n-1}Y_{n}\\
\end{pmatrix}
- \left( (-2\beta_{\mbox{-}1}m^{X^2} - 2m^{X}\beta_0 - \frac{1}{2}\beta_0^2 \beta_{\mbox{-}1}^{-1}) + \frac{\ln{(2\theta_{\mbox{-}1}})}{2} \right)\\
&+&
\ln{\frac{1}{\sqrt{2(\theta_0+\sum_i^n\theta_i m^{Y_i})}}} 
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a binary parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution variable, and let $Y$ be a binary variable. The log-conditional probability of the child-node $X$ given its binary parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(Y= y^1) \ln p_{X \mid y^1} + I(Y= y^2) \ln p_{X \mid y^2} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y= y^1)  \Big(\theta_{X1} \cdot s(X) - A(\theta_{X1})\Big) +  I(Y= y^2) \Big(\theta_{X2} \cdot s(X) - A(\theta_{X2})\Big) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y=y^1) \cdot \theta_{X1} \cdot s(X) - I(Y=y^1) \cdot A(\theta_{X1}) +  I(Y=y^2) \cdot \theta_{X2} \cdot s(X) - I(Y=y^2) \cdot A(\theta_{X2})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2}) \\
\theta_{X1} \\
\theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
s(X) \cdot I(Y=y^1) \\
s(X) \cdot I(Y=y^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(Y) \\
&=&
\begin{pmatrix}
I(Y=y^1)\\
I(Y=y^2)\\
I(Y=y^1) \cdot \theta_{X1}\\
I(Y=y^2) \cdot \theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2}) \\
s(X) \\
s(X) 
\end{pmatrix}
- 0 \\\\
&=&
\begin{pmatrix}
m^Y_1\\
m^Y_2 \\
m^Y_1 \cdot \theta_{X1}\\
m^Y_2 \cdot \theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2}) \\
s(X) \\
s(X) 
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2})\\
s(X) \cdot \theta_{X1}\\
s(X) \cdot \theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p_{X \mid \mathbf{y}^l} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{Xl}   \cdot  s(X)  \cdot  A(\theta_{Xl}) \Big)~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \theta_{Xl} \cdot s(X) - I(\mathbf{Y} =\mathbf{y}^l) \cdot A(\theta_{Xl})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq}) \\
\theta_{X1} \\
\vdots \\
\theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s(X) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s(X) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y} ) &=& \theta(\mathbf{Y} )^T s(X) - A(\mathbf{Y} ) \\
&=&
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1)\\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q)\\
I(\mathbf{Y} =\mathbf{y}^1) \cdot \theta_{X1}\\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \cdot \theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq}) \\
s(X) \\
\vdots \\
s(X) 
\end{pmatrix}
- 0 \\\\
&=&
\begin{pmatrix}
\mathbf{m}^\mathbf{Y}_1\\
\vdots \\
\mathbf{m}^\mathbf{Y}_q \\
\mathbf{m}^\mathbf{Y}_1 \cdot \theta_{X1}\\
\vdots \\
\mathbf{m}^\mathbf{Y}_q \cdot \theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq}) \\
s(X) \\
\vdots \\
s(X) 
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
s(X) \cdot \theta_{X1}\\
\vdots \\
s(X) \cdot \theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}


%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}  +  \ldots + s(X) \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}\\
\vdots \\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}   + \ldots + s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!\\
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section*{Notations}
%-----------------------------------------------------------------------------------------------------------------------------------

The list below presents a summary of the used notations:
\\

\begin{table}[ht!]
\renewcommand{\arraystretch}{1.1}
{\small
\begin{tabular}{l l}
$X$ & Child variable\\
$k$& Range of possible values of a multinomial variable $X$\\
$j$ & Index over $X$ values, i.e., $1 \geq j \geq k$ \\
$Y$ & One parent variable\\
$\mathbf{Y}$ & Set of parent variables\\
$n$& Number of parent variables \\
$i$ & Index over parent variables, i.e., $1 \geq i \geq n$ \\
$r_i$& Range of possible values of a multinomial variable $Y_i$\\
$q $ & Total number of configurations of a multinomial parent set $\mathbf{Y}$\\
$l$ & Index over the possible parental configuration values, i.e., $1 \geq l \geq q$ \\
$\mathbf{y}^l$ & The $l^{th}$ configuration of a multinomial parent set $\mathbf{Y}$\\
$\theta_{jl}$ & Equal to $\ln p_{x^j\mid \mathbf{y}^l}$, denoting the log-conditional probability of $X$ in its state $j$ \\
                    & given the $l^{th}$ parent configuration\\
$\theta_{Xl}$ & Equal to $\ln p_{ X \mid \mathbf{y}^l}$, denoting the log-conditional probability of a base distribution variable $X$ \\
                    & given the $l^{th}$ parent configuration\\
$p$ & Probability distribution\\
$m$ & Expected sufficient statistics \\
$s$ & Sufficient statistics \\
\end{tabular}}
\end{table}
-

\end{appendices}

\bibliography{biblio}
\bibliographystyle{plain}

\end{document}


