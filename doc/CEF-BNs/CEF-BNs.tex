\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeY format
\usepackage{geometry}                		% See geom\dagetry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   		% ... or a4paper or a5paper or ... 
%\geometrx{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an emptx line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatey; use eps in DVI ye
\usepackage{array}							% TeY will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsmath}
\usepackage{cite}
\usepackage[final]{fixme}
\usepackage{pdfpages}
\usepackage{tabulary}
\usepackage{fancyheadings}
\usepackage{lastpage}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage[titletoc,title]{appendix}


\parskip 6pt % 1pt = 0.351 mm
\parindent 0pt

%\title{Requirement Engineering Process in AMIDST}
%\author{The handsome AMIDST guys et. al.}
%\date{Latest version, \today}							% Activate to display a given date or no date


%\setcounter{page}{2}
\newcommand{\drop}[1]{}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\newcommand{\bu}[1]{\mathbf{#1}}
\newcommand{\bv}[1]{\bm{#1}}

\newcommand{\todo}[1]{{\bf [TODO: #1]}}

\DeclareMathOperator*{\E}{\mbox{\large E}}

\newcommand{\me}{\mathrm{e}}

\numberwithin{figure}{section}
\numberwithin{equation}{section}
\numberwithin{table}{section}

\newcommand{\e}[1]{E\left[ #1 \right]}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage{pdfpages}

\begin{document}
\title{ Representation, Inference and Learning of Bayesian Networks as Conjugate Exponential Family Models }

\maketitle
\begin{abstract}

\end{abstract}

%------------------------------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------------------------------

Defining the data structure of a Bayesian network (BN) is not a straightforward problem. The definition of the data structure of a directed acyclic graph (DAG) is not complex when compared to the definition of the data structure of the conditional probability distributions (CPDs) encoded in the BN. The DAG is an \textit{homogeneous} data structure, in the sense that it is only composed by nodes and directed edges. However, the set of different CPDs is not limited at all. For example, the data structure for representing a Multinomial distribution is, in a first look, quite different from the data structure needed to represent a Normal distribution. In the former case, we need to store the probability of each case of the multinomial variable, while in the latter case, we need to store, for example, the mean and the variance of the Normal distribution. If we consider a Poisson, an Exponential, or a MoTBF, etc., the data structures needed to represent these distributions are completely different. These are only few examples of unidimensional distributions, however, when defining the data structure of CPDs, the issue becomes much more complex and challenging. 

For instance, the data structure for representing the CPD of a Normal distribution given a set of normally distributed variables is, in a first look, totally different from the data structure needed to represent the CPD of a Multinomial variable given a set of Multinomial variables. In the former case, under the conditional linear Gaussian framework, we need to store the coefficients for the linear combination of the parent variables plus the variance of the main variable. While the data structure for the multinomial given multinomial variables is usually defined using a big conditional probability table. Therefore, if we want to use a combination of Normal, Multinomial, Poison, Exponential, etc., in the same framework, the number of data structures needed to represent all the possible CPDs combinations quickly explode.

Another challenging problem is performing inference and learning of BNs with different kinds of CPDs. For example, the maximum likelihood of a Normal distribution is obtained by computing the sample mean and variance, while the maximum likelihood of a Multinomial distribution is obtained by normalizing the sample \textit{counts} of each state. Alternative methods are required for the different possible CPDs, which means that considering a new family of variables, i.e., Normal, Poisson, or Exponential, etc., implies the definition and the implementation from scratch of new maximum likelihood methods.  

In the case of inference, things are even worse. For example, the combination and marginalization operations over probability potentials belonging to different distribution families are in general non-closed and, in principle, involve quite different approaches. I.e., the combination or multiplication of two multinomial potentials or distributions involve completely different methods than the combination or product of two Normal distributions. This similarly applies to the marginalization operations. So, defining and coding all these operations for different distribution families can become a daunting task. 

In this technical report, we propose to use the so-called conjugate exponential family (CEF) models in order to avoid most of the above mentioned problems and save time by using previously known results and algorithms. Firstly, all the CPDs inside this family can be represented using the same data structure, which is simply composed by: 

\begin{itemize}
\item two $n$-dimensional vectors, namely, natural and moment parameters, and
\item two $n$-dimensional functions, namely, sufficient statistics and log-normalizer functions.
\end{itemize}

Moreover, we describe previously proposed learning and inference algorithms than can be directly implemented on top of this general and unique CEF representation. The result is a suitable framework for coding a toolbox which aims to deal with the problem of representing, making inference, and learning general BNs from data.

%------------------------------------------------------------------------------------------------------
\section{Background and notation}
%------------------------------------------------------------------------------------------------------

\subsection*{Bayesian networks}
Let $\bm X = \{X_1,\ldots,X_N\}$ denote the set of stochastic random variables defining our domain problem and $\bm x$ an observation vector. A Bayesian network (BN) defines a joint distribution $P(\bm X)$ in the following form:

$$ p(\bm X) = \prod_{i=1}^N p(X_i|Pa(X_i))$$ 

\noindent where $Pa(X_i)\subset \bm X\setminus X_i$ represents the so-called \emph{parent variables} of $X_i$. BNs can be graphically represented by a directed acyclic graph (DAG). Each node, labelled $X_i$ in the graph, is associated with a factor or conditional probability $p(X_i|Pa(X_i))$. Additionally, for each parent $X_j \in Pa(X_i)$, the graph contains one directed edge pointing from $X_j$ to the \emph{child} variable $X_i$.

%------------------------------------------------------------------------------------------------------
\subsection*{Exponential family models}
%------------------------------------------------------------------------------------------------------

A BN defines a joint probability in the exponential family with a natural (or canonical) parametrization if the joint distribution can be functionally expressed as follows:

\begin{equation}
\label{Equation:EFCanonical}
p(\bm x | \theta) = h(x)~exp\big(\theta^T s(\bm x) - A(\theta) \big)
\end{equation}

\noindent where $h(x)$ is the base measure, $A$ is the log partition function, and $\theta$ is a natural parameter that belongs to the \emph{natural parameter space} $\Theta$ defined as follows:

\begin{equation}
\label{Equation:NPS}
\Theta \equiv \{ \theta \in\Re^K: \int_{\bm x} h(\bm x)~exp\big(\theta^T s(\bm x) \big)~d\bm x < \infty \}
\end{equation}

\noindent where $s(\bm x)$ is the vector of sufficient statistics belonging to $\mathcal{S} \subseteq\Re^K$.

In addition, the \emph{expectation or moment parameter} $\mu\in\mathcal{S}$ can be used as well to parametrize probability
distributions of the exponential family. $\mu$ is defined as the \emph{expected vector of sufficient statistics} with respect to $\theta$:

\begin{equation}
\label{Equation:NaturalToMoment}
\begin{array}{lll}
\bm \mu & \triangleq & \e{s(\bm x)|\theta}  = \int_{\bm x} s(\bm x)~p(\bm x|\theta)~d\bm x\\
\end{array}
\end{equation}

The \emph{natural parameter} $\theta$ associated to an \emph{moment parameter} $\mu$ is obtained by solving the following optimization problem denoted as:
 
\begin{eqnarray}
\label{Equation:MomentToNatural}
\theta(\mu) = arg\max_{\theta\in\Theta}~\theta^T\mu -A(\theta)
\end{eqnarray}

Given the natural parameters, the inverse step of updating the moment parameters is not trivial in the case of conditional distributions, since as we will see in Section \ref{sec:CondDist}, the transformation requires the joint probability distribution of both children and parents. 

The importance of being able to convert from one parameter space to another takes special relevance in the implementation of the variational message passing algorithm, where message passing and updates are in general carried out in two different spaces. Another less explored alternative is the use of \textit{root} parameters, in order to prevent possible numerical imprecision. More information on this topic can be found in the following technical report \cite{HowJeb05}.

%------------------------------------------------------------------------------------------------------
\subsection*{Regular exponential family}
%------------------------------------------------------------------------------------------------------

A BN belongs to the linear exponential family if the natural parameter space $\Theta$ is an open and convex set. Additionally, a linear exponential family is said to be minimal if there is non-zero constant vector $\alpha$, such that $\alpha^Ts(\bm x)$ is equal to a constant for all $\bm x$.  We will consider the \textit{regular exponential family} (REF) to be the linear exponential one with minimal representation.


The REF with a minimal representation has been widely studied in the literature. The distributions in this family present many useful properties. The following two properties are considered among the most relevant ones: i)  the transformation between $\theta$ and $\mu$ parameters is a one-to-one correspondence, i.e., $\mu$ is a dual set of the model parameter $\theta$; and ii) the moment parameters $\mu$ are equal to the gradient of the log-normalizer (the proof of this equality is included in Appendix \ref{appendix:regularEFequality}):

\begin{equation}
\label{Equation:RegularEFEquality}
\mu = \frac{\partial A(\theta)}{\partial \theta}
\end{equation}


However, any BN which contains immoralities does not induce a linear exponential family (some parameters are restricted to (non)-linear constraints to ensure the conditional independence). Consequently, some of the nice properties of the REF are lost. In this case, we need to rely on a more general family called the \textit{curved exponential family} where the probability distribution can be more generally expressed as follows:

\begin{equation*}
p(\bm x |\theta) = h(x)~exp(\psi(\theta)^Ts(\bm x) - A(\theta) )
\end{equation*}

\noindent where $\psi$ is  a parameter transformation function. In our case, we will consider only the cases where $\psi$ is invertible. In this way, we could re-parametrize the distribution in a canonical form as shown in Equation \ref{Equation:EFCanonical}, where the natural parameter space is not any more an open convex set.


\begin{exmp}

A Bernoulli distribution can be represented as a curved exponential family as follows: 

$$
\ln p(x| \rho ) = 
\begin{pmatrix}
\ln \rho\\
\ln (1-\rho)
\end{pmatrix}^T
\begin{pmatrix}
I(x=0)\\
I(x=1)
\end{pmatrix}
$$

\noindent such that $\psi$ function is defined as follows: $\psi(\rho) = \big(\ln \rho, \ln (1-\rho) \big)$. Moreover, this distribution can be re-parametrized in cannonical form as follows: 

$$
\ln p(x| \theta ) = 
\begin{pmatrix}
\theta_1\\
\theta_2
\end{pmatrix}^T
\begin{pmatrix}
I(x=0)\\
I(x=1)
\end{pmatrix}
$$

\noindent where $\theta\in\Theta = \{ (\theta_1, \theta_2) \in \Re^2 : e^{\theta_1} + e^{\theta_2} = 1\}$.  As can be seen, $\Theta$ is not an open convex set in $\Re^2$. 

\end{exmp}

%------------------------------------------------------------------------------------------------------
\subsection*{Conditional distributions as exponential family models} \label{sec:CondDist}
%------------------------------------------------------------------------------------------------------

A conditional distribution $p(X|Pa(X))$ is in the exponential family if it can be written in the following functional form:

\begin{equation}
p(x | Pa(X)) = h(x)~exp \Big( \theta \big(Pa(X) \big)^Ts(x) - A \big(\theta  \big(Pa(X)  \big) \big) \Big) 
\end{equation}

\noindent where $\theta  \big(Pa(X) \big)$ denotes that the natural parameters which are now a function of the parent variables of $X$.  %Equivalently, we can say that $p(X|Pa(X))$ is in the exponential family if for any assignment, $\bm \pi$  to the parents variables, $p(X|\bm\pi)$ is in exponential form. 

A conditional distribution $p(X|Y)$ is said to be \textit{conjugate} to a child distribution $p(W|X)$ if $p(X|Y)$ has the same functional form, with respect to $X$, as $p(W|X)$. An important property of the conjugate exponential conditional distributions is that they can also be expressed in the following functional form:
 
\begin{equation}
\label{Equation:EqCED}
p(x | \bm y) = h(x)~exp\big(\theta^T~s(x,\bm y) - B(\theta) \big) 
\end{equation}
\noindent where $B(\theta)$ is the conditional log-normalizer. 

A BN is said to be a conjugate exponential family (CEF) model if all its CPDs belong to the exponential family and are conjugate as well. Importantly enough, the use of conjugate distributions allows that the posterior for each distribution has the same form as the prior, which means that only the parameter values change, but the functional form of the distribution remains the same.

%------------------------------------------------------------------------------------------------------
\section{Bayesian networks as CEF models}\label{Section:CEFBN}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\subsection{Representation} \label{Section:CEFBN:Representation}
%------------------------------------------------------------------------------------------------------

In this section, we show why conjugate exponential family Bayesian networks (CEF-BNs) are quite amenable to be represented (and coded) as exponential family models. 

By using Equation \ref{Equation:EqCED}, a CEF-BN can be represented in the following way: 

\begin{eqnarray}
\label{Equation:CEFSS}
\ln p(X_1,\ldots, X_n) &=& \sum_{i=1}^n \ln p(X_i|Pa(X_i))\nonumber\\
&=& \sum_{i=1}^n \theta_i \big(Pa(X_i)\big)^T s_i(X_i) - A_i\Big(\theta\big(Pa(X_i)\big)\Big)\nonumber\\
&=& \sum_{i=1}^n \theta_i^T~s_i\big(X_i, Pa(X_i)\big) - B_i(\theta_i)\nonumber\\
&=&
\begin{pmatrix}
\theta_1\\
\ldots \\
\theta_n\\
\end{pmatrix}^T
\begin{pmatrix}
s_1\big(X_1,Pa(X_1)\big) \\
\ldots \\
s_n\big(X_n,Pa(X_n)\big) \\
\end{pmatrix}
- \sum_{i=1}^n B_i(\theta_i)
\end{eqnarray}

The above expression shows us that we can represent a CEF-BN by using the local representations of the conditional probability distributions. In fact, 

\begin{itemize}
\item the natural parameters $\theta$ are formed by the composition of the local natural parameters of each conditional distribution, 
\item the sufficient statistics $s$ are formed the composition of the local sufficient statistics of each conditional distribution, and 
\item the log-normalizer $B$ is the sum of the local conditional log-normalizer of each conditional distribution. 
\end{itemize}

Hence, in order to represent a BN as a CEF model, we only have to worry about the local representation of each CPD as in Equation \ref{Equation:CEFSS}. The global representation is then just obtained by composing all these local representations. 

Let us notice that without the assumption of conjugacy  for the conditional exponential distributions, the above representation would have not been possible. 

%------------------------------------------------------------------------------------------------------
\subsection{From natural to moment parameters} \label{Section:CEFBN:NaturalToMoment}
%------------------------------------------------------------------------------------------------------

In this section, we examine the transformation from natural to moment parameters in a CEF-BN. Taking Equation \ref{Equation:NaturalToMoment} into account, the vector of moment parameters in a CEF-BN decomposes as follows:

\begin{equation}
\begin{array}{lll}
\mu & = & \e{s(X_1,\ldots, X_n)|\theta} \\
&=& \int s(X_1,\ldots, X_n)~p(X_1,\ldots, X_n|\theta)~d\bm X\\
&=& (\mu_1,\ldots,\mu_n) \\
\end{array}
\end{equation}

\noindent where $\mu_i= \int s(X_i,Pa(X_i))~p(X_i,Pa(X_i))d\bm X$. I.e., the global moment parameter of a CEF-BN locally decomposes into local moment parameters associated to local marginal probabilities. Note here that in order to compute the local marginal probability $p(X_i,Pa(X_i))$ we need to perform inference over the whole BN.

%------------------------------------------------------------------------------------------------------
\subsection{From moment to natural Parameters} \label{Section:CEFBN:MomentToNatural}
%------------------------------------------------------------------------------------------------------

The transformation from moment to natural parameters also decomposes. Let us start by expanding Equation \ref{Equation:MomentToNatural}, 

\begin{eqnarray}
\label{Equation:CEFBN_MomentToNatural}
\theta(\mu) &=& \arg\max_{\theta\in\Theta} \theta^T\mu
-A(\theta)\nonumber \\
&=& \arg\max_{(\theta_1,\ldots, \theta_n) \in\Theta} \sum_{i=1}^n \theta_i^T \mu_i - B_i(\theta_i) 
\end{eqnarray}

Given that $\theta_i$ parameters are independent, the above maximization problem fully decomposes into local maximization problems, one for each CPD:  

\begin{eqnarray}
\label{Equation:CEFBN_MomentToNaturalLocal}
\theta_i(\mu_i) = \arg\max_{\theta_i\in\Theta_i} \theta_i^T\mu_i - B(\theta_i)
\end{eqnarray}

\noindent and the global solution is just the aggregation of the local solutions as follows: 

$$ \theta(\mu) = \big(\theta_1(\mu_1), \ldots, \theta_n(\mu_n)\big)$$

Let us note that, as opposed to the previous case, the transformation from moment to natural parameters can be performed locally at each conditional distribution, and as stated above, the global solution is just an aggregation of the local solutions.

%------------------------------------------------------------------------------------------------------
\section{Conditional distributions with multinomial parents in exponential form}\label{Section:CD_With_MParents}
%------------------------------------------------------------------------------------------------------

In this section, we try to exploit the commonalities in terms of structure that are present in many CPDs in order to ease its representation in the exponential form, and thereby in the associated parameter transformations.  

The structure that we intend to exploit is the presence of multinomial variables in the parent set of a conditional distribution. The advantage of this representation is that if we know how to represent in a exponential form a given distribution (i.e., a Poisson, a Normal, a Multinomial, or a Normal distribution with Normal parents, etc.), then we can directly derive the corresponding distribution conditioned to multinomial parents (i.e. Poisson given Multinomial parents , Normal given Multinomial parents, Multinomial given Multinomial parents, or Normal given Normal and Multinomial parents, etc.). 

Similarly, we also show that for these conditional distributions the transformation from moment to natural parameters can be further decomposed. 

%------------------------------------------------------------------------------------------------------
\subsection{Representation} \label{Section:CD_With_MParents:Representation}
%------------------------------------------------------------------------------------------------------

Let $(\bm Z, \bm Y)$ be the set of parents of a variable $X$ such that $\mathbf{Y}$ denotes a set of multinomial variables and $\bm Z$ denotes set of non-multinomial variables\footnote{$\bm Z$ can be empty.}.  Let $q$ denote the total number of parental configurations for the variables in $\bm Y$, and $\mathbf{y}^l$ denote the $l$-th parental configuration, such that $1 \leq l \leq q$.

The log-conditional probability of $X$ given its parent-nodes $\bm Z$ and $\mathbf{Y}$ decomposes as  follows:

\begin{eqnarray*}
\ln p(X \mid \bm Z, \bm Y) &=&  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p(X | \bm Z, \mathbf{y}^l) \\
&=& \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{l}^T s_l(X, \bm Z)  -  B_l(\theta_{l}) \Big)\\
&=& \sum_{l=1}^q \theta_{l}^T  I(\mathbf{Y} =\mathbf{y}^l) s(X, \bm Z) - \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) B_l(\theta_{l})
\end{eqnarray*}

\noindent where $\theta_l$, $s_l(X,\bm Z)$ and $B_l(\theta_l)$ are provided when the local conditional distribution $p(X | \bm Z, \mathbf{y}^l)$ is expressed in exponential form. 

So, the conditional distribution $p(X \mid \bm Z, \bm Y)$ can be written in exponential form as follows: 

\begin{eqnarray}
\label{Equation:CD_With_MParents:Representation}
\ln p(X \mid \bm Z, \bm Y)  &=& \theta^T s(X,\mathbf{Y}) - B(\theta) \nonumber \\
&=&
\begin{pmatrix}
- B_1(\theta_{1}) \\
\vdots \\
- B_q(\theta_{q}) \\
\theta_{1} \\
\vdots \\
\theta_{q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s_1(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s_q(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray}

The corresponding proof can be found in Appendix \ref{appendix:CD_With_MParents:Representation}.

As can be seen, the exponential representation of a conditional probability with multinomial parents can be expressed as the composition of the exponential representation of the conditional distributions restricted to each one of the possible configurations of the multinomial parents. 

%------------------------------------------------------------------------------------------------------
\subsection{From natural to moment parameters} \label{Section:CD_With_MParents:NaturalToMoment}
%------------------------------------------------------------------------------------------------------

By using the decomposition of the sufficient statistics vector of this conditional probability distribution, the moment parameter vector associated to a natural parameter vector $\theta$ can be expressed as a combination of the moment parameters of the marginal probabilities $p(\bm Y|\theta)$ and $p(X, \bm Z)$ as follows:

$$ 
\begin{pmatrix}
\mu^{I}_1  \\
\vdots \\
\mu^{I}_q \\
\mu^{I}_1 \mu^{local}_1\\
\vdots \\
\mu^{I}_q \mu^{local}_q\\
\end{pmatrix}
$$

\noindent where  $\mu^{I}_l$ is the $l$-th component (i.e., a scalar) of the moment vector associated to the marginal distribution $p(\bm Y|\theta)$.  $\mu^{I}_l  = \int I(\bm Y = \bm y_l) p(\bm Y|\theta) d\bm Y= p(\bm y_l|\theta)$, and $\mu^{local}_l$  is the ``local'' moment parameter associated to the marginal distribution $p(X,\bm Z| \bm y = l)$, $\mu_l = \int s_l(X, \bm Z)p(X,\bm Z|\bm y = l, \theta) dX\bm Z$. 

Again, we can see that the moment parameters of this conditional distribution can be expressed as a composition of the local moment parameters of each of the distributions conditioned to each configuration of the multinomial parents variables. 

%------------------------------------------------------------------------------------------------------
\subsection{From moment to natural parameters} \label{Section:CD_With_MParents:MomentToNatural}
%------------------------------------------------------------------------------------------------------

We now look at how to transform from moment to natural parameters. Initially, the dimension of the optimization problem is $2q$ (i.e the dimension of the natural parameter vector), however we can exploit the structure present in the natural space and see that it boils down to a $q$ dimensional problem, 

\begin{eqnarray*}
\arg\max_{(\theta_1,\ldots, \theta_q) \in \Theta} \sum_{l=1}^q \theta_l^T \mu_{q+l}- \mu_l B_l(\theta_l)
\end{eqnarray*}

Furthermore, the above optimization problem decomposes in a set of $q$ independent optimization problems, 

\begin{eqnarray*}
\arg\max_{\theta_l \in \Theta_l} \theta_l^T \mu_{q+l}- \mu_l B_l(\theta_l)
\end{eqnarray*}

The solution of the above optimization problem is not affected if the optimized expression is divided by $\mu_l$, which is a scalar\footnote{If $\mu_l=0$ it would imply that $p(\bm Y= \bm y_t)=0$, so it does not make sense to solve the problem.}. So this problem can be transformed as follows, 

\begin{eqnarray}
\label{Equation:CD_With_MParents:MomentToNatural}
\arg\max_{\theta_l \in \Theta_l} \theta_l^T\mu'_l - B_l(\theta_l)
\end{eqnarray}

\noindent where $\mu'_l =\frac{1}{\mu_{l}}\mu_{q+l}$, i.e.the element-wise division of the vector $\mu_{q+l}$ by the scalar $\mu_l$.

The above problem simply corresponds to the transformation of the moment parameters $\mu'_l$ to their corresponding natural parameters $\theta_l$ for the conditional distribution $p(X|\bm Z, \bm y_l)$. So again, we see how the transformation from moment to natural decomposes in a series of local transformations. 

%------------------------------------------------------------------------------------------------------
\section{Maximum likelihood}
%------------------------------------------------------------------------------------------------------

In this section we look at the maximum likelihood problem in CEF-BNs. Our aim is to show how the solution to this multi-dimensional optimization problem has a common characterization in terms of exponential family representation and, moreover, boils down to smaller local problems for each conditional distribution\footnote{We point out that the following derivation is made without assuming that our models belongs to the regular exponential family and, in consequence, not using the equality of Equation \ref{Equation:RegularEFEquality}.}.

Let us assume that we are given a set of $m$ i.i.d. data samples $D=\{\bm x^{(1)}, \ldots, \bm x^{(m)}\}$ indexed by $j$. The maximum likelihood problem can be stated as follows:

\begin{eqnarray*}
\theta^\star  &=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \ln p(\bm x^{(j)}|\theta) \\
&=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \theta^Ts(\bm x^{(j)})  - A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\sum_{j=1}^m s(\bm x^{(j)})\Big)  - m A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\frac{1}{m}\sum_{j=1}^m s(\bm x^{(j)})\Big)  - A(\theta) \\
\end{eqnarray*}

\noindent where the last part is achieved by dividing the optimized equation by the number of samples $m$, what does not affect the result of the optimization. 

As widely known, the maximum likelihood is equivalent to a transformation form moment to natural parameters as stated in Equation \ref{Equation:MomentToNatural}: 

$$\theta^\star = \theta (\mu) = \theta\Big(\frac{1}{m}\sum_{j=1}^m s(\bm x^{(j)})\Big)$$

As shown in Section \ref{Section:CEFBN:MomentToNatural}, this problem decomposes for each CPD. The above formation can be expressed in terms of local transformations defined in Equation \ref{Equation:CEFBN_MomentToNaturalLocal}:

$$\theta_i^\star = \theta_i\Big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i)\Big)$$

To better understand the above decomposition, we should notice that $\theta_i(\mu_i)$ is directly related to maximum likelihood estimation of the conditional distribution $p(X_i|Pa(X_i))$:

\begin{eqnarray*}
\theta^\star  &=& \arg\max_{\theta_i \in \Theta_i} \sum_{j=1}^m \ln p(x_i^{(j)}|\bm{pa}^{(j)}_i,\theta) \\
&=& \arg\max_{\theta_i \in \Theta_i} \sum_{j=1}^m \Big(\theta_i^Ts(x_i^{(j)},\bm{pa}^{(j)}_i)  - B_i(\theta_i) \Big)\\
&=& \arg\max_{\theta_i \in \Theta_i} \theta_i^T\Big(\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i) \Big)  - m B_i(\theta) \\
&=& \arg\max_{\theta_i \in \Theta_i} \theta_i^T\Big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i) \Big)  - B_i(\theta) \\
\end{eqnarray*}

\noindent where $\bm{pa}^{(j)}_i$ corresponds to the $X_i$ parents values in the $j$-th data sample $\bm x^{(j)}$.

For those conditional distributions with multinomial parents, the problem further decomposes as previously shown in Section \ref{Section:CD_With_MParents:MomentToNatural}. 


%
%The maximum likelihood computation can be addressed using the following steps:
%
%\begin{enumerate}
%
%\item  Compute the sufficient statistics of a given data sample $\bm x^{(j)}$, which is the composition of the sufficient statistics of each conditional distribution (see Equation \ref{Equation:CEFSS}). If the conditional distribution has multinomial parents then we can use Equation \ref{Equation:CD_With_MParents:Representation} to compose the sufficient statistics of the conditional distribution. 
%
%\item Sum all the sufficient statistics and divide the sum vector by the total number of samples. 
%
%\item Obtain the natural parameters associated to this normalized sufficient statistics. According to Equation \ref{Equation:CEFBN_MomentToNatural}, this natural parameters can be obtain by aggregating the the natural parameters of the conditional distributions, which can be computed locally. If a conditional distribution has multinomial parents then we can use Equation \ref{Equation:CD_With_MParents:MomentToNatural} to compose the natural parameter of this conditional distribution. 
%\end{enumerate}

%------------------------------------------------------------------------------------------------------
\section{EM algorithms in CEF-BNs}
%------------------------------------------------------------------------------------------------------


%------------------------------------------------------------------------------------------------------
\section{Variational inference in CEF-BNs}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\section{Expectation propagation inference in CEF-BNs}
%------------------------------------------------------------------------------------------------------


%--------------------------------------------------------------------------------------------------------------------------------------------
\begin{appendices}
%--------------------------------------------------------------------------------------------------------------------------------------------



%------------------------------------------------------------------------------------------------------
\section{Regular EF: moment parameters equal the gradient of the log-normalizer}\label{appendix:regularEFequality}
%------------------------------------------------------------------------------------------------------


%------------------------------------------------------------------------------------------------------
\section{Conditional distributions with multinomial parents: proof of EF representation}\label{appendix:CD_With_MParents:Representation}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\section{EF representation: A binary child given a binary parent}
%------------------------------------------------------------------------------------------------------

Let $X$ and $Y$ be two binary variables. The log-conditional probability of the child-node $X$ given its parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(X= x^1) I(Y= y^1) \ln p_{x^1 \mid y^1} + I(X=x^2) I(Y= y^1) \ln p_{x^2 \mid y^1} \\
+ I(X=x^1) I(Y= y^2) \ln p_{x^1 \mid y^2} + I(X=x^2) I(Y= y^2) \ln p_{x^2 \mid y^2}
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
\ln p_{x^1 \mid y^1}\\
\ln p_{x^2 \mid y^1}\\
\ln p_{x^1 \mid y^2}\\
\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
\theta_{11}\\
\theta_{21}\\
\theta_{12}\\
\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(Y) \\
&=&
\begin{pmatrix}
I(Y=y^1)\ln p_{x^1 \mid y^1}  + I(Y=y^2)\ln p_{x^1 \mid y^2}\\
I(Y=y^1)\ln p_{x^2 \mid y^1}  + I(Y=y^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 \\
&=&
\begin{pmatrix}
m^Y_1\cdot\theta_{11}  + m^Y_2\cdot\theta_{12}\\
m^Y_1\cdot\theta_{21}  + m^Y_2\cdot\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
I(X=x^1)\ln p_{x^1 \mid y^1}  + I(X=x^2)\ln p_{x^2 \mid y^1}\\
I(X=x^1)\ln p_{x^1 \mid y^2}  + I(X=x^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  m^X_2\cdot \theta_{21}\\
m^X_1 \cdot \theta_{12}  + m^X_2 \cdot \theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A multinomial child given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a multinomial variable with $k$ possible values such that $k \geq 2$, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

$$ \ln p(X \mid \mathbf{Y}) = \sum_{j=1}^k \sum_{l=1}^q I(X=x^j) I(\mathbf{Y} =\mathbf{y}^l) \ln p_{x^j  \mid \mathbf{y}^l} $$

Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) \\\\
&=&
\begin{pmatrix}
\ln p_{x^1\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0 \\\\
&=&
\begin{pmatrix}
\theta_{11}\\
\vdots \\
\theta_{1q}\\
\vdots \\
\theta_{k1}\\
\vdots \\
\theta_{kq}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\vspace{0.5in}
\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(\mathbf{Y})^Ts(X) - A(\mathbf{Y}) \\ \\
&=&
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^1\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^k\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}
- 0 \\ \\
&=&
\begin{pmatrix}
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{11}  + m^{\mathbf{Y}}_q \cdot \theta_{1q} \\
\vdots \\
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{k1}  + m^{\mathbf{Y}}_q \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k)
\end{pmatrix}
- 0 
\end{eqnarray*}

\noindent such that $\mathbf{m}^{\mathbf{Y}}_1 = \prod_{i=1}^n I( Y_i = y_i^1) = \prod_{i=1}^n m^{Y_i}_1$ denotes the expected sufficient statistics for the first parental configuration, and $\mathbf{m}^{\mathbf{Y}}_q = \prod_{i=1}^n I( Y_i = y_i^{r_i})  = \prod_{i=1}^n m^{Y_i}_{r_i} $ denotes the expected sufficient statistics for the last parental configuration.

\vspace{0.5in}
\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\ \\
&=&
\begin{pmatrix}
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^1}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^1} \\
\vdots \\
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^q}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0\\ \\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  \ldots + m^X_k \cdot \theta_{k1}\\
\vdots \\
m^X_1 \cdot \theta_{1q}   + \ldots + m^X_k \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&= &
\begin{pmatrix}
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^1\mid \mathbf{y'}^1}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^k\mid \mathbf{y'}^1}  \! \\
\vdots \\
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^{q'})  \ln p_{x^1\mid \mathbf{y'}^{q'}}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^{q'}) \ln p_{x^k\mid \mathbf{y'}^{q'}}\! 
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
\! - 0 \! \\ \\
&=&
\begin{pmatrix}
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{11}  +  \ldots + m^X_k \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{k1}\\
\vdots \\
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{1q'}   + \ldots + m^X_k \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{kq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\noindent where $\mathbf{m}^{\mathbf{Y'}}_1 =  I(\mathbf{Y'} =\mathbf{y'}^1) = I( Y_1 = y_1^1) \cdot \ldots I( Y_{i-1} = y_{i-1}^1) \cdot I( Y_{i+1}  = y_{i+1}^1) \cdot \ldots I( Y_{n}  = y_{n}^1)$ denotes the expected sufficient statistics for the first configuration of the parent set $\mathbf{Y'} = \mathbf{Y} \setminus Y_i$, and $\mathbf{m}^{\mathbf{Y'}}_{q'} = I(\mathbf{Y'} =\mathbf{y'}^{q'}) = I( Y_1 = y_1^{q'}) \cdot \ldots I( Y_{i-1} = y_{i-1}^{q'}) \cdot I( Y_{i+1}  = y_{i+1}^{q'}) \cdot \ldots I( Y_{n}  = y_{n}^{q'})$ denotes the expected sufficient statistics for the last configuration of the parent set $\mathbf{Y'}$, with $q' = q / r_i$ denotes the total number of configurations of the parent set $\mathbf{Y'}$.

\end{itemize}

\newpage

%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A multinomial child given a Dirichlet parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a multinomial variable with $k$ possible values such that $k \geq 2$, and let $\rho$ denote a dirichlet parents of $X$. %Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ and $\rho$ can be expressed as follows:

$$ \ln p(X \mid \mathbf{Y},\rho) = \sum_{j=1}^k I(X=x^j)  \ln p_{x^j} $$

Note that here all $ I(\cdot)$ and $p(\cdot)$ values must be taken from the moment parameters of these variables, not the natural parameters of $X$.

Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}

\vspace{0.5in}
\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(\rho)^Ts(X) - A(\mathbf{Y}) \\ \\
&=&
\begin{pmatrix}
\ln p_{x^1}\\
\vdots \\
\ln p_{x^k} \\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}
- 0 \\ \\
\end{eqnarray*}


\vspace{0.5in}
\item \textbf{Third form}:

%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(X)^Ts(\rho) - A(X) \\ \\
&=&
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}^T
\begin{pmatrix}
\ln p_{x^1}\\
\vdots \\
\ln p_{x^k} \\
\end{pmatrix}
- 0 \\ \\
\end{eqnarray*}

\end{itemize}


\subsection{Dirichlet distribution}

Let $\rho$ be a dirichlet variable with parameters $\mathbf{u}, \mathbf{p}$, where $\mathbf{p}$ are the $k$ parameters of a multinomial distribution. The log-conditional probability of $\rho$ can be expressed as follows:

\begin{eqnarray*}
\ln p(\rho) &=& \ln \left( \frac{\Gamma ( \sum_{i=1}^k u_i )}{\prod_{i=1}^k \Gamma(u_i)} \prod_{i=1}^k p_i^{u_i-1} \right)\\\\
&=&
\begin{pmatrix}
u_1 - 1\\
\vdots\\
u_k -1
\end{pmatrix}^T
\begin{pmatrix}
\log{p_1} \\
\vdots\\
\log{p_k}
\end{pmatrix}
- \left( \sum_{i=1}^k \log \Gamma(u_i) - \log{\Gamma ( \sum_{i=1}^k u_i )} \right) + 0
\end{eqnarray*}

NB: More details to be found in the EF\_DirichletDistribution.pdf scanned file.

\newpage


%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A normal child given a set of normal parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a normal variable and $ \mathbf{Y} = \{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are normal. 

The log-conditional probability of $X$ given its parents $\mathbf{Y}$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n) &=& \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \me^{-\frac{(x-(\beta_0+ \bs \beta^T \cdot \bm Y))^2}{2\sigma^2}} \right)\\\\
&=&
- \ln{\sigma} - 0.5\ln{(2\pi)} - \frac{(x-\beta_0 - \bs \beta^T \mathbf{Y})^2}{2\sigma^2}
\end{eqnarray*}

where

\begin{tabular}{p{5.5cm}p{5.5cm}}
\begin{eqnarray*}
\bs \beta &=& 
\begin{pmatrix}
\beta_1\\
\vdots\\
\beta_n\\
\end{pmatrix}
\end{eqnarray*}
&
\begin{eqnarray*}
\bm Y &=& 
\begin{pmatrix}
Y_1\\
\vdots\\
Y_n\\
\end{pmatrix}
\end{eqnarray*}
\\
\end{tabular}

Similarly the above log-conditional probability can be expressed in the following exponential forms:


\newcommand{\Z}{\bm Z}
\newcommand{\Y}{\bm Y}
\newcommand{\thb}{\bs \theta}
\newcommand{\beb}{\bs \beta}

\begin{itemize}
\item \textbf{First form - Joint suff. stat. (Maxim. Likelihood, matrix representation)}:

$$\bm Z = ( X, \bm Y)$$
\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) + h(\mathbf{X})\\\\
&=&
\begin{pmatrix}
\thb_1 \\
\thb_2 \\
\end{pmatrix}^T
\begin{pmatrix}
\Z   \\
\Z^T\Z   \\
\end{pmatrix}
- \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}



%where $\mu_{X|\bm Y} = \beta_0+ \bs \beta^T \cdot \bm Y$

\begin{eqnarray*}
\Z &=& 
\begin{pmatrix}
X & Y
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\Z^T\Z &=& 
\begin{pmatrix}
XX^T   & X\Y \\
\Y X^T & Y \Y^T
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\thb_1 &=& 
\begin{pmatrix}
\beta_0\sigma^{-2} & -\beta_0\beb\sigma^{-2}
\end{pmatrix}
=
\begin{pmatrix}
\theta_{\beta_0} & \thb_{\beta_0\beb}
\end{pmatrix}
=\beta_0\sigma^{-2}
\begin{pmatrix}
1 & -\beb
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\thb_2 &=& 
\begin{pmatrix}
-0.5\sigma^{-2}   &  \beb 0.5\sigma^{-2}\\
\beb 0.5\sigma^{-2}   & -\beb \beb^T 0.5\sigma^{-2}
\end{pmatrix}
= 
\begin{pmatrix}
\theta_{\mbox{-}1}   &  \thb_{\beb} \\
\thb_{\beb}^T   &  \thb_{\beb\beb}
\end{pmatrix}
= 0.5\sigma^{-2}
\begin{pmatrix}
-1   &  \beb \\
\beb    & -\beb \beb^T 
\end{pmatrix}
\end{eqnarray*}

\begin{itemize}
\item \textbf{From moment to natural parameters: }





\begin{itemize}


\item FIRST STEP: 
\begin{eqnarray*}
\mu_X &=& E(X)\\
\mu_\mathbf{Y} &=& \E(\Y)\\
\Sigma_{XX} &=&  \E(XX^T) - \E(X)\E(X)^T\\
\Sigma_{\mathbf{YY}} &=&  \E(\Y\Y^T) - \E(\Y)\E(\Y)^T \\
\Sigma_{X\mathbf{Y}} &=&  \E(X \Y) - \E(X)\E(\Y)^T\\
\Sigma_{\mathbf{Y}X} &=&  \E(\Y X^T) - \E(\Y)\E(X)
\end{eqnarray*}

\item SECOND STEP (Theorem 7.4 in page 253, Koller \& Friedman):
\begin{eqnarray*}
\beta_0 &=& \mu_X - \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}}\mu_\mathbf{Y}\\
\beb   &=& \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}} \\
\sigma^2 &=& \Sigma_{XX} - \Sigma_{XY}\Sigma^{-1}_{YY}\Sigma_{YX}
\end{eqnarray*}

All natural parameters $\theta$ can now be calculated considering these equations.
\end{itemize}

\item \textbf{From natural to moment parameters:}
Via inference.

\end{itemize}

\vspace{0.5in}
\item \textbf{Second form}:

\begin{eqnarray}\label{NormalGNomal2form}
\ln p(X\mid \mathbf{Y}) &=& \theta(\mathbf{Y})^T s(X) - A \big(\theta(\mathbf{Y})\big) + h(\mathbf{X})\\\nonumber\\
&=&
\begin{pmatrix}
\frac{\mu_{X|Y}}{\sigma^2} \nonumber\\
\frac{-1}{2\sigma^2} \nonumber\\
\end{pmatrix}^T
\begin{pmatrix}
X\\
X^2\\
\end{pmatrix}
- \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right) - \tfrac{1}{2}\ln{(2\pi)} \nonumber\\\nonumber\\
&=&
\begin{pmatrix}
\theta_{\beta_0}+2\thb_{\beb} \Y \nonumber\\
\theta_{\mbox{-}1} \nonumber\\
\end{pmatrix}^T
\begin{pmatrix}
X \nonumber\\
X^2 \nonumber\\
\end{pmatrix}\nonumber\\
%- \left(\frac{\ln{(2\theta_{\mbox{-}1}})}{2}-\theta_{\mbox{-}1}\left(\theta_{\beta_0}+2\thb_{\beb} \E(\Y)\right)^2 \right) \\
%&+&
% \ln{\frac{1}{\sqrt{2(\theta_{\beta_0}+2\thb_{\beb} \E(\Y))}}} 
&-& 
\left(-0.5\ln{(-2\theta_{\mbox{-}1})} - \thb_{\beta_0\beb} \Y - \thb_{\beb\beb}\Y\Y^T -\frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}}\right) - \tfrac{1}{2}\ln{(2\pi)} \nonumber
\end{eqnarray}



\vspace{0.2in}
\item \textbf{Third form (for each parent $Y_i$, $\Y' = \Y \setminus Y_i $)}: 

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X,\Y')^T s(Y_i) - A \big(\theta(X,\Y') \big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
\thb_{\beta_0\beb}^i + 2\thb_{\beb}^iX + 2\thb_{\beb\beb}^{'i \cdot}\Y \\
\thb_{\beb\beb}^{ii} 
\end{pmatrix}^T
\begin{pmatrix}
Y_i\\
{Y_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}

where $\thb_{\cdot}^i$ is the $i$th component of $\thb_{\cdot}$,\\
where $\thb_{\beb\beb}^{'i\cdot} = \thb_{\beb\beb}^{i\cdot}$ with $\thb_{\beb\beb}^{ii} = 0$, (vector)\\
where $\thb_{\beta_0\beb}' = \thb_{\beta_0\beb}$ with $\thb_{\beta_0\beb}^i = 0$,\\
where $\thb_{\beb}' = \thb_{\beb}$ with $\thb_{\beb}^i = 0$,\\
where $\thb_{\beb\beb}' = \thb_{\beb\beb}$ with $\thb_{\beb\beb}^{ii} = 0$, $\thb_{\beb\beb}^{i\cdot} = 0$  and $\thb_{\beb\beb}^{\cdot i} = 0$.\\


\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A normal child given a set of normal parents and a gamma parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a normal variable and $ \mathbf{Y} = \{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are normal. $beta_0$, $ \beb = \{beta_1,\ldots,beta_n\}$ now represents normal variables (prior distributions of of the beta parameters) and $\gamma$ is an inverse gamma distribution (prior distribution of the parameter $\sigma^2$).

The log-conditional probability of $X$ given its parents $\mathbf{Y}, \beb, \gamma$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n,\beb,\gamma) &=& \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \me^{-\frac{(x-(\beta_0+ \bs \beta^T \cdot \bm Y))^2}{2\sigma^2}} \right)\\\\
&=&
- \ln{\sigma} - 0.5\ln{(2\pi)} - \frac{(x-\beta_0 - \bs \beta^T \mathbf{Y})^2}{2\sigma^2}
\end{eqnarray*}


Note that here all $\beb$ and $\sigma^2$ values must be taken from the moment parameters of these variables, not the natural parameters of $X$.

\begin{itemize} 

\item \textbf{Second form (messages from $\beb$ and $\gamma$ variables to $X$)}:

As in \ref{NormalGNomal2form}, but now the natural parameters should be taken from the moment parameters of the parent variables, and not the natural parameters of $X$.

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(\mathbf{Y})^T s(X) - A \big(\theta(\mathbf{Y})\big) + h(\mathbf{X})\\\\
&=&
\begin{pmatrix}
\frac{\beta_0}{\sigma^2} + \frac{\beb}{\sigma^2} \Y\\
\frac{-1}{2\sigma^2}\\
\end{pmatrix}^T
\begin{pmatrix}
X \\
X^2 \\
\end{pmatrix}\\ 
&-& 
\left(0.5\ln{\sigma^2} + \frac{\beta_0\beb}{\sigma^2} \Y + \frac{\beb\beb^T}{2\sigma^2}\Y\Y^T +\frac{\beta_0^2}{2\sigma^2}\right) - \tfrac{1}{2}\ln{(2\pi)} 
\end{eqnarray*}

\item \textbf{Third form (messages from $X$ to $\beta_0$)}:
\begin{eqnarray*}
\ln p(X\mid \mathbf{Y},\beta_0,\beb,\gamma) &=& \theta(X,\Y,\beb,\gamma)^T s(\beta_0) - A \big(\theta(X,\Y,\beb,\gamma) \big) + h(\mathbf{Y,\beta_0,\beb,\gamma})\\\\
&=&
\begin{pmatrix}
X\sigma^{-2} - \beb^T \Y \sigma^{-2} \\
-0.5\sigma^{-2}
\end{pmatrix}^T
\begin{pmatrix}
\beta_0\\
\beta_0^2\\
\end{pmatrix}\\
&-& X^2 0.5\sigma^{-2} - \beb^T \beb \Y^T \Y 0.5\sigma^{-2}  + X\beb^T \Y \sigma^{-2} - \ln{\sigma} - 0.5\ln{(2\pi)}
\end{eqnarray*}


\item \textbf{Third form (messages from $X$ to $\beta_i$)}:
\begin{eqnarray*}
\ln p(X\mid \mathbf{Y},\beta_0,\beb,\gamma) &=& \theta(X,\Y,\beta_0,\beb',\gamma)^T s(\beta_i) - A \big(\theta(X,\Y,\beta_0,\beb',\gamma) \big) + h(\mathbf{Y,\beta_0,\beb,\gamma})\\\\
&=&
\begin{pmatrix}
-\beta_0 Y_i \sigma^{-2} + Y_i X \sigma^{-2} - \beb_i' Y_i \Y' \sigma^{-2} \\
-0.5Y_iY_i \sigma^{-2}
\end{pmatrix}^T
\begin{pmatrix}
\beta_i\\
{\beta_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}

where $\beb_i' = \beb$ where $\beta_i = 0$,\\


\item \textbf{Third form (messages from $X$ to $\gamma$ ($\sigma^2$))}:
\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n,\beb,\gamma) &=& \theta(X,\Y, \beta_0, \beb)^T s(\gamma) - A \big(\theta(X,\Y,\beta_0,\beb) \big) + h(\mathbf{\Y,\beta_0, \beb})\\\\
&=&
\begin{pmatrix}
-\frac{1}{2}\\
-\frac{(X-\beta_0-\beb\Y)^2}{2}
\end{pmatrix}^T
\begin{pmatrix}
\ln{\sigma^2}\\
\frac{1}{\sigma^2}
\end{pmatrix}
 - 0.5\ln{(2\pi)}
\end{eqnarray*}

\item \textbf{Third form (for each parent $Y_i$, $\Y' = \Y \setminus Y_i $)}: 

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X,\Y')^T s(Y_i) - A \big(\theta(X,\Y') \big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
-\beta_0\beta_i \sigma^{-2} + \beta_i X \sigma^{-2} - \beta_i\beb_i' \Y' \sigma^{-2} \\
-0.5\beta_i\beta_i \sigma^{-2} 
\end{pmatrix}^T
\begin{pmatrix}
Y_i\\
{Y_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}
where $\beb_i' = \beb \setminus \beta_i$ \\

\end{itemize}


\subsection{Inverse gamma distribution}

Let $\gamma$ be an inverse gamma variable with parameters $\alpha, \beta$. The log-conditional probability of $\gamma$ can be expressed as follows:

\begin{eqnarray*}
\ln p(\gamma) &=& \ln \left( \frac{\beta^{\alpha}}{\Gamma(\alpha)} \gamma^{-\alpha-1} e^{-\frac{\beta}{\gamma}} \right)\\\\
&=&
\begin{pmatrix}
-\alpha - 1\\
-\beta
\end{pmatrix}^T
\begin{pmatrix}
\ln \gamma \\
\frac{1}{\gamma}
\end{pmatrix}
- \left(\ln{\Gamma(\alpha)} - \alpha \ln{\beta}\right) + 0
\end{eqnarray*}

NB: More details to be found in the EF\_Gamma\_Inv-GammaDistributions.pdf scanned file.

\newpage



%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a binary parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution variable, and let $Y$ be a binary variable. The log-conditional probability of the child-node $X$ given its binary parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(Y= y^1) \ln p_{X \mid y^1} + I(Y= y^2) \ln p_{X \mid y^2} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y= y^1)  \Big(\theta_{X1} \cdot s(X) - A(\theta_{X1})\Big) +  I(Y= y^2) \Big(\theta_{X2} \cdot s(X) - A(\theta_{X2})\Big) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y=y^1) \cdot \theta_{X1} \cdot s(X) - I(Y=y^1) \cdot A(\theta_{X1}) +  I(Y=y^2) \cdot \theta_{X2} \cdot s(X) - I(Y=y^2) \cdot A(\theta_{X2})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2}) \\
\theta_{X1} \\
\theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
s(X) \cdot I(Y=y^1) \\
s(X) \cdot I(Y=y^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(\theta(Y)) \\
&=&
\Big(I(Y=y^1) \cdot \theta_{X1} + I(Y=y^2) \cdot \theta_{X2}\Big)
s(X) \\
&&- I(Y=y^1) \cdot  A(\theta_{X1}) -  I(Y=y^2) \cdot A(\theta_{X2})\\\\
&=&
\Big(m^Y_1 \cdot \theta_{X1} + 
m^Y_2 \cdot \theta_{X2}\Big)
s(X) 
-  m^Y_1 \cdot  A(\theta_{X1}) -  m^Y_2  \cdot A(\theta_{X2})
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2})\\
s(X) \cdot \theta_{X1}\\
s(X) \cdot \theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, Y) =  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p_{X \mid \bm Z, \mathbf{y}^l} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{Xl}   \cdot  s(X,\bm Z)  \cdot  A(\theta_{Xl}) \Big)~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \theta_{Xl} \cdot s(X,\bm Z) - I(\mathbf{Y} =\mathbf{y}^l) \cdot A(\theta_{Xl})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{First form}:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, \mathbf{Y}) &=& \theta^T s(X,\bm Z, \mathbf{Y}) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq}) \\
\theta_{X1} \\
\vdots \\
\theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s(X, \bm  Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s(X, \bm  Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{Second form}:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, \mathbf{Y} ) &=& \theta(\bm  Z, \mathbf{Y} )^T s(X) - A(\mathbf{Y} ) \\
&=&
\Big(\sum^q_{l=1} I(\mathbf{Y} =\mathbf{y}^l) \cdot \theta_{X_l}(\bm  Z) \Big)
s(X)
- \sum^q_{l=1} I(\mathbf{Y} =\mathbf{y}^l) \cdot A(\theta_{X_l}(\bm Z)) \\\\
&=&
\Big(\sum^q_{l=1} \mathbf{m}^\mathbf{Y}_l \cdot \theta_{X_l}(\bm  Z) \Big)
s(X)
- \sum^q_{l=1} \mathbf{m}^\mathbf{Y}_l \cdot A(\theta_{X_l}(\bm Z)) \\\\
\end{eqnarray*}

\item \textbf{Third form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
s(X) \cdot \theta_{X1}\\
\vdots \\
s(X) \cdot \theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}


%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}  +  \ldots + s(X) \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}\\
\vdots \\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}   + \ldots + s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!\\
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section*{Notations}
%-----------------------------------------------------------------------------------------------------------------------------------

The list below presents a summary of the used notations:
\\

\begin{table}[ht!]
\renewcommand{\arraystretch}{1.1}
{\small
\begin{tabular}{l l}
$X$ & Child variable\\
$k$& Range of possible values of a multinomial variable $X$\\
$j$ & Index over $X$ values, i.e., $1 \geq j \geq k$ \\
$Y$ & One parent variable\\
$\mathbf{Y}$ & Set of parent variables\\
$n$& Number of parent variables \\
$i$ & Index over parent variables, i.e., $1 \geq i \geq n$ \\
$r_i$& Range of possible values of a multinomial variable $Y_i$\\
$q $ & Total number of configurations of a multinomial parent set $\mathbf{Y}$\\
$l$ & Index over the possible parental configuration values, i.e., $1 \geq l \geq q$ \\
$\mathbf{y}^l$ & The $l^{th}$ configuration of a multinomial parent set $\mathbf{Y}$\\
$\theta_{jl}$ & Equal to $\ln p_{x^j\mid \mathbf{y}^l}$, denoting the log-conditional probability of $X$ in its state $j$ \\
                    & given the $l^{th}$ parent configuration\\
$\theta_{Xl}$ & Equal to $\ln p_{ X \mid \mathbf{y}^l}$, denoting the log-conditional probability of a base distribution variable $X$ \\
                    & given the $l^{th}$ parent configuration\\
$p$ & Probability distribution\\
$m$ & Expected sufficient statistics \\
$s$ & Sufficient statistics \\
\end{tabular}}
\end{table}
-

\end{appendices}

\bibliography{biblio}
\bibliographystyle{plain}

\end{document}


