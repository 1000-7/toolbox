\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeY format
\usepackage{geometry}                		% See geom\dagetry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   		% ... or a4paper or a5paper or ... 
%\geometrx{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an emptx line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatey; use eps in DVI ye
\usepackage{array}							% TeY will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsmath}
\usepackage{cite}
\usepackage[final]{fixme}
\usepackage{pdfpages}
\usepackage{tabulary}
\usepackage{fancyheadings}
\usepackage{lastpage}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage[titletoc,title]{appendix}
\usepackage{amsthm}
\usepackage{framed}

\newtheorem{mydef}{Definition}
\newtheorem{theorem}{Theorem}

\parskip 6pt % 1pt = 0.351 mm
\parindent 0pt

%\title{Requirement Engineering Process in AMIDST}
%\author{The handsome AMIDST guys et. al.}
%\date{Latest version, \today}							% Activate to display a given date or no date


%\setcounter{page}{2}
\newcommand{\drop}[1]{}
\newcommand{\bm}{\mathbf}
\newcommand{\bs}{\boldsymbol}

\newcommand{\bu}[1]{\mathbf{#1}}
\newcommand{\bv}[1]{\bm{#1}}

\newcommand{\todo}[1]{{\bf [TODO: #1]}}

\DeclareMathOperator*{\E}{\mbox{\large E}}

\newcommand{\me}{\mathrm{e}}

\numberwithin{figure}{section}
\numberwithin{equation}{section}
\numberwithin{table}{section}

\newcommand{\e}[1]{E\left[ #1 \right]}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage{pdfpages}

\begin{document}
\title{ Representation, Inference and Learning of Bayesian Networks as Conjugate Exponential Family Models }

\maketitle
\begin{abstract}

\end{abstract}

%------------------------------------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------------------------------------

Defining the data structure of a Bayesian network (BN) is not a straightforward problem. The definition of the data structure of a directed acyclic graph (DAG) is not complex when compared to the definition of the data structure of the conditional probability distributions (CPDs) encoded in the BN. The DAG is an \textit{homogeneous} data structure, in the sense that it is only composed by nodes and directed edges. However, the set of different CPDs is not limited at all. For example, the data structure for representing a Multinomial distribution is, in a first look, quite different from the data structure needed to represent a Normal distribution. In the former case, we need to store the probability of each case of the multinomial variable, while in the latter case, we need to store, for example, the mean and the variance of the Normal distribution. If we consider a Poisson, an Exponential, or a MoTBF, etc., the data structures needed to represent these distributions are completely different. These are only few examples of unidimensional distributions, however, when defining the data structure of CPDs, the issue becomes much more complex and challenging. 

For instance, the data structure for representing the CPD of a Normal distribution given a set of normally distributed variables is, in a first look, totally different from the data structure needed to represent the CPD of a Multinomial variable given a set of Multinomial variables. In the former case, under the conditional linear Gaussian framework, we need to store the coefficients for the linear combination of the parent variables plus the variance of the main variable. While the data structure for the multinomial given multinomial variables is usually defined using a big conditional probability table. Therefore, if we want to use a combination of Normal, Multinomial, Poison, Exponential, etc., in the same framework, the number of data structures needed to represent all the possible CPDs combinations quickly explode.

Another challenging problem is performing inference and learning of BNs with different kinds of CPDs. For example, the maximum likelihood of a Normal distribution is obtained by computing the sample mean and variance, while the maximum likelihood of a Multinomial distribution is obtained by normalizing the sample \textit{counts} of each state. Alternative methods are required for the different possible CPDs, which means that considering a new family of variables, i.e., Normal, Poisson, or Exponential, etc., implies the definition and the implementation from scratch of new maximum likelihood methods.  

In the case of inference, things are even worse. For example, the combination and marginalization operations over probability potentials belonging to different distribution families are in general non-closed and, in principle, involve quite different approaches. I.e., the combination or multiplication of two multinomial potentials or distributions involve completely different methods than the combination or product of two Normal distributions. This similarly applies to the marginalization operations. So, defining and coding all these operations for different distribution families can become a daunting task. 

In this technical report, we propose to use the so-called conjugate exponential family (CEF) models in order to avoid most of the above mentioned problems and save time by using previously known results and algorithms. Firstly, all the CPDs inside this family can be represented using the same data structure, which is simply composed by: 

\begin{itemize}
\item two $n$-dimensional vectors, namely, natural and moment parameters, and
\item two $n$-dimensional functions, namely, sufficient statistics and log-normalizer functions.
\end{itemize}

Moreover, we describe previously proposed learning and inference algorithms than can be directly implemented on top of this general and unique CEF representation. The result is a suitable framework for coding a toolbox which aims to deal with the problem of representing, making inference, and learning general BNs from data.

%------------------------------------------------------------------------------------------------------
\section{Exponential family models}\label{sec:ef}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\subsection{Definition}
%------------------------------------------------------------------------------------------------------

Let $\bm X = \{X_1,\ldots,X_N\}$ denote the set of stochastic random variables defining our domain problem and $\bm x$ an observation vector.  We say that the joint probability distribution $p(\cdot|\theta)$ parametrized by a parameter vector $\theta$ is an \textit{exponential family model} with a natural (or canonical) parametrization if the logarithm of $p$ can be functionally expressed as follows

\begin{equation}
\label{Equation:EFCanonical}
\ln p_\theta(\bm x) = \theta^T s(\bm x) - A(\theta) + h(\bm x),
\end{equation}

\noindent which is based on the following definitions,


\begin{itemize}
\item $h(x)$ is the log-base measure. Its domain, denoted by $\mathcal{X}$, is the Cartesian product of the domains of random variables in $\bm X$, and its codomain are the positive real numbers, $ h: \mathcal{X} \rightarrow \Re^+$.

\item $\theta\in \Theta$ is the natural parameter vector and $\Theta\subseteq \Re^K$ is the natural parameter space, where $K$ being called  the dimension of the model. This \emph{natural parameter space} $\Theta$ is defined as follows:
\begin{equation}
\label{Equation:NPS}
\Theta \equiv \{ \theta \in\Re^K: \int_{\bm x} exp\big(\theta^T s(\bm x) + h(\bm x) \big)~d\bm x < \infty \}
\end{equation}
\noindent i.e., as the the set of parameter vectors which define a proper normalizable density. 

\item $A(\theta)$ is the log-partition function, which is defined as follows:

$$ A(\theta) = \int_{\bm x} exp\big(\theta^T s(\bm x) + h(\bm x)\big)~d\bm x$$

So, its domain is $\Theta$ and its codomain are the positive real numbers, $ A: \Theta \rightarrow \Re^+$.

\item $s(\bm x)$ is the sufficient statistics function, whose domain is $\mathcal{X}$ and is codomain is denoted by $\mathcal{S}\subseteq \Re^K$, $\bm s : \mathcal{X}\rightarrow \mathcal{S}$. We also refers to $s(\bm x)$ as the sufficient statistics (vector) of the observation $\bm x$. 
\end{itemize}


More specific subfamilies inside this broad class of probabilistic models are usually considered in the literature:

\begin{description}

\item[Minimal  Exponential Family:] An exponential family is \textit{minimal} if there is no a non-zero constant vector $\alpha$, such that $\alpha^Ts(\bm x)$ is equal to a constant for all assignments $\bm x$.  In this case, there is a unique parameter vector $\theta$ associated with each probability distribution.

\item[Overcomplete  Exponential Family:] An exponential family is overcomplete if it is not minimal. In this case, there exists an entire affine subset of parameter vectors $\theta$, each one associated with the same probability distribution.

\item[Regular Exponential Family:] An exponential family for which its natural parameter space $\Theta$ is an open set. 

\item[Linear Exponential Family:] An exponential family which is regular and minimal. 

\item[Curved Exponential Family:] Equation \ref{Equation:EFCanonical} describing exponential families can be generalized by writing 

\begin{equation}
\label{Equation:EFCurved}
\ln p_\theta(\bm x) = \eta(\theta)^T s(\bm x) - A(\theta) + h(\bm x),
\end{equation}

\noindent where now $\eta$ is a function that maps the parameters $\theta$ to the canonical parameters $\bm \eta=\eta(\theta)$. A model belongs to the \textit{curved exponential family} if it is described by Equation \ref{Equation:EFCurved} and $dim(\theta)<dim(\eta(\theta))$, which means that the model has more sufficient statistics than parameters. 

\end{description}

%------------------------------------------------------------------------------------------------------
\subsection{Dual Parametrization}
%------------------------------------------------------------------------------------------------------
A key property of exponential family models is that they can be alternatively parametrized by a so-called \emph{moment parameter} vector $\bm \mu \in \mathcal{S}$. The relevancy of this dual parametrization is that several statistical computations, such as marginalization and maximum likelihood estimation, can be understood as transforming from one parameterization to the other.  

By definition, a vector $\bm \mu$ is defined as the \emph{expected vector of sufficient statistics} with respect to $\theta$ as follows
\begin{equation}
\label{Equation:NaturalToMoment}
\begin{array}{lll}
\bm \mu & \triangleq & \e{s(\bm x)|\theta}  = \int_{\bm x} s(\bm x)~p_\theta(\bm x)~d\bm x\\
\end{array}
\end{equation}

As can be seen, computing the moment parameter vector requires to perform inference over the probability distribution $p$. When $p$ belongs to the exponential family, the association between $p$ and $\mu$ is one-to-one.


The transformation from \emph{natural parameters} to  \emph{moment parameters} can be achieved by solving the following optimization problem,
 \begin{eqnarray}
\label{Equation:MomentToNatural}
\theta(\mu) = arg\max_{\theta\in\Theta}~\theta^T\mu -A(\theta)
\end{eqnarray}

\noindent where $\theta (\cdot)$ is, by abuse of notation, the transformation function from expectation parameters to natural parameters, i.e. $\theta (\cdot) : \mathcal{S} \rightarrow \Theta$.

The above equation is also known as the \emph{maximum likelihood function}, because $\theta(\frac{1}{n}\sum_{i=1}^n s(x_i))$ gives the maximum likelihood estimation $\theta^\star$ for a data set with $n$ i.i.d. observations $\{x_1,\ldots,x_n\}$.

For the minimal exponential family, the transformation between $\theta$ and $\mu$ is one-to-one: $\mu$ is a dual set of the model parameter $\theta$ \cite{amari1985differential}. That is to say, for each $\theta\in \Theta$ we always have an associated $\mu\in\mathcal{S}$ and both have the same dimension and parameterize the same probability distribution. For overcomplete exponential families,  there is an entire affine subset of parameters $\theta$ associated to the moment parameter vector $\mu$. 

For regular exponential families, the transformation from \emph{natural parameters} to \emph{moment parameters} can be nicely interpreted as the gradient of the log-normalizer function, 

\begin{equation}
\label{Equation:NaturalToMomentRegular}
\begin{array}{lll}
\bm \mu & \triangleq & \e{s(\bm x)|\theta}  = \partial A(\theta)/\partial \theta \\
\end{array}
\end{equation}

Regular exponential families with a minimal representation, i.e. linear exponential family, also enjoy a wider a set of nice properties and has been widely studied in the literature. However, they are not relevant for this paper. 



%The above equation is also known as the \emph{maximum likelihood
%function}, because $\theta(\frac{1}{n}\sum_{i=1}^n s(y_i,x_i))$
%gives the maximum likelihood estimation $\theta^\star$ for a data
%set with $n$ observations $\{(y_1,x_1),\ldots,(y_n,x_n)\}$.

%The transformation between $\theta$ and $\mu$ is one-to-one: $\mu$ is a dual set of the model parameter $\theta$ \cite{amari1985differential}. That is to say, for each $\theta\in \Theta$ we always have an associated $\mu\in\mathcal{S}$ and both have the same dimension and parameterize the same probability distribution. Therefore, Equation (\ref{Equation:NaturalToMoment})

%Given the natural parameters, the inverse step of updating the moment parameters is not trivial in the case of conditional distributions, since as we will see in Section \ref{sec:CondDist}, the transformation requires the joint probability distribution of both children and parents. 

%The importance of being able to convert from one parameter space to another takes special relevance in the implementation of the variational message passing algorithm, where message passing and updates are in general carried out in two different spaces. Another less explored alternative is the use of \textit{root} parameters, in order to prevent possible numerical imprecision. More information on this topic can be found in the following technical report \cite{HowJeb05}.

%%------------------------------------------------------------------------------------------------------
%\subsection*{Regular exponential family}
%%------------------------------------------------------------------------------------------------------
%
%A BN belongs to the linear exponential family if the natural parameter space $\Theta$ is an open and convex set. Additionally, a linear exponential family is said to be minimal if there is non-zero constant vector $\alpha$, such that $\alpha^Ts(\bm x)$ is equal to a constant for all $\bm x$.  We will consider the \textit{regular exponential family} (REF) to be the linear exponential one with minimal representation.
%
%
%The REF with a minimal representation has been widely studied in the literature. The distributions in this family present many useful properties. The following two properties are considered among the most relevant ones: i)  the transformation between $\theta$ and $\mu$ parameters is a one-to-one correspondence, i.e., $\mu$ is a dual set of the model parameter $\theta$; and ii) the moment parameters $\mu$ are equal to the gradient of the log-normalizer (the proof of this equality is included in Appendix \ref{appendix:regularEFequality}):
%
%\begin{equation}
%\label{Equation:RegularEFEquality}
%\mu = \frac{\partial A(\theta)}{\partial \theta}
%\end{equation}
%
%
%However, any BN which contains immoralities does not induce a linear exponential family (some parameters are restricted to (non)-linear constraints to ensure the conditional independence). Consequently, some of the nice properties of the REF are lost. In this case, we need to rely on a more general family called the \textit{curved exponential family} where the probability distribution can be more generally expressed as follows:
%
%\begin{equation*}
%p(\bm x |\theta) = h(x)~exp(\psi(\theta)^Ts(\bm x) - A(\theta) )
%\end{equation*}
%
%\noindent where $\psi$ is  a parameter transformation function. In our case, we will consider only the cases where $\psi$ is invertible. In this way, we could re-parametrize the distribution in a canonical form as shown in Equation \ref{Equation:EFCanonical}, where the natural parameter space is not any more an open convex set.
%
%
%\begin{exmp}
%
%A Bernoulli distribution can be represented as a curved exponential family as follows: 
%
%$$
%\ln p(x| \rho ) = 
%\begin{pmatrix}
%\ln \rho\\
%\ln (1-\rho)
%\end{pmatrix}^T
%\begin{pmatrix}
%I(x=0)\\
%I(x=1)
%\end{pmatrix}
%$$
%
%\noindent such that $\psi$ function is defined as follows: $\psi(\rho) = \big(\ln \rho, \ln (1-\rho) \big)$. Moreover, this distribution can be re-parametrized in cannonical form as follows: 
%
%$$
%\ln p(x| \theta ) = 
%\begin{pmatrix}
%\theta_1\\
%\theta_2
%\end{pmatrix}^T
%\begin{pmatrix}
%I(x=0)\\
%I(x=1)
%\end{pmatrix}
%$$
%
%\noindent where $\theta\in\Theta = \{ (\theta_1, \theta_2) \in \Re^2 : e^{\theta_1} + e^{\theta_2} = 1\}$.  As can be seen, $\Theta$ is not an open convex set in $\Re^2$. 
%
%\end{exmp}

%%------------------------------------------------------------------------------------------------------
%\subsection*{Conjugate Distributions} \label{sec:ConjuagateDist}
%%------------------------------------------------------------------------------------------------------
%
%A distribution $p(\theta|\alpha)$, parametrized by the parameter vector $\alpha$, is a conjugate prior of the distribution $p(\bm x|\theta)$ if it can be expressed in the following functional form, 
%\begin{equation*}
%p(\theta|\alpha) \propto exp(\langle s(\theta), \alpha \rangle -
%A_g(\alpha))
%\end{equation*}
%\noindent where the sufficient statistics of $\theta$ are expressed as follows
%$s(\theta)=(\theta,-A(\theta))$ and the parameter vector $\alpha$
%is defined by two components $(\bar{\alpha}, \nu)$ where $\nu$ is a positive
%scalar and $\bar{\alpha}$ is a vector also belonging to
%$\mathcal{S}$ \cite{bernardo2009bayesian}.  Then $p(\bm x|\theta)$ and $p(\theta|\alpha)$ forms an exponentially 
%
%%The second component $\nu$ is a positive scalar and  corresponds to the effective number of observations that the prior distribution contributes. And the first component $\bar{\alpha}$, also belonging to $\mathcal{S}$, determines how these pseudo-observations contribute to the sufficient statistics.
%
%Alternatively, we say that 
%
%
%
%
%
%
%
%If, moreover, if 
%the distributions of variables, conditioned on their parents, are drawn from the exponential family and are conjugate2 with respect to the distributions over these parent variables. A model where both of these constraints hold is known as a conjugate-exponential model.
%
%A conditional distribution $p(\bm X | \bm Y)$ is said to be \textit{conjugate} to a child distribution $p(\bm W| \bm X)$ if $p(\bm X|\bm Y)$ has the same functional form, with respect to $\bm X$, as $p(\bm W|\bm X)$. 


%\begin{equation}
%\label{eq:CE_F1}
%\ln p(\bm x | \bm y) =\theta(\bm y)^Ts(x) - A \big(\theta  (\bm y) \big)  + h(\bm x)
%\end{equation}
%\noindent where $\theta(\bm y)$ denotes that the natural parameters are now a function of $\bm y$.  %Equivalently, we can say that $p(X|Pa(X))$ is in the exponential family if for any assignment, $\bm \pi$  to the parents variables, $p(X|\bm\pi)$ is in exponential form. 


%------------------------------------------------------------------------------------------------------
\subsection{Conjugate exponential (CE) models} \label{sec:CondDist}
%------------------------------------------------------------------------------------------------------

A conditional distribution $p(\bm X | \bm Y)$ is  in the exponential family if, for any assignment $\bm y$ of the variables in $\bm Y$, the probability distribution $p(\bm X|\bm y)$ can be expressed in exponential form. Moreover,  $p(\bm X | \bm Y)$ is said to be \textit{conjugate} with respect to the distribution $p(\bm Y)$ if the latter has the same functional form than the posterior $p(\bm Y|\bm X)\propto p(\bm X|\bm Y)p(\bm Y)$. Then, the joint distribution $p(\bm X, \bm Y)= p(\bm X|\bm Y)p(\bm Y)$ is said to be a \textit{conjugate exponential family} (CE) model. 
% In fact, Equation \ref{eq:CE_F1} usually implies the existence a \textit{conjugate prior} $p(\bm Y)$ for $p(\bm X | \bm Y)$.  

An important property of CE models is that they are a multi-linear function of the sufficient statistic functions of $\bm X$ and $\bm Y$ [?]. This implies that we can alternatively express the conditional distribution $p(\bm X|\bm Y)$ in any of the three following functional forms 

\begin{description}

\item[Conditional form (C-form):]
\begin{equation}
\label{Equation:EqCED}
\ln p_\theta(\bm x | \bm y) = \theta_c(\bm y)^Ts(\bm x) - A_c(\theta(\bm x)) + h(\bm x) 
\end{equation}

\item[Posterior form (P-form):]
\begin{equation}
\label{Equation:EqCED}
\ln p_\theta(\bm x | \bm y) = \theta_p(\bm x)^Ts(\bm y) - A_p(\theta(\bm y)) + h(\bm x) 
\end{equation}

\item[Full form (F-form):]
\begin{equation}
\label{Equation:EqCED}
\ln p_\theta(\bm x | \bm y) = \theta_f^Ts(\bm x,\bm y) - A_f(\theta) + h(\bm x) 
\end{equation}

\end{description}

\noindent where the suffixes for $\theta$ and $A$ denote that both terms vary from one form to another. For example, in the first and second functional form, $\theta_c(\bm y)$ and $\theta_p(\bm x)$ denote that the natural parameters are now a function of $\bm y$ and $\bm x$, respectively, which is not the case for the last functional form. 

In Table ? we list the main conjugate exponential distributions. We point out that many of these pairs are usually omitted in the literature because they are usually consider under Bayesian learning settings. For example, as can be seen this table, every distribution conditioned to a set of multinomial variables defined a conjugate exponential pair. 


%A BN is said to be a conjugate exponential family (CEF) model if all its CPDs belong to the exponential family and are conjugate as well. Importantly enough, the use of conjugate distributions allows that the posterior for each distribution has the same form as the prior, which means that only the parameter values change, but the functional form of the distribution remains the same.

%------------------------------------------------------------------------------------------------------
\section{Bayesian networks as conjugate exponential models}\label{Section:CEFBN}
%------------------------------------------------------------------------------------------------------

In this section we how Bayesian networks can be compactly represented when they define conjugate exponential models. We also show how the transformation between natural and moment parameters also enjoys some relevant properties which allow to decompose and simplify this transformation. 

A Bayesian network (BN) defines a joint distribution $p_\theta(X_1,\ldots,X_n)$ over a set of variables in the following form:

$$ p_\theta(\bm X) = \prod_{i=1}^N p_{\theta}(X_i|Pa(X_i))$$ 

\noindent where $Pa(X_i)\subset \bm X\setminus X_i$ represents the so-called \emph{parent variables} of $X_i$ and $p_{\theta}(X_i|Pa(X_i))$ denotes the local conditional probability of $X_i$ given its parents $Pa(X_i)$. BNs can be graphically represented by a directed acyclic graph (DAG). Each node, labelled $X_i$ in the graph, is associated with a factor or conditional probability $p_\theta(X_i|Pa(X_i))$. Additionally, for each parent $X_j \in Pa(X_i)$, the graph contains one directed edge pointing from $X_j$ to the \emph{child} variable $X_i$.

In our case, we also restrict ourselves to Bayesian networks models satisfying the so-called \emph{global independence parameter} property \cite{heckerman1995learning}. Under this assumption, the parameters defining each local conditional probability are independent between them. This assumption is highlighted by denoting by $\theta_i$ the parameters defining the local conditional probability $p_{\theta_i}(X_i|Pa(X_i))$. We also denote by $\Theta_i$ to the space of the parameters $\theta_i$.

Finally, we introduce the definition of a conjugate exponential Bayesian network (ce-BN),

\begin{mydef}
A BN is said to be a conjugate exponential model if the probability distribution of $X_i$ given its parents, $p(X_i|Pa(X_i))$, is conjugate with respect to the distribution of their parents, for all the variables in the model.  
\end{mydef}

So, the following developments applies to ce-BNs which satisfy the \emph{global independence parameter property}.
%Importantly enough, the use of conjugate distributions allows that the posterior for each distribution has the same form as the prior, which means that only the parameter values change, but the functional form of the distribution remains the same.

%------------------------------------------------------------------------------------------------------
\subsection{Representation} \label{Section:CEFBN:Representation}
%------------------------------------------------------------------------------------------------------

As we show in the next theorem, we can exploit the \textit{F-form} of a conjugate model to obtain a compact representation of a Bayesian network as an exponential family model. 

\begin{theorem}\label{thm:representation}
A ce-BN can be represented in exponential form in the following structured way, by composing the F-form representations of its conditional probability distributions, as follows 

\begin{itemize}
\item the natural parameters  $\theta$ of a ce-BN are formed by the composition of the local natural parameters of each conditional distribution, 

$$\theta = (\theta_1, \ldots, \theta_n)$$

\noindent where $\theta_i$ corresponds to natural parameters of the conditional distribution of $X_i$ expressed in F-form.

\item the sufficient statistics $s(X_1,\ldots,X_n)$ of a ce-BN are formed the composition of the local sufficient statistics of each conditional distribution, 

$$s(X_1,\ldots,X_n) = (s_1(X_1,Pa(X_1)), \ldots, s_n(X_n,Pa(X_n))$$

\noindent where $s_i(X_i,Pa(X_i))$ corresponds to sufficient statistics of the conditional distribution of $X_i$ expressed in F-form.

\item the log-normalizer $A(\theta)$ of a ce-BN is the sum of the local conditional log-normalizer of each conditional distribution,


$$ A(\theta) = \sum_{i=1}^n B_i(\theta_i) $$

\noindent where $B_i(\theta_i)$ corresponds to \textit{conditional log-normalizer} of the conditional distribution of $X_i$ expressed in F-form.

\end{itemize}

\end{theorem}
\begin{proof}
By using Equation \ref{Equation:EqCED}, a ce-BN can be represented in the following way: 

\begin{eqnarray}
\label{Equation:CEFSS}
\ln p(X_1,\ldots, X_n) &=& \sum_{i=1}^n \ln p(X_i|Pa(X_i))\nonumber\\
&=& \sum_{i=1}^n \theta_i \big(Pa(X_i)\big)^T s_i(X_i) - A_i\Big(\theta\big(Pa(X_i)\big)\Big)\nonumber\\
&=& \sum_{i=1}^n \theta_i^T~s_i\big(X_i, Pa(X_i)\big) - B_i(\theta_i)\nonumber\\
&=&
\begin{pmatrix}
\theta_1\\
\ldots \\
\theta_n\\
\end{pmatrix}^T
\begin{pmatrix}
s_1\big(X_1,Pa(X_1)\big) \\
\ldots \\
s_n\big(X_n,Pa(X_n)\big) \\
\end{pmatrix}
- \sum_{i=1}^n B_i(\theta_i)
\end{eqnarray}
\end{proof}

Hence, in order to represent a BN as a CEF model, we only have to worry about the local representation of each CPD as in Equation \ref{Equation:CEFSS}. The global representation is then just obtained by composing all these local representations. Let us notice that without the assumption of conjugacy  for the conditional exponential distributions, the above representation would have not been possible. 


%------------------------------------------------------------------------------------------------------
\subsection{From natural to moment parameters} \label{Section:CEFBN:NaturalToMoment}
%------------------------------------------------------------------------------------------------------

In this section, we examine the transformation from natural to moment parameters in a ce-BN, which is stated in the following theorem. 

\begin{theorem}
\label{thm:naturalToMoment}
The moment parameter vector of a ce-BN associated to a vector of natural parameter $\theta$ locally decomposes into local moment parameters as follows,

$$\bm \mu = (\mu_1,\ldots,\mu_n)$$

\noindent where $\mu_i= \int s_i(X_i,Pa(X_i))~p_\theta(X_i,Pa(X_i))d\bm X$ is the local moment vector associated to the local sufficient statistic $s_i(X_i,Pa(X_i))$, as defined in Theorem \ref{thm:representation},  and the local marginal probability $p(X_i,Pa(X_i))$.
\end{theorem}
\begin{proof}
By definition, $\mu = \e{s(X_1,\ldots,X_n)|\theta} = \int s(X_1,\ldots, X_n)~p_\theta(X_1,\ldots, X_n)~d\bm X$. According to Theorem \ref{thm:representation}  $s(X_1,\ldots, X_n)$ locally decomposed in a set of local sufficient statistics $s_i(X_i,Pa(X_i))$. Using this decomposition we arrived to the decomposition of the vector of moment parameters $\bm \mu$ as follow,

\begin{equation*}
\begin{array}{lll}
\mu & = & \e{s(X_1,\ldots, X_n)|\theta} \\
&=& \int s(X_1,\ldots, X_n)~p_\theta(X_1,\ldots, X_n)~d\bm X\\
&=& \int \big(s_1(X_1,Pa(X_1)),\ldots, s_n(X_n,Pa(X_n))\big)~p_\theta(X_1,\ldots, X_n)~d\bm X\\
&=&  \Big(\int s_1(X_1,Pa(X_1)) p_\theta(X_1,Pa(X_1))~d(X_1,Pa(X_1)) ,\ldots, \\ 
& &   \int s_n(X_1,Pa(X_n)) p_\theta(X_n,Pa(X_n))~d(X_n,Pa(X_n))\Big)\\
&=& (\mu_1,\ldots,\mu_n) \\
\end{array}
\end{equation*}
\end{proof}


Note here that in order to compute the local moment parameter $\mu_i$ we need to obtain the local marginal probability $p_\theta (X_i,Pa(X_i))$, which requires running inference over the whole BN.

%------------------------------------------------------------------------------------------------------
\subsection{From moment to natural Parameters} \label{Section:CEFBN:MomentToNatural}
%------------------------------------------------------------------------------------------------------

As shown in Equation \ref{Equation:MomentToNatural}, the transformation from moment to natural parameters involves solving an optimization problem. In the following theorem we show that for ce-BNs this problem decomposes into a set of simpler and local optimization problems, 

\begin{theorem}
\label{thm:MomentToNatural}
Given a moment parameter vector $\bm \mu$, the associated natural parameter vector, denoted by $\theta(\bm \mu)$, of a ce-BN locally decomposes into local natural parameters as follows,

$$ \theta(\mu) = \big(\theta_1(\mu_1), \ldots, \theta_n(\mu_n)\big)$$

\noindent where each $\theta_i(\mu_i)$ is the solution of the following optimization problem, 

\begin{eqnarray}
\label{Equation:CEFBN_MomentToNaturalLocal}
\theta_i(\mu_i) = \arg\max_{\theta_i\in\Theta_i} \theta_i^T\mu_i - B(\theta_i)
\end{eqnarray}

\end{theorem}
\begin{proof}
By definition, 
\begin{eqnarray*}
\label{Equation:CEFBN_MomentToNatural}
\theta(\mu) &=& \arg\max_{\theta\in\Theta} \theta^T\mu
-A(\theta)\nonumber \\
&=& \arg\max_{(\theta_1,\ldots, \theta_n) \in\Theta} \sum_{i=1}^n \theta_i^T \mu_i - B_i(\theta_i) 
\end{eqnarray*}

According to the \emph{global independence parameter} assumption, the $\theta_i$ parameters are independent and, then, the above maximization problem fully decomposes into local maximization problems, one for each conditional probability distribution,
\begin{eqnarray*}
\label{Equation:CEFBN_MomentToNaturalLocal}
\theta_i(\mu_i) = \arg\max_{\theta_i\in\Theta_i} \theta_i^T\mu_i - B(\theta_i)
\end{eqnarray*}

\noindent and the global solution is just the aggregation of the local solutions.

% as follows: 
%$$ \theta(\mu) = \big(\theta_1(\mu_1), \ldots, \theta_n(\mu_n)\big)$$

\end{proof}

Let us note that, as opposed to the previous case, the transformation from moment to natural parameters can be performed locally at each conditional distribution, and as stated above, the global solution is just an aggregation of the local solutions.


%------------------------------------------------------------------------------------------------------
%\subsection{Coding ce-BNs} \label{Section:CEFBN:Coding}
%------------------------------------------------------------------------------------------------------


%------------------------------------------------------------------------------------------------------
\section{Conditional distributions with multinomial parents in ce-BNs}\label{Section:CD_With_MParents}
%------------------------------------------------------------------------------------------------------

In many real cases, BNs contain conditional distributions with multinomial parents. In this section, we show that this conditional probability distributions has some particular inner structure that can be exploited when representing them in  exponential form and, also, when performing the associated parameter transformations.  

Let $(\bm Z, \bm Y)$ be the set of parents of a variable $X$ such that $\mathbf{Y}$ denotes a set of multinomial variables and $\bm Z$ denotes set of non-multinomial variables\footnote{$\bm Z$ can be empty.}.  Let $q$ denote the total number of parental configurations for the variables in $\bm Y$, and $\mathbf{y}^l$ denote the $l$-th parental configuration, such that $1 \leq l \leq q$. Let us denote by $\theta_X$ the parameters defining the conditional probability of $X$ given $\bm Z$ and $\bm Y$,  $p_{\theta_X}(X \mid \bm Z, \bm Y)$.  In our case, we restrict ourselves to those BNs which satisfy the so-called \emph{local independence parameter} property \cite{heckerman1995learning}. Under this assumption, the conditional probability of $X_i$ conditioned to each parental configuration $\mathbf{y}^l$, $p_{\theta_{X,l}}(X \mid \bm Z, \bm Y = \bm y^l)$,  is defined by parameter vector $\theta_{X,l}$ and these parameter vectors independent among them. 

In this section we show that if we know how to represent in a exponential form a given distribution (i.e., a Poisson, a Normal, a Multinomial, or a Normal distribution with Normal parents, etc.), then we can directly derive the exponential representation of the corresponding distribution conditioned to multinomial parents (i.e. Poisson given Multinomial parents , Normal given Multinomial parents, Multinomial given Multinomial parents, or Normal given Normal and Multinomial parents, etc.).  Similarly, we also show that for these conditional distributions the transformation from moment to natural parameters can be further decomposed. 

%------------------------------------------------------------------------------------------------------
\subsection{Representation} \label{Section:CD_With_MParents:Representation}
%------------------------------------------------------------------------------------------------------


Next theorem shows how a conditional distribution with multinomial parents in a ce-BN is represented in \textit{F-form}, 

\begin{theorem}\label{thm:representation:multiparents}

The conditional probability distribution of variable $X$ with multinomial parents $\bm Y$ and non-multinomial parents $\bm Z$ can be represented in exponential \textit{F-form} as follows

\begin{itemize}
\item the natural parameters  in F-form $\theta_f$ are formed by the composition of the local natural parameters in F-form of the conditional distribution restricted to each parental configuration and their log-conditional normalizers, 

$$\theta_f = \big(\theta_{1},\ldots,\theta_{q},-B_1(\theta_{1}), \ldots,-B_q(\theta_{q})\big)$$

\noindent where $\theta_l$ and $B_l(\theta_l)$ are given by F-form representation of the local conditional distribution $p(X | \bm Z, \bm Y = \mathbf{y}^l)$. 


\item the sufficient statistics in F-form $s(X,\bm Z, \bm Y)$ are similarly expressed as follows, 

$$
s(X,\bm Z, \bm Y) = 
\begin{pmatrix}
s_1(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s_q(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)\\
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
$$

%$$ s(X,\bm Z, \bm Y) = \big(s_1(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^1),\ldots, s_q(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^q),I(\mathbf{Y} =\mathbf{y}^1),\ldots,I(\mathbf{Y} =\mathbf{y}^q)\big)$$

\noindent where $s_l(X, \bm Z)$ corresponds to sufficient statistics in F-form for the conditional distribution $p(X | \bm Z, \bm Y = \mathbf{y}^l)$.

\item the log-normalizer in F-form $A_f(\theta)$ is the null function.

\end{itemize}

\end{theorem}
\begin{proof}

The log-conditional probability of $X$ given its parent-nodes $\bm Z$ and $\mathbf{Y}$ decomposes as  follows:
\begin{eqnarray*}
\ln p(X \mid \bm Z, \bm Y) &=&  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p(X | \bm Z, \mathbf{y}^l) \\
&=& \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{l}^T s_l(X, \bm Z)  -  B_l(\theta_{l}) \Big)\\
&=& \sum_{l=1}^q \theta_{l}^T  I(\mathbf{Y} =\mathbf{y}^l) s(X, \bm Z) - \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) B_l(\theta_{l})
\end{eqnarray*}

\noindent where $\theta_l$, $s_l(X,\bm Z)$ and $B_l(\theta_l)$ are provided when the local conditional distribution $p(X | \bm Z, \mathbf{y}^l)$ is expressed in exponential form. So, the conditional distribution $p(X \mid \bm Z, \bm Y)$ can be written in exponential form as follows: 
\begin{eqnarray}
\label{Equation:CD_With_MParents:Representation}
\ln p(X \mid \bm Z, \bm Y)  &=& \theta^T s(X,\mathbf{Y}) - B(\theta) \nonumber \\
&=&
\begin{pmatrix}
- B_1(\theta_{1}) \\
\vdots \\
- B_q(\theta_{q}) \\
\theta_{1} \\
\vdots \\
\theta_{q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s_1(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s_q(X, \bm Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray}
\end{proof}

%The corresponding proof can be found in Appendix \ref{appendix:CD_With_MParents:Representation}.

As can be seen, the exponential representation of a conditional probability with multinomial parents can be expressed as the composition of the exponential representation of the conditional distributions restricted to each one of the possible configurations of the multinomial parents. 

In Appendix \ref{appendix:CD_With_MParents:Representation}, we also show how the C-form and P-form representations of the conditional distribution of $p(X_i|\bm Z, \bm Y)$ can be equally expressed as the composition of the respective C-form and P-form representation of its local distributions. 


%------------------------------------------------------------------------------------------------------
\subsection{From natural to moment parameters} \label{Section:CD_With_MParents:NaturalToMoment}
%------------------------------------------------------------------------------------------------------

According to Theorem \ref{thm:naturalToMoment}, the moment parameter vector decomposed for each variable $X_i$. In the next theorem we show who the local moment parameter $\mu_i$ further decomposes if the conditional probability has a set of multinomial parents by exploiting the how the sufficient statistics vector in F-form decomposes for this kind of conditional probability as shown in Theorem \ref{thm:representation:multiparents}.

\begin{theorem}
\label{thm:naturalToMoment:multiparents}

In a ce-BN,  the moment parameter vector $\mu_i$ associated to a variable $X_i$ with multinomial parents $\bm Y$ and non-multinomial parents $\bm Z$ decomposes into local moment parameters associated to each of the configurations of the variables in $\bm Y$ as follows,
$$
\mu_i =  
\begin{pmatrix}
\lambda_{i,1}\cdot \mu_{i,l}\\
\vdots \\
\lambda_{i,q}\cdot \mu_{i,q}\\
\lambda_{i,1} \\
\vdots \\
\lambda_{i,q}
\end{pmatrix}
$$
\noindent where  $\lambda_{i,l}$ is the $l$-th component (i.e., a scalar) of the moment vector associated to the marginal distribution $p_\theta(\bm Y)$,  $\lambda_{i,l} = \int I(\bm Y = \bm y_l) p_\theta(\bm Y) d\bm Y= p_\theta(\bm y_l)$, and $\mu_{i,l}$  is the ``local'' moment parameter associated to the marginal distribution $p(X,\bm Z| \bm y = l)$, $\mu_{i,l}= \int s_l(X, \bm Z)p_\theta(X,\bm Z|\bm y = l) dX\bm Z$. 

\end{theorem}
\begin{proof}
The proof easily follows by using the decomposition of the sufficient statistics vector of this conditional probability distribution as shown in Theorem \ref{thm:representation:multiparents}.
\end{proof}

Again, we can see that the moment parameters of this conditional distribution can be expressed as a composition of the local moment parameters of each of the distributions conditioned to each configuration of the multinomial parents variables. 

%------------------------------------------------------------------------------------------------------
\subsection{From moment to natural parameters} \label{Section:CD_With_MParents:MomentToNatural}
%------------------------------------------------------------------------------------------------------


In Theorem \ref{thm:MomentToNatural}, we saw that transforming moment to natural parameters reduces to local transformations between the local moment parameters $\mu_i$ and the local natural parameters $\theta_i$ by solving the following optimization problem: $\theta_i(\mu_i) = \arg\max_{\theta_i\in\Theta_i} \theta_i^T\mu_i - B(\theta_i)$. Next theorem shows how this optimization problem further simplifies when $X_i$ is conditioned to a set of multinomial parents. 

\begin{theorem}
\label{thm:MomentToNatural:multiparents}
Given a local moment parameter vector $\bm \mu_i$ associated to a variable $X_i$ with multinomial parents $\bm Y$ and non-multinomial parents $\bm Z$, the associated natural parameter vector $\theta_i(\bm \mu_i)$ decomposes as follows,
$$ \theta_i(\mu_i) = \big(\theta_{i,1}(\mu'_{i,1}), \ldots, \theta_{i,q}(\mu'_{i,q}), -A_i(\theta_{i,1}(\mu'_{i,1})), \ldots,-A_i(\theta_{i,q}(\mu'_{i,q}))\big)$$

\noindent where, as a mentioned earlier, $\theta_{i,l}$ denotes the parameter sub-vector associated to the F-form representation of the conditional distribution $p(X|\bm Z,\bm Y  =\bm y_l)$ and where $\mu'_{i,l} =\frac{1}{\lambda_{i,l}}\mu_{i,l}$, i.e. the element-wise division of the vector $\mu_{i,l}$ by the scalar $\lambda_{i,l}$, which was defined in Theorem \ref{thm:naturalToMoment:multiparents}. 

Similarly, the $\theta_{i,l}(\mu'_{i,l})$ parameter is obtained by solving the following optimization problem, 
\begin{eqnarray*}
\theta_{i,l}(\mu'_{i,l}) = \arg\max_{\theta_{i,l} \in \Theta_{i,l}} \theta_{i,l}^T \mu'_{i,l}- A_i(\theta_{i,l})
\end{eqnarray*}

\noindent  So, this optimization problem simply corresponds to the transformation of a moment parameter vector $\mu'_{i,l}$ to their corresponding natural parameters for the conditional distribution $p(X|\bm Z, \bm y_l)$.
\end{theorem}
\begin{proof}
The optimization problem to solve can be stated as follows by using the decomposition of the natural parameters, of the sufficient statistics, and of the log-normalizer as stated in Theorem \ref{thm:representation:multiparents}, 
\begin{eqnarray*}
\arg\max_{(\theta_{i,1},\ldots, \theta_{i,q})} \sum_{l=1}^q \theta_{i,l}^T \mu_{i,l}- \mu_l A_i(\theta_{i,l})
\end{eqnarray*}

Under the \textit{local parameters independence assumption}, the above optimization problem decomposes in a set of $q$ independent optimization problems, 
\begin{eqnarray*}
\arg\max_{\theta_{i,l} \in \Theta_{i,l}} \theta_{i,l}^T \mu_{i,l}- \mu_l A_i(\theta_{i,l})
\end{eqnarray*}

The solution of each of above optimization problem is not affected if the optimized expression is divided by the scalar $\lambda_{i,l}$ \footnote{If $\lambda_{i,l}=0$, it would imply that $p(\bm Y= \bm y_t)=0$, so it does not make sense to solve the problem.}. So we arrived to the claim of the theorem. 
\end{proof}


%------------------------------------------------------------------------------------------------------
\section{Inference with conjugate exponential Bayesian networks}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\subsection{Variational Message Passing}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\subsection{Expectation propagation}
%------------------------------------------------------------------------------------------------------



%------------------------------------------------------------------------------------------------------
\section{Learning from data with conjugate exponential Bayesian networks}
%------------------------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------------------
\subsection{Maximum likelihood}
%------------------------------------------------------------------------------------------------------

In this section we look at the maximum likelihood problem in ce-BNs. Our aim is to show how the solution to this multi-dimensional optimization problem has a common characterization in terms of exponential family representation and, moreover, boils down to smaller local problems for each conditional distribution by relying on the results presented in the previous sections. 

\begin{theorem}

Given a set of fully observed i.i.d. data samples $D=\{\bm x^{(1)}, \ldots, \bm x^{(m)}\}$ indexed by $j$, the maximum likelihood estimator of ce-BN is defined as follows $\theta^\star=\Big(\theta^\star_1,\ldots,\theta^\star_n\Big)$, where each $\theta^\star_i$ is computed according to following moment to natural parameter transformation, 

$$\theta^\star_i= \theta_i\big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i)\big)$$

\noindent where $\theta_i\big(\frac{1}{m}\sum_{j=1}^m s(x_i^{(j)},\bm{pa}^{(j)}_i)\big)$ is defined as shown in Theorem \ref{thm:MomentToNatural}.  

If the variable $X_i$ has a non-empty set of multinomial parents $\bf Y$ and a set of non-multinomial parents $\bf Z$, then  $\theta_i^\star$ would also decomposes according to Theorem \ref{thm:MomentToNatural:multiparents} and using the following equality,


$$\mu'_{i,l} = \frac{\sum_{j=1}^m s_l(X, \bm Z)}{\sum_{j=1}^m  I(\mathbf{Y} =\mathbf{y}^l)} $$

\end{theorem}
%%%%%%%%%%%%%%%%%
\begin{proof}
Given a set of fully observed data samples $D=\{\bm x^{(1)}, \ldots, \bm x^{(m)}\}$ indexed by $j$. The maximum likelihood problem can be stated as follows by using the F-form of the ce-BN, 
\begin{eqnarray*}
\theta^\star  &=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \ln p(\bm x^{(j)}|\theta) \\
&=& \arg\max_{\theta \in \Theta} \sum_{j=1}^m \theta^Ts(\bm x^{(j)})  - A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\sum_{j=1}^m s(\bm x^{(j)})\Big)  - m A(\theta) \\
&=& \arg\max_{\theta \in \Theta} \theta^T\Big(\frac{1}{m}\sum_{j=1}^m s(\bm x^{(j)})\Big)  - A(\theta) \\
\end{eqnarray*}
\noindent where the last part is achieved by dividing the optimized equation by the number of samples $m$, what does not affect the result of the optimization. 
As widely known, it can be seen that the maximum likelihood is equivalent to a transformation form moment to natural parameters as stated in Equation \ref{Equation:MomentToNatural}. Then, the first claim of this theorem directly derive from the application of Theorem \ref{thm:MomentToNatural}. 
When there are variables with multinomial parents, the second claim easily follows by the application of Theorem \ref{thm:representation:multiparents} and \ref{thm:naturalToMoment:multiparents}.
\end{proof}
%%%%%%%%%%%%%%%%%

%------------------------------------------------------------------------------------------------------
\subsection{EM algorithm}
%------------------------------------------------------------------------------------------------------


%------------------------------------------------------------------------------------------------------
\subsection{Bayesian Learning}
%------------------------------------------------------------------------------------------------------


%--------------------------------------------------------------------------------------------------------------------------------------------
\begin{appendices}
%--------------------------------------------------------------------------------------------------------------------------------------------



%------------------------------------------------------------------------------------------------------
\section{Regular EF: moment parameters equal the gradient of the log-normalizer}\label{appendix:regularEFequality}
%------------------------------------------------------------------------------------------------------


%------------------------------------------------------------------------------------------------------
\section{Conditional distributions with multinomial parents: proof of EF representation}\label{appendix:CD_With_MParents:Representation}
%------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------
\section{EF representation: A binary child given a binary parent}
%------------------------------------------------------------------------------------------------------

Let $X$ and $Y$ be two binary variables. The log-conditional probability of the child-node $X$ given its parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(X= x^1) I(Y= y^1) \ln p_{x^1 \mid y^1} + I(X=x^2) I(Y= y^1) \ln p_{x^2 \mid y^1} \\
+ I(X=x^1) I(Y= y^2) \ln p_{x^1 \mid y^2} + I(X=x^2) I(Y= y^2) \ln p_{x^2 \mid y^2}
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{F-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
\ln p_{x^1 \mid y^1}\\
\ln p_{x^2 \mid y^1}\\
\ln p_{x^1 \mid y^2}\\
\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
\theta_{11}\\
\theta_{21}\\
\theta_{12}\\
\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(Y=y^1) \\
I(X=x^2)I(Y=y^1) \\
I(X=x^1)I(Y=y^2) \\
I(X=x^2)I(Y=y^2) 
\end{pmatrix}
- 0
\end{eqnarray*}

\item \textbf{C-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(Y) \\
&=&
\begin{pmatrix}
I(Y=y^1)\ln p_{x^1 \mid y^1}  + I(Y=y^2)\ln p_{x^1 \mid y^2}\\
I(Y=y^1)\ln p_{x^2 \mid y^1}  + I(Y=y^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 \\
&=&
\begin{pmatrix}
m^Y_1\cdot\theta_{11}  + m^Y_2\cdot\theta_{12}\\
m^Y_1\cdot\theta_{21}  + m^Y_2\cdot\theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
I(X=x^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{P-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
I(X=x^1)\ln p_{x^1 \mid y^1}  + I(X=x^2)\ln p_{x^2 \mid y^1}\\
I(X=x^1)\ln p_{x^1 \mid y^2}  + I(X=x^2)\ln p_{x^2 \mid y^2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0\\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  m^X_2\cdot \theta_{21}\\
m^X_1 \cdot \theta_{12}  + m^X_2 \cdot \theta_{22}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A multinomial child given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a multinomial variable with $k$ possible values such that $k \geq 2$, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

$$ \ln p(X \mid \mathbf{Y}) = \sum_{j=1}^k \sum_{l=1}^q I(X=x^j) I(\mathbf{Y} =\mathbf{y}^l) \ln p_{x^j  \mid \mathbf{y}^l} $$

Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}

\item \textbf{P-form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) \\\\
&=&
\begin{pmatrix}
\ln p_{x^1\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^1}\\
\vdots \\
\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1)I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k)I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0 \\\\
&=&
\begin{pmatrix}
\theta_{11}\\
\vdots \\
\theta_{1q}\\
\vdots \\
\theta_{k1}\\
\vdots \\
\theta_{kq}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^1) I(\mathbf{Y}=\mathbf{y}^q)\\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(X=x^k) I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\vspace{0.5in}
\item \textbf{C-form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(\mathbf{Y})^Ts(X) - A(\mathbf{Y}) \\ \\
&=&
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^1\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^1\mid \mathbf{y}^q}\\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^1) \ln p_{x^k\mid \mathbf{y}^1} + \ldots + I(\mathbf{Y}=\mathbf{y}^q)\ln p_{x^k\mid \mathbf{y}^q}\\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}
- 0 \\ \\
&=&
\begin{pmatrix}
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{11}  + m^{\mathbf{Y}}_q \cdot \theta_{1q} \\
\vdots \\
\mathbf{m}^{\mathbf{Y}}_1 \cdot \theta_{k1}  + m^{\mathbf{Y}}_q \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k)
\end{pmatrix}
- 0 
\end{eqnarray*}

\noindent such that $\mathbf{m}^{\mathbf{Y}}_1 = \prod_{i=1}^n I( Y_i = y_i^1) = \prod_{i=1}^n m^{Y_i}_1$ denotes the expected sufficient statistics for the first parental configuration, and $\mathbf{m}^{\mathbf{Y}}_q = \prod_{i=1}^n I( Y_i = y_i^{r_i})  = \prod_{i=1}^n m^{Y_i}_{r_i} $ denotes the expected sufficient statistics for the last parental configuration.

\vspace{0.5in}
\item \textbf{P-form}:

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\ \\
&=&
\begin{pmatrix}
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^1}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^1} \\
\vdots \\
I(X=x^1)  \ln p_{x^1\mid \mathbf{y}^q}  + \ldots + I(X=x^k)  \ln p_{x^k\mid \mathbf{y}^q}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0\\ \\
&=&
\begin{pmatrix}
m^X_1 \cdot \theta_{11}  +  \ldots + m^X_k \cdot \theta_{k1}\\
\vdots \\
m^X_1 \cdot \theta_{1q}   + \ldots + m^X_k \cdot \theta_{kq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y}=\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y}=\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&= &
\begin{pmatrix}
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^1\mid \mathbf{y'}^1}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^1) \ln p_{x^k\mid \mathbf{y'}^1}  \! \\
\vdots \\
\! m^X_1 I(\mathbf{Y'} =\mathbf{y'}^{q'})  \ln p_{x^1\mid \mathbf{y'}^{q'}}  + \ldots + m^X_k I(\mathbf{Y'} =\mathbf{y'}^{q'}) \ln p_{x^k\mid \mathbf{y'}^{q'}}\! 
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
\! - 0 \! \\ \\
&=&
\begin{pmatrix}
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{11}  +  \ldots + m^X_k \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{k1}\\
\vdots \\
\! m^X_1 \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{1q'}   + \ldots + m^X_k \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{kq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\noindent where $\mathbf{m}^{\mathbf{Y'}}_1 =  I(\mathbf{Y'} =\mathbf{y'}^1) = I( Y_1 = y_1^1) \cdot \ldots I( Y_{i-1} = y_{i-1}^1) \cdot I( Y_{i+1}  = y_{i+1}^1) \cdot \ldots I( Y_{n}  = y_{n}^1)$ denotes the expected sufficient statistics for the first configuration of the parent set $\mathbf{Y'} = \mathbf{Y} \setminus Y_i$, and $\mathbf{m}^{\mathbf{Y'}}_{q'} = I(\mathbf{Y'} =\mathbf{y'}^{q'}) = I( Y_1 = y_1^{q'}) \cdot \ldots I( Y_{i-1} = y_{i-1}^{q'}) \cdot I( Y_{i+1}  = y_{i+1}^{q'}) \cdot \ldots I( Y_{n}  = y_{n}^{q'})$ denotes the expected sufficient statistics for the last configuration of the parent set $\mathbf{Y'}$, with $q' = q / r_i$ denotes the total number of configurations of the parent set $\mathbf{Y'}$.

\end{itemize}

\newpage

%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A multinomial child given a Dirichlet parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a multinomial variable with $k$ possible values such that $k \geq 2$, and let $\rho$ denote a dirichlet parents of $X$. %Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ and $\rho$ can be expressed as follows:

$$ \ln p(X \mid \mathbf{Y},\rho) = \sum_{j=1}^k I(X=x^j)  \ln p_{x^j} $$

Note that here all $ I(\cdot)$ and $p(\cdot)$ values must be taken from the moment parameters of these variables, not the natural parameters of $X$.

Similarly the above log-conditional probability can be expressed in the following exponential forms:

\begin{itemize}

\vspace{0.5in}
\item \textbf{C-form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(\rho)^Ts(X) - A(\mathbf{Y}) \\ \\
&=&
\begin{pmatrix}
\ln p_{x^1}\\
\vdots \\
\ln p_{x^k} \\
\end{pmatrix}^T
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}
- 0 \\ \\
\end{eqnarray*}


\vspace{0.5in}
\item \textbf{P-form}:

%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(X)^Ts(\rho) - A(X) \\ \\
&=&
\begin{pmatrix}
I(X=x^1) \\
\vdots \\
I(X=x^k) 
\end{pmatrix}^T
\begin{pmatrix}
\ln p_{x^1}\\
\vdots \\
\ln p_{x^k} \\
\end{pmatrix}
- 0 \\ \\
\end{eqnarray*}

\end{itemize}


\subsection{Dirichlet distribution}

Let $\rho$ be a dirichlet variable with parameters $\mathbf{u}, \mathbf{p}$, where $\mathbf{p}$ are the $k$ parameters of a multinomial distribution. The log-conditional probability of $\rho$ can be expressed as follows:

\begin{eqnarray*}
\ln p(\rho) &=& \ln \left( \frac{\Gamma ( \sum_{i=1}^k u_i )}{\prod_{i=1}^k \Gamma(u_i)} \prod_{i=1}^k p_i^{u_i-1} \right)\\\\
&=&
\begin{pmatrix}
u_1 - 1\\
\vdots\\
u_k -1
\end{pmatrix}^T
\begin{pmatrix}
\log{p_1} \\
\vdots\\
\log{p_k}
\end{pmatrix}
- \left( \sum_{i=1}^k \log \Gamma(u_i) - \log{\Gamma ( \sum_{i=1}^k u_i )} \right) + 0
\end{eqnarray*}

NB: More details to be found in the EF\_DirichletDistribution.pdf scanned file.

\newpage


%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A normal child given a set of normal parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a normal variable and $ \mathbf{Y} = \{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are normal. 

The log-conditional probability of $X$ given its parents $\mathbf{Y}$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n) &=& \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \me^{-\frac{(x-(\beta_0+ \bs \beta^T \cdot \bm Y))^2}{2\sigma^2}} \right)\\\\
&=&
- \ln{\sigma} - 0.5\ln{(2\pi)} - \frac{(x-\beta_0 - \bs \beta^T \mathbf{Y})^2}{2\sigma^2}
\end{eqnarray*}

where

\begin{tabular}{p{5.5cm}p{5.5cm}}
\begin{eqnarray*}
\bs \beta &=& 
\begin{pmatrix}
\beta_1\\
\vdots\\
\beta_n\\
\end{pmatrix}
\end{eqnarray*}
&
\begin{eqnarray*}
\bm Y &=& 
\begin{pmatrix}
Y_1\\
\vdots\\
Y_n\\
\end{pmatrix}
\end{eqnarray*}
\\
\end{tabular}

Similarly the above log-conditional probability can be expressed in the following exponential forms:


\newcommand{\Z}{\bm Z}
\newcommand{\Y}{\bm Y}
\newcommand{\thb}{\bs \theta}
\newcommand{\beb}{\bs \beta}

\begin{itemize}
\item \textbf{F-form - Joint suff. stat. (Maxim. Likelihood, matrix representation)}:

$$\bm Z = ( X, \bm Y)$$
\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta^T s(X,\mathbf{Y}) - A(\theta) + h(\mathbf{X})\\\\
&=&
\begin{pmatrix}
\thb_1 \\
\thb_2 \\
\end{pmatrix}^T
\begin{pmatrix}
\Z   \\
\Z^T\Z   \\
\end{pmatrix}
- \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}



%where $\mu_{X|\bm Y} = \beta_0+ \bs \beta^T \cdot \bm Y$

\begin{eqnarray*}
\Z &=& 
\begin{pmatrix}
X & Y
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\Z^T\Z &=& 
\begin{pmatrix}
XX^T   & X\Y \\
\Y X^T & Y \Y^T
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\thb_1 &=& 
\begin{pmatrix}
\beta_0\sigma^{-2} & -\beta_0\beb\sigma^{-2}
\end{pmatrix}
=
\begin{pmatrix}
\theta_{\beta_0} & \thb_{\beta_0\beb}
\end{pmatrix}
=\beta_0\sigma^{-2}
\begin{pmatrix}
1 & -\beb
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\thb_2 &=& 
\begin{pmatrix}
-0.5\sigma^{-2}   &  \beb 0.5\sigma^{-2}\\
\beb 0.5\sigma^{-2}   & -\beb \beb^T 0.5\sigma^{-2}
\end{pmatrix}
= 
\begin{pmatrix}
\theta_{\mbox{-}1}   &  \thb_{\beb} \\
\thb_{\beb}^T   &  \thb_{\beb\beb}
\end{pmatrix}
= 0.5\sigma^{-2}
\begin{pmatrix}
-1   &  \beb \\
\beb    & -\beb \beb^T 
\end{pmatrix}
\end{eqnarray*}

\begin{itemize}
\item \textbf{From moment to natural parameters: }





\begin{itemize}


\item FIRST STEP: 
\begin{eqnarray*}
\mu_X &=& E(X)\\
\mu_\mathbf{Y} &=& \E(\Y)\\
\Sigma_{XX} &=&  \E(XX^T) - \E(X)\E(X)^T\\
\Sigma_{\mathbf{YY}} &=&  \E(\Y\Y^T) - \E(\Y)\E(\Y)^T \\
\Sigma_{X\mathbf{Y}} &=&  \E(X \Y) - \E(X)\E(\Y)^T\\
\Sigma_{\mathbf{Y}X} &=&  \E(\Y X^T) - \E(\Y)\E(X)
\end{eqnarray*}

\item SECOND STEP (Theorem 7.4 in page 253, Koller \& Friedman):
\begin{eqnarray*}
\beta_0 &=& \mu_X - \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}}\mu_\mathbf{Y}\\
\beb   &=& \Sigma_{X\mathbf{Y}}\Sigma^{-1}_{\mathbf{YY}} \\
\sigma^2 &=& \Sigma_{XX} - \Sigma_{XY}\Sigma^{-1}_{YY}\Sigma_{YX}
\end{eqnarray*}

All natural parameters $\theta$ can now be calculated considering these equations.
\end{itemize}

\item \textbf{From natural to moment parameters:}
Via inference.

\end{itemize}

\vspace{0.5in}
\item \textbf{C-form}:

\begin{eqnarray}\label{NormalGNomal2form}
\ln p(X\mid \mathbf{Y}) &=& \theta(\mathbf{Y})^T s(X) - A \big(\theta(\mathbf{Y})\big) + h(\mathbf{X})\\\nonumber\\
&=&
\begin{pmatrix}
\frac{\mu_{X|Y}}{\sigma^2} \nonumber\\
\frac{-1}{2\sigma^2} \nonumber\\
\end{pmatrix}^T
\begin{pmatrix}
X\\
X^2\\
\end{pmatrix}
- \left( \frac{\beta_0^2}{2\sigma^2} + \ln{\sigma}\right) - \tfrac{1}{2}\ln{(2\pi)} \nonumber\\\nonumber\\
&=&
\begin{pmatrix}
\theta_{\beta_0}+2\thb_{\beb} \Y \nonumber\\
\theta_{\mbox{-}1} \nonumber\\
\end{pmatrix}^T
\begin{pmatrix}
X \nonumber\\
X^2 \nonumber\\
\end{pmatrix}\nonumber\\
%- \left(\frac{\ln{(2\theta_{\mbox{-}1}})}{2}-\theta_{\mbox{-}1}\left(\theta_{\beta_0}+2\thb_{\beb} \E(\Y)\right)^2 \right) \\
%&+&
% \ln{\frac{1}{\sqrt{2(\theta_{\beta_0}+2\thb_{\beb} \E(\Y))}}} 
&-& 
\left(-0.5\ln{(-2\theta_{\mbox{-}1})} - \thb_{\beta_0\beb} \Y - \thb_{\beb\beb}\Y\Y^T -\frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}}\right) - \tfrac{1}{2}\ln{(2\pi)} \nonumber
\end{eqnarray}



\vspace{0.2in}
\item \textbf{P-form (for each parent $Y_i$, $\Y' = \Y \setminus Y_i $)}: 

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X,\Y')^T s(Y_i) - A \big(\theta(X,\Y') \big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
\thb_{\beta_0\beb}^i + 2\thb_{\beb}^iX + 2\thb_{\beb\beb}^{'i \cdot}\Y \\
\thb_{\beb\beb}^{ii} 
\end{pmatrix}^T
\begin{pmatrix}
Y_i\\
{Y_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}

where $\thb_{\cdot}^i$ is the $i$th component of $\thb_{\cdot}$,\\
where $\thb_{\beb\beb}^{'i\cdot} = \thb_{\beb\beb}^{i\cdot}$ with $\thb_{\beb\beb}^{ii} = 0$, (vector)\\
where $\thb_{\beta_0\beb}' = \thb_{\beta_0\beb}$ with $\thb_{\beta_0\beb}^i = 0$,\\
where $\thb_{\beb}' = \thb_{\beb}$ with $\thb_{\beb}^i = 0$,\\
where $\thb_{\beb\beb}' = \thb_{\beb\beb}$ with $\thb_{\beb\beb}^{ii} = 0$, $\thb_{\beb\beb}^{i\cdot} = 0$  and $\thb_{\beb\beb}^{\cdot i} = 0$.\\


\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A normal child given a set of normal parents and a inv-gamma parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be a normal variable and $ \mathbf{Y} = \{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are normal. $beta_0$, $ \beb = \{beta_1,\ldots,beta_n\}$ now represents normal variables (prior distributions of of the beta parameters) and $\gamma$ is an inverse gamma distribution (prior distribution of the parameter $\sigma^2$).

The log-conditional probability of $X$ given its parents $\mathbf{Y}, \beb, \gamma$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n,\beb,\gamma) &=& \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \me^{-\frac{(x-(\beta_0+ \bs \beta^T \cdot \bm Y))^2}{2\sigma^2}} \right)\\\\
&=&
- \ln{\sigma} - 0.5\ln{(2\pi)} - \frac{(x-\beta_0 - \bs \beta^T \mathbf{Y})^2}{2\sigma^2}
\end{eqnarray*}


Note that here all $\beb$ and $\sigma^2$ values must be taken from the moment parameters of these variables, not the natural parameters of $X$.

\begin{itemize} 

\item \textbf{C-form (messages from $\beb$ and $\gamma$ variables to $X$)}:

As in \ref{NormalGNomal2form}, but now the natural parameters should be taken from the moment parameters of the parent variables, and not the natural parameters of $X$.

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(\mathbf{Y})^T s(X) - A \big(\theta(\mathbf{Y})\big) + h(\mathbf{X})\\\\
&=&
\begin{pmatrix}
\frac{\beta_0}{\sigma^2} + \frac{\beb}{\sigma^2} \Y\\
\frac{-1}{2\sigma^2}\\
\end{pmatrix}^T
\begin{pmatrix}
X \\
X^2 \\
\end{pmatrix}\\ 
&-& 
\left(0.5\ln{\sigma^2} + \frac{\beta_0\beb}{\sigma^2} \Y + \frac{\beb\beb^T}{2\sigma^2}\Y\Y^T +\frac{\beta_0^2}{2\sigma^2}\right) - \tfrac{1}{2}\ln{(2\pi)} 
\end{eqnarray*}

\item \textbf{P-form (messages from $X$ to $\beta_0$)}:
\begin{eqnarray*}
\ln p(X\mid \mathbf{Y},\beta_0,\beb,\gamma) &=& \theta(X,\Y,\beb,\gamma)^T s(\beta_0) - A \big(\theta(X,\Y,\beb,\gamma) \big) + h(\mathbf{Y,\beta_0,\beb,\gamma})\\\\
&=&
\begin{pmatrix}
X\sigma^{-2} - \beb^T \Y \sigma^{-2} \\
-0.5\sigma^{-2}
\end{pmatrix}^T
\begin{pmatrix}
\beta_0\\
\beta_0^2\\
\end{pmatrix}\\
&-& X^2 0.5\sigma^{-2} - \beb^T \beb \Y^T \Y 0.5\sigma^{-2}  + X\beb^T \Y \sigma^{-2} - \ln{\sigma} - 0.5\ln{(2\pi)}
\end{eqnarray*}


\item \textbf{P-form (messages from $X$ to $\beta_i$)}:
\begin{eqnarray*}
\ln p(X\mid \mathbf{Y},\beta_0,\beb,\gamma) &=& \theta(X,\Y,\beta_0,\beb',\gamma)^T s(\beta_i) - A \big(\theta(X,\Y,\beta_0,\beb',\gamma) \big) + h(\mathbf{Y,\beta_0,\beb,\gamma})\\\\
&=&
\begin{pmatrix}
-\beta_0 Y_i \sigma^{-2} + Y_i X \sigma^{-2} - \beb_i' Y_i \Y' \sigma^{-2} \\
-0.5Y_iY_i \sigma^{-2}
\end{pmatrix}^T
\begin{pmatrix}
\beta_i\\
{\beta_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}

where $\beb_i' = \beb$ where $\beta_i = 0$,\\


\item \textbf{P-form (messages from $X$ to $\gamma$ ($\sigma^2$))}:
\begin{eqnarray*}
\ln p(X|Y_1,\ldots,Y_n,\beb,\gamma) &=& \theta(X,\Y, \beta_0, \beb)^T s(\gamma) - A \big(\theta(X,\Y,\beta_0,\beb) \big) + h(\mathbf{\Y,\beta_0, \beb})\\\\
&=&
\begin{pmatrix}
-\frac{1}{2}\\
-\frac{(X-\beta_0-\beb\Y)^2}{2}
\end{pmatrix}^T
\begin{pmatrix}
\ln{\sigma^2}\\
\frac{1}{\sigma^2}
\end{pmatrix}
 - 0.5\ln{(2\pi)}
\end{eqnarray*}

\item \textbf{P-form (for each parent $Y_i$, $\Y' = \Y \setminus Y_i $)}: 

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X,\Y')^T s(Y_i) - A \big(\theta(X,\Y') \big) + h(\mathbf{Y})\\\\
&=&
\begin{pmatrix}
-\beta_0\beta_i \sigma^{-2} + \beta_i X \sigma^{-2} - \beta_i\beb_i' \Y' \sigma^{-2} \\
-0.5\beta_i\beta_i \sigma^{-2} 
\end{pmatrix}^T
\begin{pmatrix}
Y_i\\
{Y_i}^2\\
\end{pmatrix}\\
&+& \thb_{\beta_0\beb}'Y + 2\thb_{\beb}'XY  + \thb_{\beb\beb}'\Y\Y^T + \theta_{\beta_0}X + \theta_{\mbox{-}1}XX \\
&-& \left( \frac{\theta_{\beta_0}^2}{4\theta_{\mbox{-}1}} \right) - \tfrac{1}{2}\ln{(2\pi)}
\end{eqnarray*}
where $\beb_i' = \beb \setminus \beta_i$ \\

\end{itemize}


\subsection{Inverse gamma distribution}

Let $\gamma$ be an inverse gamma variable with parameters $\alpha, \beta$. The log-conditional probability of $\gamma$ can be expressed as follows:

\begin{eqnarray*}
\ln p(\gamma) &=& \ln \left( \frac{\beta^{\alpha}}{\Gamma(\alpha)} \gamma^{-\alpha-1} e^{-\frac{\beta}{\gamma}} \right)\\\\
&=&
\begin{pmatrix}
-\alpha - 1\\
-\beta
\end{pmatrix}^T
\begin{pmatrix}
\ln \gamma \\
\frac{1}{\gamma}
\end{pmatrix}
- \left(\ln{\Gamma(\alpha)} - \alpha \ln{\beta}\right) + 0
\end{eqnarray*}

NB: More details to be found in the EF\_Gamma\_Inv-GammaDistributions.pdf scanned file.

\newpage



%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a binary parent}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution variable, and let $Y$ be a binary variable. The log-conditional probability of the child-node $X$ given its binary parent-node $Y$ is expressed as follows:

\begin{eqnarray*}
\ln p(X \mid Y) =  I(Y= y^1) \ln p_{X \mid y^1} + I(Y= y^2) \ln p_{X \mid y^2} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y= y^1)  \Big(\theta_{X1} \cdot s(X) - A(\theta_{X1})\Big) +  I(Y= y^2) \Big(\theta_{X2} \cdot s(X) - A(\theta_{X2})\Big) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= I(Y=y^1) \cdot \theta_{X1} \cdot s(X) - I(Y=y^1) \cdot A(\theta_{X1}) +  I(Y=y^2) \cdot \theta_{X2} \cdot s(X) - I(Y=y^2) \cdot A(\theta_{X2})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{F-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta^T s(X,Y) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2}) \\
\theta_{X1} \\
\theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
s(X) \cdot I(Y=y^1) \\
s(X) \cdot I(Y=y^2)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{C-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(Y)^Ts(X) - A(\theta(Y)) \\
&=&
\Big(I(Y=y^1) \cdot \theta_{X1} + I(Y=y^2) \cdot \theta_{X2}\Big)
s(X) \\
&&- I(Y=y^1) \cdot  A(\theta_{X1}) -  I(Y=y^2) \cdot A(\theta_{X2})\\\\
&=&
\Big(m^Y_1 \cdot \theta_{X1} + 
m^Y_2 \cdot \theta_{X2}\Big)
s(X) 
-  m^Y_1 \cdot  A(\theta_{X1}) -  m^Y_2  \cdot A(\theta_{X2})
\end{eqnarray*}

\item \textbf{P-form}:

\begin{eqnarray*}
\ln p(X \mid Y) &=& \theta(X)^T s(Y) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
- A(\theta_{X2})\\
s(X) \cdot \theta_{X1}\\
s(X) \cdot \theta_{X2}
\end{pmatrix}^T
\begin{pmatrix}
I(Y=y^1) \\
I(Y=y^2) \\
I(Y=y^1) \\
I(Y=y^2)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}

\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section{EF representation: A base distribution given a set of multinomial parents}
%-----------------------------------------------------------------------------------------------------------------------------------

Let $X$ be any base distribution, and let $\mathbf{Y} =\{Y_1,\ldots,Y_n\}$ denote the set of parents of $X$, such that all of them are multinomial. Each parent $Y_i$, $1 \geq i \geq n$, has $r_i$ possible values or states such that $r_i \geq 2$. A parental configuration for the child-node $X$ is then a set of $n$ elements $\{Y_1 = y_1^{v}, \ldots, Y_i = y_i^{v},\ldots, Y_n = y_n^{v} \}$ such that $y_i^{v}$ denotes a potential value of variable $Y_i$ such that  $1 \leq v \leq r_i$. Let $q = r_1 \times \ldots \times r_n$ denote the total number of parental configurations, and let $\mathbf{y}^l$ denote the $l^{th}$ parental configuration such that $1 \leq l \leq q$.

The log-conditional probability of the child-node $X$ given its parent-nodes $\mathbf{Y}$ can be expressed as follows:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, Y) =  \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \ln p_{X \mid \bm Z, \mathbf{y}^l} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \Big(  \theta_{Xl}   \cdot  s(X,\bm Z)  \cdot  A(\theta_{Xl}) \Big)~~~~~~~~~~~~~\\
= \sum_{l=1}^q I(\mathbf{Y} =\mathbf{y}^l) \cdot \theta_{Xl} \cdot s(X,\bm Z) - I(\mathbf{Y} =\mathbf{y}^l) \cdot A(\theta_{Xl})
\end{eqnarray*}

This conditional probability distribution can be expressed in different exponential forms as follows:

\begin{itemize}

\item \textbf{F-form}:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, \mathbf{Y}) &=& \theta^T s(X,\bm Z, \mathbf{Y}) - A(\theta) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq}) \\
\theta_{X1} \\
\vdots \\
\theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
s(X, \bm  Z) \cdot I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
s(X, \bm  Z) \cdot I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0 
\end{eqnarray*}

\item \textbf{C-form}:

\begin{eqnarray*}
\ln p(X \mid \bm  Z, \mathbf{Y} ) &=& \theta(\bm  Z, \mathbf{Y} )^T s(X) - A(\mathbf{Y} ) \\
&=&
\Big(\sum^q_{l=1} I(\mathbf{Y} =\mathbf{y}^l) \cdot \theta_{X_l}(\bm  Z) \Big)
s(X)
- \sum^q_{l=1} I(\mathbf{Y} =\mathbf{y}^l) \cdot A(\theta_{X_l}(\bm Z)) \\\\
&=&
\Big(\sum^q_{l=1} \mathbf{m}^\mathbf{Y}_l \cdot \theta_{X_l}(\bm  Z) \Big)
s(X)
- \sum^q_{l=1} \mathbf{m}^\mathbf{Y}_l \cdot A(\theta_{X_l}(\bm Z)) \\\\
\end{eqnarray*}

\item \textbf{P-form}:

\begin{eqnarray*}
\ln p(X \mid \mathbf{Y}) &=& \theta(X)^T s(\mathbf{Y}) - A(X) \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
s(X) \cdot \theta_{X1}\\
\vdots \\
s(X) \cdot \theta_{Xq}
\end{pmatrix}^T
\begin{pmatrix}
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q) \\
I(\mathbf{Y} =\mathbf{y}^1) \\
\vdots \\
I(\mathbf{Y} =\mathbf{y}^q)
\end{pmatrix}
- 0
\end{eqnarray*}

\end{itemize}


%----------------------------------------- with one parent

\begin{eqnarray*}
\ln p(X\mid \mathbf{Y}) &=& \theta(X, \mathbf{Y'} )^T s(Y_i) - A(X) ~~\textrm{such~that} ~\mathbf{Y'} = \mathbf{Y} \setminus Y_i \\ \\
&=&
\begin{pmatrix}
- A(\theta_{X1}) \\
\vdots \\
- A(\theta_{Xq})\\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}  +  \ldots + s(X) \cdot \mathbf{m}^{\mathbf{Y'}}_1 \cdot \theta'_{X1}\\
\vdots \\
\! s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}   + \ldots + s(X) \cdot  \mathbf{m}^{\mathbf{Y'}}_{q'} \cdot \theta'_{Xq'}
\end{pmatrix}^T \!
\begin{pmatrix}
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!\\
I(Y_i=y_i^1) \! \\
\vdots \\
I(Y_i=y_i^{r_i}) \!
\end{pmatrix}
- 0 \!
\end{eqnarray*}


\newpage
%-----------------------------------------------------------------------------------------------------------------------------------
\section*{Notations}
%-----------------------------------------------------------------------------------------------------------------------------------

The list below presents a summary of the used notations:
\\

\begin{table}[ht!]
\renewcommand{\arraystretch}{1.1}
{\small
\begin{tabular}{l l}
$X$ & Child variable\\
$k$& Range of possible values of a multinomial variable $X$\\
$j$ & Index over $X$ values, i.e., $1 \geq j \geq k$ \\
$Y$ & One parent variable\\
$\mathbf{Y}$ & Set of parent variables\\
$n$& Number of parent variables \\
$i$ & Index over parent variables, i.e., $1 \geq i \geq n$ \\
$r_i$& Range of possible values of a multinomial variable $Y_i$\\
$q $ & Total number of configurations of a multinomial parent set $\mathbf{Y}$\\
$l$ & Index over the possible parental configuration values, i.e., $1 \geq l \geq q$ \\
$\mathbf{y}^l$ & The $l^{th}$ configuration of a multinomial parent set $\mathbf{Y}$\\
$\theta_{jl}$ & Equal to $\ln p_{x^j\mid \mathbf{y}^l}$, denoting the log-conditional probability of $X$ in its state $j$ \\
                    & given the $l^{th}$ parent configuration\\
$\theta_{Xl}$ & Equal to $\ln p_{ X \mid \mathbf{y}^l}$, denoting the log-conditional probability of a base distribution variable $X$ \\
                    & given the $l^{th}$ parent configuration\\
$p$ & Probability distribution\\
$m$ & Expected sufficient statistics \\
$s$ & Sufficient statistics \\
\end{tabular}}
\end{table}
-

\end{appendices}

\bibliography{biblio}
\bibliographystyle{plain}

\end{document}


