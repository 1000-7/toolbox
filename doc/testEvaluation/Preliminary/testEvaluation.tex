\documentclass{article}

\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}

\usepackage{bm}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{subfigure}

\usepackage{theorem}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\bu}[1]{\mathbf{#1}}
\newcommand{\bv}[1]{\bm{#1}}




\title{Practical Considerations for Performance Measures on Classifiers on Real World Streaming Data}
%\author{Sigve Hovda \\
%Norwegian University of Science and Technology\\
%Department of Computer and Information Science,
%Trondheim, Norway\\
%sigveh@idi.ntnu.no}
\date{}


\begin{document}
\maketitle

\begin{abstract}
Several real world binary classification problems in the domains of automobile, energy and bank are outlined.  This paper do not discuss how to solve the problems, but rather how any solution can be evaluated.  All problems involve huge data sets which have various limitations related to the question of what is the ground truth. The problems also involve streaming data and earliness of warnings need to be balanced with accuracy.  It is argued that each of the problems presented in this paper can be discussed using two measures; concordance probability related to the output function and the empirical risk function in some forms.  The paper includes practical considerations tailored to the specific properties of each particular problem.
\end{abstract}

\section{Introduction}





We consider the random variables $X$ and $Y$ with joint probability distribution $P(X,Y)$ and output spaces $\Omega_X$ and $\Omega_Y = \{0,1\}$, respectively.  We consider a continuous output function $q: \Omega_X \rightarrow \mathbb{R}^+$ and a family of hypothesis functions $\mathcal{H}$, where each element $h_T:\Omega_X \rightarrow \Omega_Y$ has the form 

\begin{equation}
\label{eq:ht}
h_T(x) = 
\begin{cases}
0 \quad \mbox{for} \quad q(x) \leq T\\
1 \quad \mbox{else}.
\end{cases}
\end{equation}
Also, let $X_0$ and $X_1$ be random variables with probability distributions $P(X | Y = 0)$ and $P(X | Y = 1)$, respectively.  We define the random variables $Q_0 = q(X_0)$ and $Q_1 = q(X_1)$.

In this paper, we will discuss the continuous output function in the light of the Mann-Whitney $U$ test and the concordance probability $P(Q_1 > Q_0)$.  It is important to mention that in the context of concordance probability, the shapes of $P(Q_0)$ and $P(Q_1)$ may be different. 

Also, we want to mention that the concordance probability is exactly equal to the area under the receiver operating characteristic curve (ROC) and the common language effect size of the Mann-Whitney $U$ test.

In terms of discussing the family of hypothesis functions, we have chosen empirical risk as the quantity of interest.  This involves defining a loss function and the risk function is simply the expected loss in a frequentist perspective.

\subsection{Mann-Whitney $U$ test}
\label{sec:U}

In statistics, the Mann-Whitney $U$ test (also called the Mann-Whitney-Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon-Mann-Whitney test) is a nonparametric test of the null hypothesis that two populations are the same against an alternative hypothesis, especially that a particular population tends to have larger values than the other.  It has greater efficiency than the $t$-test on non-normal distributions and it is nearly as efficient as the $t$-test on normal distributions.

We define a training set $\bu{x} \times \bv{y}$ with $n$ input-output pairs $(x_i, y_i)$, independently drawn from $P(X,Y)$.  From the training set we have two populations $\bv{q_0} = \{q(x_i) , | \, y_i = 0 \}$ and $\bv{q_1} = \{q(x_i) , | \, y_i = 1 \}$.  Their sizes are $n_0$ and $n_1$ so that $n_0 + n_1 = n$.  Calculating the $U$ statistics is straightforward, where these two values are obtained 

\begin{equation}
\label{eq:U}
U_0 = \sum_{i=1}^{n_0}\sum_{j=1}^{n_1} H(\,q_j - q_i    \,) \quad \mbox{and} \quad 
U_1 = \sum_{i=1}^{n_0}\sum_{j=1}^{n_1} H( \,q_i - q_j    \,).
\end{equation}
Here $H(\cdot)$ is the heaviside step function and notice that $U_0 + U_1 = n_0n_1$.  For large samples, each U is approximately normally distributed. In that case, the standardized value

\begin{equation}
\label{eq:z}
z = \frac{U_0 - m_{U}}{\sigma_{U}},
\end{equation}
where $m_U$ and $\sigma_U$ are the mean and standard deviation of $U$ given by

\begin{equation}
\label{eq:z}
m_U = \frac{n_0n_1}{2} \quad \mbox{and} \quad 
\sigma_U = \sqrt{\frac{n_0n_1(n_0 + n_1 + 1)}{12} }.
\end{equation}

Significance of test can be checked in tables of the normal distribution.  Although, such an hypothesis test is interesting by itself, we are more interested in the concordance probability $P(Q_1 > Q_0)$ which is defined by

\begin{equation}
\label{eq:concordance}
P(Q_1 > Q_0) = \frac{U_1}{n_0n_1}.
\end{equation}


\subsection{Empirical risk}
\label{sec:empRisk}

In mathematical optimization, statistics, decision theory and machine learning, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number that is intuitively representing some \emph{cost} associated with the event. Loss functions can be used on optimization problems, where an algorithm or method is optimized by minimizing the loss function.  Moreover, loss functions are frequently used to diagnose and compare various algorithms or methods.  

In this paper define the \emph{loss function} as a real and lower-bounded function $L$ on $\Omega_X \times \Omega_Y \times \Omega_Y$.  The value of the loss function at an arbitrary point $(x, h(x), y)$ is interpreted as the loss, or cost, of taking the decision $h(x)$ at $x$, when the right decision is $y$.  Notice that in this paper, the loss function is dependent on $x$ as well.  This is of high practical use, because a certain misclassification might be more expensive that another. 

In the frequentist perspective, the expected loss is often referred to as the risk function.  It is obtained by taking the expected value over the loss function with respect to the probability distribution $P(X,Y): \Omega_X\times \Omega_Y \rightarrow \mathbb{R}^+$.  The \emph{risk function} is given by

\begin{equation}
\label{def:risk}
R(h) = \int_{\Omega_X,\Omega_Y} L(x,h(x),y) dP(x,y).
\end{equation}

In the case when the costs are independent of $x$ and also that there is no cost related to correct classification, the risk function reduces to the well known expected cost of misclassification (ECM)

\begin{equation}
\label{eq:ecm}
ECM =  c(1|0)p(1|0)p_0  + c(0|1)p(0|1)p_1.
\end{equation}
Here, $c(1|0)$ is the cost for misclassifying an item of class zero as class one and $p(1|0)$ is the misclassification probability given class zero.  The quantities $c(0|1)$ and $p(0|1)$ are defined equivalently, while $p_0$ and $p_1$ are the priors.  

In general, the risk $R(h)$ cannot be computed because the distribution $P(x, y)$ is unknown.  However, we can compute an approximation, called empirical risk, by averaging the loss function on the training set given by 

\begin{equation}
\label{def:empRisk}
R_{emp}(h, \bv{x}) = n^{-1} \sum_{i=1}^n L(x_i, h(x_i), y_i).
\end{equation}
Notice that $L$ is an array of $n \times 2\times 2$ elements.  Many supervised learning algorithms are optimized by finding the $h$ in a hypothesis space $\mathcal{H}$ that minimizes the empirical risk.  This paper will not focus on empirical risk minimization, but rather focus on using empirical risk to compare methods.
%
%\subsection{Emprical risk function on practical problems}
%\label{sec:practical}



\section{Practical problems}
\label{sec:practicalEmpRisk}

In this section we will outline a number of practical using concordance probability and the empirical risk function.


\subsection{Automotive use cases }
\label{sec:automotive}

There are two application scenarios here.  The first one is early recognition of lane change manoeuvre. The second is prediction of the need for lane change based on relative dynamics between two vehicles following the same lane.

For the first use case scenario it is required that the concordance probability (AUROC) should be above 0.96 for prediction 1 second before lane crossing and 0.90 for the 2 second prediction.

For the second use case scenario it is required that the concordance probability (AUROC) should be above 0.96 for prediction 1 second before lane crossing and 0.90 for the 2 second prediction.

These two application scenarios seem straightforward to calculate.

\subsubsection*{Questions: }
\begin{enumerate}
\item What will be the sizes of $n_0$ and $n_1$.
\item Are each test sample completely independent?
\item Are the shapes of $P(Q_0)$ and $P(Q_1)$ equal? If so, we can also do the hypothesis test.
\item Are you sure that a cost invariant test is sufficient for your use?
\end{enumerate}


\subsection{Financial use case: Default prediction of clients}
\label{sec:financial}

There are two application scenarios here.  The first one is prediction of whether a client will default within two years and the second is related to the benefit of a marketing campaign.

\subsubsection*{Discussion using concordance probability}

In the first use case scenario, it is required that the concordance probability (AUROC) should be above 0.90.

This use case involves to predict the probability $p_i$ of defaulting for certain customer $i$ that is applying for a loan in a bank.  The $y$'s can take value 0, which is non defaulting and value 1, which is defaulting.  
In the cases where a loan is given, each $y_i$ is determined by whether the loan has defaulted or not, exactly two years later.  This means that the both $\bv{q_0}$ and $\bv{q_1}$ are severely biased.  Estimating the concordance probability will only be relevant if we use the AMIDST software as an addition to the existing software.

\subsubsection*{Questions for the first use case scenario: }
\begin{enumerate}
\item Is it ok to test on only these samples that are known?  Should we try to make a different sample (by expert estimates or similar) that is independent on whether a loan was given to them or not. 
\item What will be the sizes of $n_0$ and $n_1$.
\item Are each test sample completely independent?
\item Are the shapes of $P(Q_0)$ and $P(Q_1)$ equal? If so, we can also do the hypothesis test.
\end{enumerate}

\subsubsection*{Discussion using empirical risk}

In the second use case scenario it is required that the benefit of a AMIDST induced marketing campaign should be more than 5 percent higher than a normal campaign. 

Based on the size of the loan it is possible to reason about the cost of defaulting $c_i(0|1)$ and also the cost of declining the loan application if the customer actually would have not defaulted $c_i(1|0)$.  The classification problem reduces to whether $c_i(0|1)p_i$ is higher than $c_i(1|0)(1-p_i)$, which is the same as comparing $p_i$ with $c_i(1|0)/(c_i(0|1 + c_i(1|0))$. 

In order to discuss the cost of using this classification method, compared to a perfect classifier, we propose this implementation of the loss function

\begin{equation}
\label{def:empRiskBank}
L(x_i, h(x_i), y_i) = 
\begin{cases}
0     &\quad \mbox{for} \quad h(x_i) = 0 \quad \& \quad y_i = 0\\
c_i(1|0)    &\quad \mbox{for} \quad h(x_i) = 1 \quad \& \quad y_i = 0\\
c_i(0|1)      &\quad \mbox{for} \quad h(x_i) = 0 \quad \& \quad y_i = 1\\
0   &\quad \mbox{for} \quad h(x_i) = 1 \quad \& \quad y_i = 1.
\end{cases}
\end{equation}

In this context, the loss function is only partially known.  It is only known at the $x_i$s where the subject was part of the default marketing campaign and a loan was actually given.  We define a function $h_{pre}(x): \Omega_X \rightarrow \Omega_Y$ as the decision rule which involves that the bank decided to offer a loan and also that the subject decided to accept the loan more than two years ago.  Consequently, $y_i$ is only known given $h_{pre}(x_i) = 1$.  We define $\bv{x_{acc}} = \{x_i \in \bv{x} | h_{pre}(x_i) = 1\}$, $\bv{y_{acc}} = \{y_i \in \bv{y} | h_{pre}(x_i) = 1\}$ and the sizes of $\bv{x_{acc}} $ and $\bv{y_{acc}} $ are equal to $n_{acc}$.

If we let $y_{acc,i}$ be an element of $\bv{y_{acc}} $ and $x_{acc,i} \in \bv{x_{acc}} $ be a corresponding element to $y_{acc.i}$, then an estimate of empirical risk is 

\begin{equation}
\label{def:empRiskLimited}
R_{emp}(h, \bv{x_{acc}}) = n_{acc}^{-1} \sum_{i=1}^{n_{acc}} L(x_{acc,i}, h(x_{acc,i}), y_{acc,i}).
\end{equation}

This approximation must be treated with care because the $x_{acc,i}$s are not taken randomly, but they are filtered by when the decision rule $h_{pre}(x_i)$ is equal to one.  However, $R_{emp}(h, X_{acc})$ has a practical interpretation.  It is the extra cost of using the decision rule $h$ instead of a perfect classifier on data that are already filtered by $h_{pre}(x_i)$.  Moreover, this number can be compared to $R_{emp}(h_{pre}, \bv{x_{acc}})$, which is the cost associated with using $h_{pre}$ on $\bv{x_{acc}}$.  It is therefore possible to outline if there is a financial gain of using a two stage filter, that is using $h_{pre}$ prior to $h$, compared to only using $h_{pre}$.

The two stage scenario is of cause an interesting scenario by itself, but it is probably more interesting to see whether it makes sense to use $h$ instead of $h_{pre}$.  

%If only a low percentage of customers are denied a loan, then equation \eqref{def:empRiskLimited} is a good approximation of the total empirical risk.  However, if there is a substantial amount of customers that is denied a loan, then the measure contains more uncertainty. 
%
%One practical way to compensate for this is to randomly draw a certain subset of customers that are denied a loan two years ago $\bv{x_{dec}}$ and use domain experts to reason about their abilities to not default.  After an empirical risk is found on $\bv{x_{dec}}$, it would be possible to \emph{estimate} the total empirical risk by weighting by the fractions of sizes 
%
%\begin{equation}
%\label{def:empRiskTotal}
%\hat{R}_{emp}(h, \bv{x}) = \frac{n_{acc}}{n}R_{emp}(h, \bv{x_{acc}})  +  \frac{(n - n_{acc})}{n}R_{emp}(h, \bv{x_{dec}}).
%\end{equation}



\subsubsection*{Questions for the second use case scenario: }
\begin{enumerate}
\item How can we possibly find out which is least costly of $h$ and $h_{pre}$? This is the key problem in my opinion.
\item What will be the sizes of $n_0$ and $n_1$.
\item Are each test sample completely independent?
\end{enumerate}





%
%On a real world problem, the costs $c(1|2)$ and $c(2|1)$, can usually be found by reasoning about a problem.  For instance how expensive is it to give a person medical treatment when it is not needed and how \emph{expensive} is it if the treatment do not start at all.  Moreover, the prior and the misclassification probabilities are usually not known and must be estimated from running the algorithm on a labeled population.   
%
%
%
%%%%%%%%%%%%%%%%%%%%%
%
%
%
%Performance measures have been viewed as the only way for evaluating the quality of machine learning programs.
%Several researchers have pointed out that the focus on performance measures has made the machine learning community lose sight of the real-world.
%Wagstaff  \cite{Wagstaff_2012} challenged the community to seek to focus on machine learning that matters and transfer results to impact.
%However, what exactly is meant by impact is not specified, but six impact challenges that could help set the scene are proposed instead.
%In this paper, we seek to remedy the lack of a proper definition and discussion of impact.
%We contrast impact with performance measures and propose that impact is the effect the performance of the machine learning program has on its environment.
%Furthermore, we show how such a definition help to guide the development of machine learning programs that matters.
%This is done by presenting and analysing a case study of a commercially deployed machine learning system that has helped saving tens of millions of US dollars in the oil well drilling industry.
%
%
%One of the most quoted definitions of machine learning, which was proposed by Mitchell \cite{Mitchell_1997}, is: "A computer program is said to learn from experience \emph{E} with respect to some class of tasks \emph{T} and performance measure \emph{P}, if its performance at tasks in \emph{T}, as measured by \emph{P}, improves with experience \emph{E}." 
%Since the performance measures are essential parts of the very definition of whether a program is learning, the machine learning community have had a strong focus on performance measures. 
%
%Performance measures can take many forms. 
%In case of classification problems, performance measures are generally quantities that can be deduces from the confusion matrix such as accuracy, precision, recall, f-number and many others. 
%In case of regression, the performance measures are deduced from the residuals. 
%In unsupervised, semi-supervised learning and reinforcement learning algorithms, the performance measures are even more complex and the variety even broader. 
%It is out of the scope of this paper to give an exhaustive discussion of performance measures in machine learning. 
%Furthermore, the authors understand their importance as they are required for machine learning programs (MLPs) to be able to learn from experience. 
%However, performance measures are only a part of what is needed for practical purposes, such as guiding the development of MLPs solving real-world problems.
%
%Machine learning is an experimental science, as noted by Langley \cite{Langley_1988}.
%However, the focus on controlled experiments had several side-effects, which Langley pointed out over twenty years later \cite{Langley_2011}.
%One of these side-effects was a focus on classification and regression tasks that were as these far easier validate by experiments. 
%This has lead to machine learning as a community has gone astray; there is little real-world orientation nor a focus on impact, which was observed by Wagstaff \cite{Wagstaff_2012}.
%
%The term impact is used by Wagstaff to connect machine learning research to "the larger world of scientific inquiry and humanity," and performance measures do not possess this quality.
%Several examples are given to illustrate this lack of quality.
%However, a proper definition and discussion of what impact exactly is is not presented.
%Instead six impact challenges were proposed.
%These are measured on whether machine learning can impact law, company profit, international conflict resolution, cyber criminality, human safety and the quality of human life.
%
%Our intention is to remedy the lack of a proper, in depth discussion and definition of the impact of machine learning programs (MLP) and to shed light on how impact can guide their development.
%This is not a straight forward task.
%The contributions of this work are: 1) An analysis of the life cycle of MLPs, 2) identification of shortcomings in the MLP life cycle, 3)  a precise definition of impact, 4) a definition of impact analysis and a framework performing it, and 5) a real-world case study of how impact analysis guides the development of a commercial MLP that is used daily in a production environment. 
%The program is currently used by several major oil companies, such as Shell and Baker Hughes, and has at any time over the last year been monitoring between 30 and 50 oil well drilling operations.
%
%The rest of this paper is organized as follows:
%In section \ref{sec:the_machine_learning_program_life_cycle}, we analyse the life cycle of MLPs.
%Then, in section \ref{sec:defining_impact}, we define impact while impact analysis is defined and presented in section \ref{sec:impact_analysis}. 
%A case study of how impact guides the development of DrillEdge is presented in in section \ref{sec:case_study:oil_well_drilling}.
%A short discussion is given in section \ref{sec:discussion}, and we conclude in section \ref{sec:conclusion}.
%
%\section{The Machine Learning Program Life Cycle}
%%\label{sec:the_machine_learning_program_life_cycle}
%%\begin{figure}[t]
%%\begin{center}
%%\includegraphics[scale=0.75]{media/Process_old.pdf}
%%\caption{A brief overview of the MLP life cycle.}
%%\label{fig:life_cycle}
%%\end{center}
%%\end{figure}
%We present a brief overview of the MLP life cycle in this section.
%The cycle is illustrated, in figure \ref{fig:life_cycle}, as a process diagram where rounded boxes depict different processes, arrows indicate the sequence of the processes, square boxes indicate output from the processes, and open boxes indicate the requirements that must be met for a processes to start.
%
%The MLP life cycle starts with a \emph{Requirement Analysis} in which the objectives and the requirements for the MLP are analysed and made explicit.
%Objectives vary from the more scientific "explore a new idea for a machine learning method" to the business driven "reduce down-time of some process."
%The output is a set of objectives and requirements, such as the MLP must learn and reason in real-time for a given data rate," that guides the design and development. 
%\emph{Design and Development} includes the learning algorithm design as well as the software aspect of design in addition to development.
%The output of this acivity is the not yet trained machine learning program (MLP). 
%
%During \emph{Training}, the MLP is trained, for example, through setting weights in a Artificial Neural Network using the Backpropagation method.
%Training requires performance measures to evaluate the effect of the training and thresholds for guiding when to stop.
%In addition, both a training data set and a MLP are requried to produce the output, which is the trained MLP (tMLP).
%\emph{Evaluation} is done by executing the tMLP on a test data set, and the output of this activity, if the tMLP performs well, is a validated MLP (vMLP). 
%A set of performance measures are required as well as thresholds for these in order to validate the tMLP.
%The performance measures and thresholds need not be and are often not the same as the ones used for training.
%Typically, the performance measures and the thresholds are chosen as part the training and evaluation processes, which is a poor practice.
%A vMLP and production data are requried for \emph{Deployment}.
%
%Often, production data change, and the change can include new features as well as data behavior. 
%Both changes might decrease the performance of the MLP, and in order to improve the program further development is required with a new requirement analysis.
%
%The above analysis of the MLP life cycle suggests some shortcomings of how MLPs are developed:
%\begin{enumerate}
%
%\item
%There is no guarantee that the identified objectives and requirements will guide the development of MLPs that matter; requirements are not necessarily related to the impact of the MLP.
%Objectives and requirements can certainly be specified to cover both deployment and evaluation criteria, but there is no way to ensure this.
%
%\item
%Both performance measures and thresholds for these are typically chosen ad-hoc and often as part of the training and evaluation processes.
%Choosing the right performance measures and thresholds are imperative for MLP development, and therefore more attention must be given to how to chose them.
%
%\item
%Production data changes, but there is no way of deciding when the effect of these changes are big enough to justify improvement of an old MLP rather than developing something completely new.
%
%%\item 
%%Currently, there is no way of telling whether the performance measures that measure the quality how the MLP performs cover the needs by the users.
%\end{enumerate}
%
%Thus, there is a lack of a formalized way of 1) choosing which performance measures to optimize, 2) setting thresholds that are sufficient for a system to be deployed, and 3) supporting the decision about which solutions to develop.
%We propose impact analysis as a tool to guide the development of MLPs so that the wanted impact is achieved.
%Before discussing how to analyse impact, we have to define exactly what impact is.
%
%\section{Defining Impact}
%\label{sec:defining_impact}
%%Performance measures indicate how well a machine learning system perform some task. 
%Performance measures compare the task performed by the system to some optimal outcome that typically can be verified by analysing the results in relation to the training data.
%%In contrast, as indicated by the six impact challenges refered to above, impact cannot be measured by using the machine learning training data alone, if at all.
%In contrast, impact cannot be measured by using the machine learning training data alone, if at all.
%%Impact is related to something else than to the system's performance on training data.
%Impact is related to the effects the system has on the environment in which it is situated.
%These effects can be related to different dimensions, such a profit, peoples health, security and so on.
%%Hence, we interpret impact as the effect the performance of the machine learning system has on its environment.
%Put more precisely:
%
%\begin{description}
%\item [Definition:] Impact \textbf{I} is the overall effect that a machine learning program \emph{M} has on its real-world environment \emph{R} measured along a set of dimensions \emph{D}.
%An impact component \emph{$I_d$} is the effect the machine learning program M has along a given dimension \emph{d}, and it is calculated as the sum of all gains and costs for that given dimension \emph{d}.
%The impact \textbf{I} is therefore the real-valued vector of impact components.
%
%\end{description}
%
%This definition is further detailed in the following two equations:
% 
%\begin{equation}
%\textbf{I} =  \{I_1, I_2, ..., I_n\},
%\end{equation}
%
%and 
%
%\begin{equation}
%I_d=\sum_{j=1}^m g_{d,j} - \sum_{k=1}^o c_{d,k}, 
%\end{equation}
%
%where $g_{d,j}$ and $c_{d,k}$  are the gains and costs along dimension $d$.
%
%The definition is illustrated with an example; let us look at the possible impact that an imagined High Frequencey Trading MLP has on a company's profit.
%Profit \emph{p} is only one dimension of the possible dimensions that this MLP can impact.
%There are costs related to developing new software: Software developers require money to develop software, $c_{dev}$, the software needs computers to run on, $c_{hw}$ as well as power, $c_{pow}$, and quality assurance does not come for free, $c_{qa}$.
%By using machine learning for High Frequency Trading, the company now is perceived as a technology leader. 
%Being a technology leader attracts new customers, and thus this is considered a gain, $g_{tl}$.
%The trading result can be both a gain and a cost, depending on how well the program does.
%In our example the trading result has been positive, and therefore the trading result is also a gain, $g_{tr}$.
%The following equations describe the impact:
%
%\begin{equation}
%\textbf{I} =  \{I_1\},
%\end{equation}
%
%and 
%
%\begin{equation}
%I_1= g_{1,tl} + g_{1,tr} - c_{1,dev} - c_{1,hw} - c_{1,pow} - c_{1,qa},
%\end{equation}
%
%The qualitites of our definition are: 1) impact $\textbf{I}$ is related to the real-world environment $R$ that the program is situated in, 2) it explicitly states that MLPs impact several different dimensions $D$ while acknowledging that the set $D$ is not static as different dimensions are relevant for fulfilling different goals in different domains, 3) it explicitly expresses costs $c$ and gains $g$ for the different dimensions $d$, and 4) impact $\textbf{I}$ is defined as a vector not a value.
%
%\section{Impact Analysis}
%\label{sec:impact_analysis}
%Impact analysis is an analysis of how a MLP affects its environment, which encompasses identifying which dimensions that are affected by the program as well as the costs and gains related to these.
%The analysis enables prioritization of dimensions, but it also provides an understanding of which costs and gains are affecting the dimensions the most.
%This understanding allows selecting the performance measures that express the most relevant changes.
%Furthermore, finding the right thresholds for these performance measures can be related to the effect of a given dimension, such as profit.
%
%What we present in this section is not a methodology for how to develop MLPs.
%It is merely a loose framework to help guide the process around developing machine learning programs that matters.
%%
%%\begin{figure*}[t!]
%%\begin{center}
%%\includegraphics[scale=0.65]{media/Process_new.pdf}
%%\caption{An extended MLP life cycle that includes impact analysis.}
%%\label{fig:extended_life_cycle}
%%\end{center}
%%\end{figure*}
%
%\subsection{Impact Analysis Activities}
%As illustrated in figure \ref{fig:extended_life_cycle}, the first process in the extended MLP life cycle is impact analysis, which should be performed if you want to ensure that you develop a MLP that impact others.
%For a researcher, this means giving thought to how the research can impact the real-world while for a company this might mean to listen to customers and their view on how the commercial software can improve their lifes.
%The output of the impact analysis, as illustrated in figure \ref{fig:extended_life_cycle} is performance measures (PM) to use for training and evaluation, their thresholds T, impact dimensions (D), costs (C) and gains (G).
%All the other processes are unchanged from figure \ref{sec:the_machine_learning_program_life_cycle}.
%
%We have identified the following activities related to impact analysis:
%
%\begin{description}
%\item [Choose solutions to compare:]
%This typically is a comparison between status quo and and adding an MLP, or comparing different MLP systems to decide which has the greatest impact. 
%
%\item [Identify relevant dimensions:] 
%One could argue that there exists one superset of dimensions that impact can be measured along.
%However, it is not feasible nor desired to analyse all dimensions for all domains, as impact is highly domain specfic; different dimensions are important in different domains.
%%Clearly, different dimensions are important to an investment bank and an environmental organization.
%This can be guided by identifying which impact components that change.
%
%\item[Identify costs and gains:] 
%Identifying the costs and gains for a dimension and list them in a cost and gain tables.
%Deciding on which costs and gains are the most impartant, can help the selection of performance measures and thresholds.
%
%%\item[Decide on which costs and gains are the most impartant:]
%%MLPs that affect the most important costs and gains will have higher impact.
%
%\item[Prioritize dimensions:]
%A relative priorization of dimension can be used when understanding the domain and comparing several solutions against each other, as will be discussed later.
%%The most relevant dimensions should be identified by analysing the costs and gains in order to map out for which dimensions the MLP can make the most impact.
%%Then the dimensions should be prioritized according to which ones the MLP can impact the most.
%%Is it possible for the MLP to impact any of the costs and gains? 
%%Will the impact increase or decrease because of the MLP, and how much? 
%
%\item[Choose performance measures:]
%As performance measures guides the reasoning process, great consideration should be taken in order to ensure that the wanted effect is optimized for.
%Thus, performance measures that emphasize the wanted behavior of the MLP must be chosen.
%
%\item[Set thresholds:]
%After choosing the perfomance measures, thresholds must be set to indicate when the desired impact is achieved in order for the soltuion to be deployed. 
%\end{description}
%
%By performing these activities before starting development, a well informed decision can be made on whether to start the development or not.
%Different projects can be measured against each other. 
%The decision for which to embark on can be justified by their possible impact.
%Impact analysis seeks to answer at least parts of why the MLP is developed -- both for researchers and commercial developers.
%
%In order to be able to compare different projects, some method for comparing the impact between project.
%We propose two impact measures to use as part of the impact analysis: Impact Change and Weighted Sum Impact.
%
%\subsection{Comparing the Impact of Different Solutions}%Comparing Differences in Impact} %Comparing the Impact of Different Solutions
%Impact measures can be used to emphasize differences in impact between solutions, such as improvements between versions of a solution or to show difference in impact between different solutions.
%%A short description of two different impact measures that treat the impact vector $\textbf{I}$ differently follows.
%
%\subsubsection{Impact Change} 
%The result of the impact change measure is a vector, $\Delta \textbf{I}$, that shows the changes in impact along all dimensions $D$.  
%Given two different solutions, A and B with impacts $\textbf{I}_A$  and $\textbf{I}_B$ respectively, the impact change is computed as follows:
%
%\begin{equation}
%\Delta \textbf{I}_{A,B} = \textbf{I}_A - \textbf{I}_B
%\end{equation} 
%
%The advantage of the impact change measure is that the result is a vector that shows the changes in the impact components explicitly.
%This enables a discussion about which solution impacts which dimensions without stating which dimensions are the most important.
%
%\subsubsection{Weighted Sum Impact}
%The weighted sum impact, $W$, is calculated for all solutions $S$ that are considered.
%The output of this calculation is a value that can be used for comparing the different solutions in $S$.
%Given a solution A, the $W$ is the weighted sum of all impact components $I_d$ along all dimensions $d$: % in the set of relevant dimensions $D$:
%
%\begin{equation}
%W = \sum_{n=0}^d W_n \cdot I_n,
%\end{equation}
%
%where $W_n$ is the relative importance for an impact component. This method works better when a common agreement about the relative importance between the relevant dimensions exists.
%Also, this method enables comparing sets of solutions $S$ that contains more than two solutions.
%
%\section{Machine Learning in Oil Well Drilling}
%\label{sec:case_study:oil_well_drilling}
%In order to be able to perform an impact analysis, the domain in which the MLP is to be situated in has to be investigated.
%This case study presents the domain of oil well drilling and challenges related to performing machine learning in this domain before the impact analysis is performed.
%
%
%\subsection{The Domain: Oil Well Drilling}
%Oil well drilling is a very complicated process that involves drilling a hole with a diameter between 5 to 50 inches into the earth.
%This is done by a drilling rig that rotates a drill string with a drill bit attached at the end. 
%Petroleum oil hydrocarbons are generally located approximately 3000 meters below sea level, and the holes can be up to 10 000 meters deep. 
%As the drilling rigs often are fixed installations, all wells cannot be vertical, so it is common to drill horizontally in order to to hit the reservoir. 
%While drilling, the drilling crew needs to take into account the well plan, which involves analyses of seismic data, drilling data from nearby wells  and input from numerous experts to set the path of the well, select equipment and more. 
%
%Sensors measure different parameters during an oil well drilling operation, both on the surface and down in the hole.
%Typical sensor data include downhole pressure, torque, rotation speed, rate of penetration, depth of the drill bit, hole depth and several others.
%These data are used by the rig crew, but they are also sent to a server so that they can be used to monitor the drilling process by remote personnel.
%
%\subsection{The Problem: Montioring Drilling Operations}
%Oil well drilling operations are typically monitored remotely by drilling engineers 24/7.
%While the driller, which is situated at the rig, has to solve all the small problems happening all the time, the remotely located drilling engineers monitor the overall situation.
%The reason for having personnel monitoring remotely is multifaceted.
%First of all, as drilling rigs often are located in remote areas, locating personnel on-site is more costly than locating them close to their home and families.
%Furthermore, by removing drilling engineers from the rigs, less personnel is needed.
%Adding the fact that there is a general lack of experienced personnel, co-location makes even more sense.
%
%More specifically, the monitoring is done by staring at graphs of sensor measurements plotted on time.
%Drilling engineers are able to interpret the situation on the rig and downhole from these sensor readings.
%However, staring at graphs for 12 hours straight is a tiring task, in addition to having the potential of being extremely boring.
%Also, keeping a mental model updated continuously is very hard work.
%The monitoring is done while making decisions about which actions to take at all times.
%Drilling to the target depth as fast as possible is desired, but it is highly important to avoid getting the drilling equipment stuck downhole, twisting the drill string off or in any other way damage the equipment and lose time. 
%
%\subsection{The ML Task: Symptom Classification}
%Problematic situations are often indicated by several lesser problems observed prior to the problematic situations.
%The smaller problems that indicate that larger problems are brewing are naturally associated with the distinction between symptoms and diagnoses.
%We define such smaller problems that leads to larger problems as symptoms.
%In drilling, unexplained drops in pressure can be symptoms of a hole being made in the drill string.
%If the hole is let alone to grow, the drill string will be twisted off and thus parted, leaving tools in the hole.
%Thus, pressure drops are symptoms of wash-outs, which can result in costly lost time.
%%An example of a symptom is erratic torque, which sometimes lead to tool failure. 
%Another example is a pressure spike, which may be caused by gravel packing off around the drill bit and thus can lead to the drill string being stuck in the hole.
%
%We limit our discussion to an MLP that classifies symptoms in real-time based on the sensor readings.
%However, even though the term pressure spike indicates that the symptom is only a sudden increase in pressure, this is not the case.
%In order for pressure spike to be a symptom of a larger problem, it has, for example, to happen while the mud flow parameter is stable during certain drilling activities.
%Thus, classifying symptoms based on multiple time series is not a simple task.
%
%%In drilling, unexplained drops in pressure can be symptoms of a hole being washed out in the drill string.
%%If the hole is let alone to grow, the drill string will be twisted off and thus parted, leaving tools in the hole.
%%Thus, pressure drops are symptoms of wash-outs, which can result in costly lost time.
%%Another example of a symptoms is a spike in the torque measurement.
%%Enduring excessive torque will 
%
%
%\subsection{The Data: Time Series}
%Oil well drilling data are time series data consisting of measurements that are performed  every second for time periods ranging from a week to several months.
%The data has the following characteristics:
%
%\begin{description}
%
%\item [High dimensonality:] 
%In typical oil well drilling operations around fourty parameters are measured every second for up to several months.
%
%\item [Complex interactions:] 
%Parameters interact. 
%Sometimes, but not always, a change in a parameter is related to the change of other parameters in the same time period.
%Other times, the change in a parameter is related to something that happened a long time ago.
%
%\item [Context dependent parameters:]
%Parameters change with the context.
%For example, pressure increases with the true vertical depth of a well, and hence a parameter reading that is perfectly normal at a given depth is catastrphical at another.
%
%\item [Imbalanced data:] 
%The data contains relatively few examples of problems, and thus the data consists mostly of true negatives, which means that the training data is highly imbalanced. 
%Given the task of classifying pressure spikes.
%Because of the imbalance in the data, a classifier that never classifes a pressure spike will be correct more than 99\% of the time.
%%A classifier that classifies whether there is a pressure spike or not in the data will 
%\end{description}
%
%These characteristics have implications for the impact of the MLP.
%
%
%%\subsection{DrillEdge}
%%
%%Verdande Technology launch a product (DrillEdge) in 2009, which is a software for real-time monitoring drilling data (ref). 
%%A market differentiating component of DrillEdge is a pattern recognition PR and a case based reasoning CBR system that continuously searches a case base for similar situations to the present. 
%%The system can be viewed as two stage process, where the pattern recognition stage involves searching for numerous of extraordinary events which happen during drilling. 
%%An example of this can be an erratic torque, which sometimes lead to tool failure. 
%%Another example is a pressure spike, which may be caused by increased hydraulic friction due to the well packing off. 
%%The second stage, or the cased based reasoning stage, involves using these automatically detected events and other static data such as which equipments are used, hole geometry and prognosed lithology. 
%%The cases include information about how they were retrieved, which explains how the previous situation is similar to the present situation.
%% Also, the cases contain a user story which involves a detailed situation description, how it was dealt with and advises on how it should have been dealt with.
%
%\subsection{Impact Analysis}
%
%The Development of the MLP in DrillEdge is guided by impact analysis. 
%In this analysis, the default approach of manual inspection of real-time data is compared to manual inspection of both real-time data and automatically detected symptoms.
%The next step is to identify which dimensions change based on the comparison and here the dimensions that need attention is dollars saved/used and increased awareness of the companies best practices. 
%
%%As said before, numerous of events are automatically detected in DrillEdge and it is outside the scope of this paper to give a proper overview of that. However, we will go into two examples of how impact analysis is used to guide the development. 
%
%%\subsubsection{Detection of Twist-Offs}
%\subsubsection{Classification of Pressure Drops}
%A twist-off can often be a result of mechanical wear of the drillstring, which eventually becomes a small hole. 
%Because there is a substantial pressure difference between the inside and the outside of the drillpipe, drilling fluid is flowing through at a high speed, which lead to erosion, hole enlargenment and eventually the twist-off. 
%However, this can be seen from the surface because of a pressure drop, as some of the flow is short-circuiting through the hole. 
%A twist-off causes a very expensive operation in order to fish up the lost part of the drill string. 
%The MLP is detecting pressure drops as a symptoms of a twist-off of the drill string. 
%Here, we \emph{compare} status quo to a new solution that involves an MLP that classifies pressure drops, and the \emph{relevant dimensions} are profit and improved awareness of best practices.
%The \emph{cost and gains} are listed in the table below:
%
%\vspace{1ex}
%\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|}
%\hline
%\multicolumn{2}{|c|}{Profit}\\
%\hline
%\multicolumn{1}{|c|}{Gains} & 
%\multicolumn{1}{|c|}{Costs} \\
%\hline
%Average saved costs, based on avoided accidents &
%License costs and other installation costs \\ 
%\hline
%&
%Extra labor costs involved with inspecting detected events \\ 
%\hline
%\end{tabular}
%\vspace{1ex}
%
%\vspace{1ex}
%\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|}
%\hline
%\multicolumn{2}{|c|}{Awareness of Companies Best Practices}\\
%\hline
%\multicolumn{1}{|c|}{Gains} & 
%\multicolumn{1}{|c|}{Costs} \\
%\hline
%Emergency warnings acompanied with best practices will make the crew act faster and more decisive in emergencies & \\
%\hline
%\end{tabular}
%\vspace{1ex}
%
%Avoiding one accident can cost between one and two million dollars, which is a high number compared to extra labor cost of checking false positives. 
%However, there is obviously a limit of how many false positives that can be tolerated. 
%From a user point of view, maybe once a week can be acceptable and maybe even desired from an emergency training point of view. 
%Wells take weeks to drill and most wells do not have a twist-off, so a precision (true positives / (true positives + false positives) of 10 percent might be acceptable as long as the recall is maximized. 
%Thus, the \emph{performance measures} are precision and recall with the suggested \emph{thresholds}.
%The MLP has a positive impact as long as it occationally prevents one more twist-off, and does not bind all resources.
%
%\subsubsection{Detection of bad sensor data}
%As oil well drilling rigs are situated in a rather harsh environment, sensors frequently fails and provides bad data. 
%This may be harmless under normal conditions, but in cases of emergencies it can be fatal to not have thrustworthy readings. 
%An MLP is classifying sensor failures and notifies the user when sensors need to be checked. 
%As above, we \emph{compare} no solution with an MLP, and the same \emph{dimensions} are considered.
%\emph{Cost and gain} tables for this impact analysis are listed below:
%
%\vspace{1ex}
%\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|}
%\hline
%\multicolumn{2}{|c|}{Profit}\\
%\hline
%\multicolumn{1}{|c|}{Gains} & 
%\multicolumn{1}{|c|}{Costs} \\
%\hline
%Average saved costs, based on avoided accidents &
%License costs and other installation costs \\ 
%\hline
%Labor saved because manual inspection work that is done automatically by the system &
%Extra labor costs involved with inspecting false positvies \\ 
%\hline
%\end{tabular}
%\vspace{1ex}
%
%\vspace{1ex}
%\begin{tabular}{|p{0.2\textwidth}|p{0.2\textwidth}|}
%\hline
%\multicolumn{2}{|c|}{Attention Noise}\\
%\hline
%\multicolumn{1}{|c|}{Gains} & 
%\multicolumn{1}{|c|}{Costs} \\
%\hline
%& False alarms take focus away from the important tasks \\ 
%\hline
%\end{tabular}
%\vspace{1ex}
%
%%In this case, the user normally inspects graphs and reports bad sensor readings. 
%If the system is accurate, more sensor issues are found.
%High degrees of false positives takes the attention of the users, and should thus be avoided. 
%Missing positives does not have a high severity.
%The suggested \emph{performance measure} is minimizing false positives, and the \emph{threshold} should be close to zero.
%
%\section{Discussion}
%\label{sec:discussion}
%Machine learning is a scientific activity, and therefore, experiments and evaluation are essential parts of the process \cite{Langley_1988}.
%We suggest that this also applies to the development of MLPs rather than to machine learning research alone.
%The argument is the following: 
%Given the task to develop an MLP using an off-the-shelf algorithm, one starts with a \emph{hypothesis} that the chosen algorithm will work.
%However, one cannot know before the hypothesis is evaluated by training and testing it on data.
%Thus, evaluation of the hypothesis is a core task in MLP development.
%In contrast, development of an editor or generic software lacks this scientific element.
%This difference in development methodology leads to awkwardness when developing MLPs, as usual requirement analysis is not enough to ensure that MLPs that matter are developed.
%Because of this we suggests to perform impact analysis as the first step of the development process.
%
%Our definition of impact has several advantages, where 2-4 are related to the shortcomings identified in the section \ref{sec:the_machine_learning_program_life_cycle}:
%
%\begin{enumerate}
%
%\item
%Impact is made quantiative, which makes the concept less fuzzy and enables impact analysis.
%
%
%\item
%The definition relates impact to the effect the MLP has on its real-world environment. 
%This ensures that impact analysis will focus on the environment that the MLP is situated in rather than the MLP's performance as measured on test data. 
%Thus, requirements and objetives can be related to the impact as measured along specific and relevant dimensions.
%
%\item
%By analysing impact, the most important dimension are revealed, and so are the gains and costs related to them.
%In this way, performance measures and thresholds that emphasize wanted effects can be chosen.
%
%\item
%The gains and costs can be analysed in order to decide when to improve an already deployed MLP rather than develop a new one.
%\end{enumerate}
%
%The definition of impact identifies several imporant aspects of what an impact analysis should entail.
%
%
%
%
%%VI BØR NEVNE HVORDAN DATA ENDRER SEG I OLJEDOMENE - BORE MED NYTT UTSTYR I CANADA VERSUS OFF-SHORE I NORDSJØEN.
%
%
%\section{Conclusion}
%\label{sec:conclusion}
%Reserchers have noted that the machine learning community lacks a real-world orientation and a focus on the impact that the machine learning should have.
%In this paper, we have presented a definition of impact and suggestions for how to perform impact analysis.
%We have also showed how impact analysis guides the development of an MLP deployed in the oil well drilling domain.
%

%Analyse av dimensjoner
%Har følger => Flere grupper domeneeksperter må kanskej til for å kunne vurdere impact!

%This has several consequences: 
%1) Impact cannot be judged in relation to the 
%
%
%Because of this fact, impact is tougher to use as a guide when developing a system.
%Furthermore, for many tasks a domain expert is required to say something about impact while accuracy can be analysed by the machine learning expert.
%
%So impact is related to some effect in the application domain of the system, but this effect can also be negative.
%Often when talking about impact, impact is used as something positive. 
%This is not neccesarily the case.
%The impact of a system can have a negative effect.
%
%Symptoms vs. diagnosis
%
%\subsection{The Data}
%%Ikke forklare boring, men data og målet (monitorering av boreoperasjoner)
%

%
%%Multiple time series => Snevre inn til
%
%\end{itemize}
%
%
%Problematic situations are often indicated by several lesser problems observed prior to the problematic situations.
%
%
%Solution:
%
%\begin{itemize}
%
%\item Divide problems into symptoms and problematic situations, which are comprised of several symptoms.
%
%\item Divide problematic situations into problem types
%
%\item Divide symoptoms into different categories.
%
%
%
%\item 
%
%\item
%
%
%\end{itemize}

\bibliographystyle{named}
\bibliography{ijcai13}


\end{document}

