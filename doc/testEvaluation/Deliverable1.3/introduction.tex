\section{Introduction}

Even though the number of algorithms designed for learning on streaming data is increasing, there is still not a unified and well accepted way for evaluating them.  This is because testing and evaluating algorithms that are designed to work on streaming data are generally more complicated than those designed to work on non streaming data.  There are both statistical and computational reasons for this.  

Static data is data, where each instance can be assumed to be identically and independently distributed i.i.d. 
If the data instances in a data stream are i.i.d., then the only new challenge compared to static data is that the data streams are open ended.  It can be seen as a static data that is always growing in size.  One way to deal with this is to take out a data set of fixed size, a holdout data set, and perform tests on this.  Another way is to let the performance measures be "accumulating" as the data set is growing.  

There are generally two important categories for performance measures on data streams, that have been proposed in the literature.   Some involve loss functions and some involve using the area under the receiver operating characteristics curve (AUC). 

Various performance measures involving loss functions on stream data have been proposed in the papers of Gama et. al. \cite{Gam09}, \cite{Gam09_2}, \cite{Gam13}.  The loss function on regression problems is a function of both the predicted value and the ground truth, while the loss function on classification problems is a function of predicted and real class labels.  The holdout error is basically the average loss on a holdout dataset of fixed size.  The predictive sequential, or \emph{prequential} error is defined as the average loss function up to time step $i$, where $i$ is the current time step. 

Stationary data is data that follow a stationary process, i.e. that the joint probability distribution of any time window does not change when shifted in time. Consequently, parameters such as the mean and variance, if they are present, also do not change over time and do not follow any trends.  It was suggested to use  \emph{prequential} error on stationary streams in \cite{Gam13}.  Moreover, in the case when data was slowly drifting, it was suggested to use the prequental error measures with forgetting mechanisms. The forgetting mechanisms involves either using a time window or fading factors.  In paper \cite{Gam13}, convergence towards the Bayes error was shown for all these performance measures provided that the learners are consistent and data is i.i.d.  Moreover, it was shown that in the case of drifting data, the prequental error measures with forgetting mechanisms were favorable.

The error measures in \cite{Gam13}, have in common that the loss is calculated at each time step. In the AMIDST project there are use case scenarios where this approach is relevant, but there are also use case scenarios where this approach need a generalization.  To be more specific, this report will describe a use case scenario where it is important to predict that certain event is happening somewhere in an interval prior to the event takes place.  This is not possible, by directly using the approach of \cite{Gam13}.  

It is also of great importance that all the use case scenarios outlined in this report are binary classification problems, where an output function is compared to a threshold.  The output function is typically the result of a prediction of a probability based  on a Bayesian network.  It is of high interest to discuss the output function, regardless of which threshold is set.  

The AUC measure provides such insight and is very popular for classification on highly imbalanced data. 

\todo{Sigve has printed out a number of papers that use AUC on streams and he needs to read them and give a short summary including references.} 

In common for all the performance measures that is outlined in this report is that the data streams are chopped up into a number of sub streams that are approximately i.i.d and labeled as either a positive or a negative.  To make this a bit more clear, a stream in this context is referred to as an ordered list of elements at each time step.  A sub stream is a stream on a fixed time window.

We have generally two approaches for generating the sub streams.  When the data is multiple i.i.d. streams, the i.i.d. condition of the sub streams is ensured by sampling at the exact same window.  As an example, the data for a client in a bank can be described by a data stream and all the clients are described by multiple streams.  The sub streams are all the clients observed over a fixed time window.

A second approach for generating the sub streams, is when data is essentially one stream, but we can assume that sub streams that are far apart from each other can be assumed to be i.i.d. It is important that enough space between sub streams are left so that dependencies are withdrawn.

In essence, the i.i.d. assumption of the sub streams allows one to investigate the output function independently from the prior distributions and the choice of loss functions.  This is done by computing AUC as will be defined later. 

Loss functions are also investigated to obtain a more practical understanding of the benefit of using the system compared to other models.   


%
%It is also important to note that the algorithms themselves are often designed to weight measurements that are close to the actual time step higher than measurements that are further back.  On locally dependent streams, we must therefore not only assume that data are generated from underlying distributions that are time dependent, but also that the algorithms themselves are time dependent.  

Computational challenges are related to the fact that the data come from an open-ended data stream, conceptually infinitely long, which imposes practical challenges related to restrictions on cpu-time and memory allocation.  

\todo{Elaborate more on computational challenges.}

In section \ref{sec:methodology}, AMIDST relevant methodologies for evaluation of both batch and streaming algorithms are identified and discussed.  This section forms the foundation of the next three sections, where the exact evaluation routines for each use case provider is given. These sections contains a description of the requirements related to evaluation as described in Delivery 1.2 and a short description of the algorithms and the data.  At the end of these sections, the methods for evaluating predictive and runtime performances are exposed and discussed.  Section \ref{sec:conclusion} concludes the report.

%
%The test and evaluation procedure includes specification of what metrics are relevant to use to quantify the ability of the AMIDST system, such as loss functions, maximum response-times, memory limits and output format.  The paper also includes considerations about quantitative improvements AMIDST should obtain over the state of the art.



%\quote{\emph{Task description: In this task we will establish formal procedures for testing and evaluating the
%    developed models and algorithms. This includes specification of maximum response-times, output format,
%    relevant formalization of loss functions, investigations into what metrics are relevant to use to quantify
%    the ability of the AMIDST system, and considerations about what quantitative improvements AMIDST should
%    obtain over state of the art.}}
%
%
%From Helge's slides at the WP 3 kickoff meeting:
%\begin{itemize}
%\item Massive datasets: find relevant techniques, ensure scalability, etc.
%\item Online evaluation of streams: find relevant techniques, ensure scalability, define behavior in changing environment, etc.
%\item Significance of results, e.g., considering changing environment vs. ``reproducability'', distribution for test-statistic, significance levels/sizes of test-sets, etc.
%\end{itemize}
