\section{Introduction}

Even though the number of algorithms designed for learning on streaming data is increasing, there is still not a unified and well accepted way for evaluating them.  This is because testing and evaluating algorithms that are designed to work on streaming data are generally more complicated than those designed to work on non streaming data.  There are both statistical and computational reasons for this.  

If the data instances in a data stream are identically and independently distributed i.i.d., then the only new challenge compared to static i.i.d. data is that the data streams are open ended.  It can be seen as a static i.i.d data that continuously grow in size.  Evaluation methods related to these streams are closely related to the evaluation methods that are known for static i.i.d. data.  

A generalization of i.i.d. data is stationary data, which is data that is generated from a stationary process. A stationary process is a stochastic process where the joint probability distribution over any sized time window do not change when shifted in time.  Consequently, parameters such as the mean and variance do not change over time.

Various performance measures on stationary data streams have been proposed in the literature.  Performance measures involving loss functions have been proposed in the papers of Gama et. al. \cite{Gam09}, \cite{Gam09_2}, \cite{Gam13}.  The loss function on regression problems is a function of both the predicted value and the ground truth, while the loss function on classification problems is a function of predicted and real class labels.  The holdout error is basically the average loss on a holdout dataset of fixed size.  The predictive sequential, or \emph{prequential} error is defined as the average loss function up to time step $i$, where $i$ is the current time step. 

In this paper, we focus the exposition on binary classification, although we are aware that much can be generalized to multi classification or regression.  Moreover, most classification rules in the Amidst software involves comparing an output function, or a score, to a threshold.  The output function in the Amidst software is usually a Bayesian network that predicts a probability of a class label.  In this case, the area
under the receiver operator characteristics curve (AUC) is an interesting alternative for stationary streams.

AUC is a popular method for evaluating classification problems where class imbalance is vital, because it is invariant to the class distribution. In the case of i.i.d. data, AUC has the statistical interpretation that it is the probability that a member of the "positive" class is scored higher than a member of the "negative" class.  AUC is therefore a measure of the \emph{ranking ability} of the output function.  This is particularly relevant if one wants to change the classification threshold as a consequence of changing class distributions or  changing misclassification costs \cite{Wu07}.  Moreover, it also pointed out that AUC is more preferable than accuracy for model evaluation in \cite{Gam10}.  

Specialized learning algorithms on imbalanced streams are proposed in \cite{Dit13, Hoe12, Lic10}, where these papers points out that this problem is particularly difficult and effective algorithms for evaluating such a classifier is vital.  In the papers \cite{Dit13, Lic10}, the area under the receiver operator characteristics curve (AUC) is calculated on limited holdout sets, while in \cite{Hoe12} AUC is calculated on the entire stream.

AUC calculation involves sorting all instances and iteration over the sorted list.  In a streaming context, this means that the calculation is $O(n)$ where $n$ is the length of the data stream.  If the hole stream is used, it may have computational problems related to memory and cpu time.  

However, most streams are not stationary.  Problems that involve \emph{consept-drift} are problems when both data are drifting and the learners are changing over time.  In \cite{Gam13}, it was suggested to use prequental loss with forgetting mechanisms to compensate for concept drift. The forgetting mechanisms involves either using a time window or fading factors.  In paper \cite{Gam13}, convergence towards the Bayes error is shown for all these performance measures provided that the learners are consistent, loss is zero-one and data is i.i.d.  Moreover, it is shown that in the case of concept-drift, the prequental error measures with forgetting mechanisms are favorable over those without a forgetting mechanism.

In \cite{Brz14}, the prequential AUC with a forgetting mechanism is proposed.  On one side this measure allows for concept drift and on the other side it solve the computational problems in \cite{Dit13, Hoe12, Lic10} as pointed out above.

In the AMIDST project there are use case scenarios where the data is multiple i.i.d. streams.  These use case scenarios are dealt with by dividing these streams into sub streams over a particular time window, where a evaluation time is in between the start time and the end time.  The data from the start time to the evaluation time is basically the explanatory variables and the data at the end time is basically the class label.  This problem reduces to a problem of non streaming nature, and the evaluation is not taking concept-drift into account.  However, it is worth noting that this problem can be expanded in the sense of prequental loss as of \cite{Gam13} or prequental AUC as of \cite{Brz14}.  

Moreover, there are also use case scenarios in the Amidst project that do not directly fit the prequental performance measures as of \cite{Gam13} and \cite{Brz14}.  All of these measures involve calculating a loss or a score at each time step.  In this report, we will describe a use case scenario where it is important to predict that certain event is happening somewhere inside an \emph{interval} prior to the event actually takes place.  

This problem has been solved by chopping the streams into a number of sub streams that are approximately i.i.d and labeled as either a positive or a negative.  It is important that enough space between sub streams are left so that dependencies are withdrawn.  In essence, the i.i.d. assumption of the sub streams allows one to investigate the output function independently from the prior distributions and the choice of loss functions.  

%To make this a bit more clear, a stream in this context is referred to as an ordered list of elements at each time step.  A sub %stream is a stream on a fixed time window.

%We have generally two approaches for generating the sub streams.  When the data is multiple i.i.d. streams, the i.i.d. condition of the sub streams is ensured by sampling at the exact same window.  As an example, the data for a client in a bank can be described by a data stream and all the clients are described by multiple streams.  The sub streams are all the clients observed over a fixed time window.
%
%A second approach for generating the sub streams, is when data is essentially one stream, but we can assume that sub streams that are far apart from each other can be assumed to be i.i.d. 


%
%It is also important to note that the algorithms themselves are often designed to weight measurements that are close to the actual time step higher than measurements that are further back.  On locally dependent streams, we must therefore not only assume that data are generated from underlying distributions that are time dependent, but also that the algorithms themselves are time dependent.  
%
%Computational challenges are related to the fact that the data come from an open-ended data stream, conceptually infinitely long, which imposes practical challenges related to restrictions on cpu-time and memory allocation.  
%
%\todo{Elaborate more on computational challenges.}

In section \ref{sec:methodology}, AMIDST relevant methodologies for evaluation of both batch and streaming algorithms are identified and discussed.  This section forms the foundation of the next three sections, where the exact evaluation routines for each use case provider is given. These sections contains a description of the requirements related to evaluation as described in Delivery 1.2 and a short description of the algorithms and the data.  At the end of these sections, the methods for evaluating predictive and runtime performances are exposed and discussed.  Section \ref{sec:conclusion} concludes the report.

%
%The test and evaluation procedure includes specification of what metrics are relevant to use to quantify the ability of the AMIDST system, such as loss functions, maximum response-times, memory limits and output format.  The paper also includes considerations about quantitative improvements AMIDST should obtain over the state of the art.



%\quote{\emph{Task description: In this task we will establish formal procedures for testing and evaluating the
%    developed models and algorithms. This includes specification of maximum response-times, output format,
%    relevant formalization of loss functions, investigations into what metrics are relevant to use to quantify
%    the ability of the AMIDST system, and considerations about what quantitative improvements AMIDST should
%    obtain over state of the art.}}
%
%
%From Helge's slides at the WP 3 kickoff meeting:
%\begin{itemize}
%\item Massive datasets: find relevant techniques, ensure scalability, etc.
%\item Online evaluation of streams: find relevant techniques, ensure scalability, define behavior in changing environment, etc.
%\item Significance of results, e.g., considering changing environment vs. ``reproducability'', distribution for test-statistic, significance levels/sizes of test-sets, etc.
%\end{itemize}
