% !TEX root = deliverable1.3.tex

\section{Verdande Technology: Test and evaluation}



\subsection{Use-case requirements}

%The test and evaluation procedures for Verdande Technology will be developed along the lines introduced in Deliverable 2.1: Instead of testing each use case separately, we utilize the notion of {\em application scenarios}. 
%An application scenario is defined by a sequence of use cases that combined constitutes a full interaction procedure leading to a verifiable result. In D2.1 we defined two scenarios:
In Deliverable 2.1 the following application scenarios for Verdande Technology (VT):
\bde
\item[VER1:  Detection of drill string vibrations] covering these use cases:
\begin{itemize}
\item UC1: Parsing data sets
\item UC2: Erratic torque detection (model construction)
\item UC5: Model application
\item UC6: Result generation
\end{itemize}

\item[VER2: Semi-automatic labelling] covering these use cases:
\begin{itemize}
\item UC1: Parsing data sets
\item UC3: Semi-automatic labelling (model construction)
\item UC5: Model application
\item UC6: Result generation
\end{itemize}

\item[VER3: Automatic formation detection] covering these use cases:
\begin{itemize}
\item UC1: Parsing data sets
\item UC4: Formation detection (model construction)
\item UC5: Model application
\item UC6: Result generation
\end{itemize}

\ede
Requirements for the different use-cases were defined in Deliverable 1.2. Those that can be evaluated quantitatively are repeated in \tabref{verdande:requirements} for completeness.
We summarize the  requirements for each application scenario as follows:
\bde
\item[VER1:] The evaluation process for this application scenario was not quantified, but postponed to this document, as requirement VER.U2.D2 states that \textit{``[\ldots]Formalisation of the test procedure will be developed in task 1.2''}.
\item[VER2:] The result of the system is to be compared to the prior rates and the results of tagging performed by an inexperienced driller (requirements VER.U3.D2, VER.U3.D3, VER.U3.D4 and VER.U2.D5).
\item[VER3:] Quality is estimated as distance from the estimated formation shifts to the ground truth, and improvement over the a priori formation chart is required (VER.U4.D2 and VER.U4.D3).
\ede

\begin{table}[ht]
\scalebox{.89}{

\begin{tabular}{|p{18mm}|p{17mm}|p{98mm}|p{12mm}|}\hline
ID & Sub-phase & Description & Task(s)\\ \hline\hline
VER.U1.O1 &	Develop.	&Data import should take less than 10 minutes  on a single desktop computer or equivalent	&	7.1\\ \hline
%VER.U2.D2 & 	Testing	& Formalization of the test procedure will be developed in task 1.2. & 1,2 /\, 7.2\\ \hline
%VER.U2.D3 &	Testing	& The accuracy of the result and the discussion should be published in a journal or a conference.  & 7.2 /\, 9.3 /\, 9.4\\\hline
VER.U3.D2 & 	Testing	& The rate of missed abnormalities must be no larger than the prior probability of abnormality.  &  7.2\\ \hline
VER.U3.D3 &	Testing	&The rate of missed normal states must be no larger than the prior probability of normality.  & 7.2\\\hline
VER.U3.D4 & 	Testing	& The rate of missed abnormalities could be below what is obtained by an inexperienced drilling engineer or a software program at that level. & 7.2\\ \hline
VER.U3.D5 &	Testing	& The rate of missed normalities could be below what is obtained by an inexperienced drilling engineer or a software program at that level.  & 7.2\\\hline
VER.U4.D2 & 	Testing	& MD\_algorithm must be less than MD\_prior (defined in Use Case Scenario 3 in Delivery 1.2), on at least 1 randomly chosen drilling log.  &  7.3\\ \hline
VER.U4.D3 &	Testing	&MD\_algorithm should be 10 percent less than MD\_prior (defined in Use Case Scenario 3 in Delivery 1.2) on at least 3 randomly chosen drilling logs.  & 7.3\\\hline
VER.U5.D4 &	Testing	& Agents need to run fast enough for real-time implementation. In practice, this means that once probability tables are calculated  (post learning), the agent once provided a single new data point must be able to calculate probabilities, values or classifications and returning these in under 1 second on a single desktop computer or equivalent hardware  & 3.3,  7.3
\\
\hline \hline\end{tabular}

}
\caption{Testable requirements for VT's use-cases. }\label{tab:verdande:requirements}
\end{table}


\subsection{Model and data characteristics}
In order to make this document self contained, a summary of  VT's three application  scenarios are given below:

\bde
\item[Detection of drill string vibrations:] This task aims to better diagnose the shape of the wellbore and the
state of the equipment. Currently,  VT's solution is a simple algorithm
based on comparing root mean squared differences in a time window between the
measured torque and a smoothed version of the torque, thereby measuring the local amplitude variations. 
This approach is rather limited and does not inherently take the
existing uncertainty in the streaming data  into account. The main objective for scenario VER1
is to design a probabilistic graphical model for erratic torque monitoring and
detection of abnormal situations.

\item[Semi-automatic labelling:] 
% The main contribution from DrillEdge [28] during
% operation is that it enables the driller to see in real time if the current situation
% is \emph{ similar} to previous situations that lead to undesired circumstances. If this
% is the case, remedial actions can be found to avoid undesired events. At the
% core of the analysis are historical examples of noteworthy episodes, ranging from
% clearly undesired events, for instance getting stuck, to more subtle indications of
% evolving problems, such as abnormal readings of hook-load. 
VT's solution uses supervised learning for detecting undesired events, and therefore requires \textit{labelled} datasets where undesired events or episodes are identified. 
VT already have extensive datasets labelled in this way, 
but manually labelling new data sequences, or adapting the existing labelling to
new definitions, can be a very time-consuming activity. 
VT is therefore interested in looking into techniques for semi-automatic labelling. 
Given unlabelled data streams collected over time from typical drilling conditions,
this application scenario aims to compute a normality score for each 
drilling situation, then label it as either \emph{normal} or \emph{abnormal}. As for the
previous task, a probabilistic graphical model will be designed, taking into account
the temporal dynamics of the drilling process and continuously adapting to changes
in the incoming streaming data.

\item[Automatic formation detection:]
This scenario aims to predict in real time the formation tops from the MWD 
(measurements while drilling) data using a probabilistic graphical model. Once again, this should be performed taking the temporal
dynamics of the drilling process into account. The automatic formation detection
is vital for dealing with several issues such as hole instability and vibrations, and
also important for reducing the costs and the overall non-productive time. 
%To VT's knowledge, this has never been done before, neither as an
%academic exercise nor as in a product.
\ede




\subsubsection{The data generation process} \label{sec:VT:Dataset:1}


The datasets available from VT are very complex, and contain a high number of variables (a  number that varies between the different logs). 
Many of the variables  will not be used by  the application scenarios developed in AMIDST, and feature selection both based on expert knowledge and using data-driven methods will be required. 
Another complicating factor is that mnemonics and units are generally not consistent between logs.  
In order to simplify these issues, all the drilling logs are preprocessed before given to the AMIDST software, resulting in two separate datasets:  
The first data-set is used by application scenarios VER1 and VER2, while the second is used by application scenario VER3.

\subsubsection{Dataset one}
This data set consists of 100 drilling logs of various sizes; typically one log covers one drilling section (that is  a few weeks of data). The data is cleaned and contains a fixed number of variables defined using  common mnemonics and SI units. Each log is represented in two XML-files, one that is  \textit{holistic}, the other  \textit{drilling-focused}. The first file contains a stream of data, with new observations coming with an update frequency between $0.1$Hz and $1$Hz. In addition to the actual drilling data, the time-indexed XML-file is amended with three calculated fields:
\bde
\item [Torque vibration index:] This index indicates whether or not there was abnormal torque at the time the data was captured. The index is not available in every log. This variable is central for the application scenario VER1.
\item [Normality index:] This index  is related to the application scenario VER2. It is generated automatically by VT's DrillEdge as follows:  A point in time is defined as normal if there are no cases on the case radar, no events have fired in a 10 minutes time-window (looking to the past \textit{and} to the future),  and no events have fired in a depth window of 5 meters (upwards and downwards).
\item [Depth-log indicator:] This indicator points out which time stamps are used for drilling. As such it is used to link to the elements in the second XML file. 
\ede

The second XML file is made by simply filtering out the drilling periods from the first XML-file (using the the depth log indicator). This data-set is \textit{depth-indexed}, as the drilling activity is defined through the increasing depth of the wellbore.\footnote{Some care must be taken when depth is adjusted manually. This is also performed during preprocessing of the data, and is not part of the AMIDST solution.}




%
%The subset includes ONLY:
%
%\begin{itemize}
%\item Time stamp (for event export)
%\item Time stamp in seconds since the operation started (verdande calculated)
%\item Depth of bit (DBTM)
%\item Hole depth (DMEA)
%\item Block position (BPOS)
%\item Rate of penetration (ROP) (verdande calculated)
%\item Hook load (HKL)
%\item Weight on bit (WOB) (verdande calculated if exist)
%\item Mud flow in (MFI) (not verdande calculated)
%\item Stand pipe pressure (SPP)
%\item Rotations per minute (RPM)
%\item Torque TRQ (not verdande calculated)
%\item Activity code (verdande calculated) 
%\item Depth log indicator (no, yes) (verdande calculated)
%\item Normality index (normal, maybe abnormal) (verdande calculated)
%\item Torque vibration index (no, yes, NA) (verdande calculated from tags)
%\item True vertical depth (TVD)
%\end{itemize}
%
%Moreover, these static properties exist for each log:
%\begin{itemize}
%\item Bit size
%\item Mud motor (yes/no)
%\item Tagalog ID (Well name etc)
%\end{itemize}


%
%The depth logs includes ONLY:
%
%\begin{itemize}
%\item Time stamp (for event export)
%\item Time stamp in seconds since the operation started (verdande calculated)
%\item Block position (BPOS)
%\item Rate of penetration (ROP) (verdande calculated)
%\item Hook load (HKL)
%\item Weight on bit (WOB) (verdande calculated if exist)
%\item Mud flow in (MFI) (not verdande calculated)
%\item Stand pipe pressure (SPP)
%\item Rotations per minute (RPM)
%\item Torque TRQ (not verdande calculated)
%\item Normality index (normal, maybe abnormal) (verdande calculated)
%\item Torque vibration index (no, yes, NA) (verdande calculated from tags)
%\item True vertical depth (TVD)
%\end{itemize}

\subsubsection{Dataset two} \label{sec:VT:Dataset:2}

The second dataset is related to  application scenario VER3. The data originates from OMV Aktiengesellschaf, but VT has unlimited access to the data also for use in the AMIDST project.  
This data  is an extension of the logs in the previous dataset,  as it contains down hole measurements such as gamma ray and resistivity.  Moreover, lithology charts from the planning phase (i.e., before drilling) and after the well is drilled are also included.
 
 \vekk{

For each data log two XML files have been generated. The first XML file contains only a subset of graphs with common mnemonics and SI units on a time scale. The XML files contains a fixed number of variables , where the resolution of the logs are below ten seconds between updates.

%The subset includes ONLY:
%
%\begin{itemize}
%\item Time stamp (for event export)
%\item Time stamp in seconds since the operation started (verdande calculated)
%\item Depth of bit (DBTM)
%\item Hole depth (DMEA)
%\item Block position (BPOS)
%\item Rate of penetration (ROP) (verdande calculated)
%\item Hook load (HKL)
%\item Weight on bit (WOB) (verdande calculated if exist)
%\item Mud flow in (MFI) (not verdande calculated)
%\item Stand pipe pressure (SPP)
%\item Rotations per minute (RPM)
%\item Torque TRQ (not verdande calculated)
%\item Activity code (verdande calculated) 
%\item Depth log indicator (no, yes) (verdande calculated)
%\item Normality index (normal, maybe abnormal) (verdande calculated)
%\item Torque vibration index (no, yes, NA) (verdande calculated from tags)
%\item True vertical depth (TVD)
%\item Gamma Ray (GAM)
%\item Resistivity (RES)
%\end{itemize}
%
%Moreover, these static properties exist for each log:
%\begin{itemize}
%\item Bit size
%\item Mud motor (yes/no)
%\item Tagalog ID (Well name etc)
%\item Distance from Gamma sensor to bit
%\item Distance from Resistivity sensor to bit
%\end{itemize}


The second XML file contains a subset of graphs with common mnemonics and SI units on depth scale. This subset is using the activity code (that is when we are drilling) to produce a set of depth logs as discussed above. The XML files contains a fixed number of variables, which are indexed on the depth they were drilled.


%
%The depth logs includes ONLY:
%
%\begin{itemize}
%\item Time stamp (for event export)
%\item Time stamp in seconds since the operation started (verdande calculated)
%\item Block position (BPOS)
%\item Rate of penetration (ROP) (verdande calculated)
%\item Hook load (HKL)
%\item Weight on bit (WOB) (verdande calculated if exist)
%\item Mud flow in (MFI) (not verdande calculated)
%\item Stand pipe pressure (SPP)
%\item Rotations per minute (RPM)
%\item Torque TRQ (not verdande calculated)
%\item Normality index (normal, maybe abnormal) (verdande calculated)
%\item Torque vibration index (no, yes, NA) (verdande calculated from tags)
%\item True vertical depth (TVD)
%\item Gamma Ray (GAM)
%\item Resistivity (RES)
%\item Lithology from planning (LITPLAN)
%\item Lithology afterwards (LITAFTER)
%\end{itemize}

}




\subsection{Predictive performance: test and evaluation}

\subsubsection{Application scenario 1}

The goal of the first application scenario is to detect abnormal torque states.  The output of the AMIDST system is a probability of abnormal torque at each time step, and during evaluation these numbers will be compared to the torque vibration index from the logs that are tagged with this information.  We note that even if domain experts have supplied the labelling of the data (that is, they have marked the parts of the time-series that are abnormal), the classifications are not well defined:  Two different experts may disagree both whether or not a time-period actually has abnormal torque, and -- if they agree that the torque has been abnormal -- they may still disagree regarding the start and end points of the erratic period.  

Bursts of abnormal torque typically lasts for a period of from two to ten minutes, but we note that detecting torque vibrations \textit{early} is generally not important.  
The driller feels the vibrations himself, and is need not be told what to him is obvious by a software system.  
The value in automatically detecting the vibrations is rather to be able to diagnose how much cumulative damage  the equipment has been subject to, and thereby predict  wear and potential equipment failure. 
The evaluation can therefore be seen as inspecting a series of classifications (one at each time-point), where each can be seen in isolation.
In other words, it is not important to evaluate  the streaming fashion of the classification for this application scenario. 
We have therefore decided to use  the AUC as the evaluation metric, only taking whether or not the tag is detected at each point in time  into account.  

There are two reasons why it is not possible to determine a quantitative requirement for the AUC  before further examining the data: 
$i)$ the effect of the before-mentioned ambiguities in the expert-provided tagging has not yet been quantified, and 
$ii)$ it is  unknown how well-defined the start and end points of each period of abnormal torque are in the data. 
We will therefore proceed in the following manner:
\ben
\item Logs containing the torque-information will be separated into training and test-set using a documented procedure (including a fixed random seed).
\item 10 logs that have already been tagged by a domain expert will be re-tagged by a different expert. The quality of the  domain expert's tags will be calculated using AUC.
\item All logs in the test-set will be tagged by AMIDST, and the AUC for each log is calculated.
\item The poorest results from AMIDST (in terms of AUC) should be be at least as good as the poorest result from the expert. Similarly,  the mean AUC result obtained by  AMIDST must be at least as good as the mean result obtained by the domain expert.
\een











.  



\subsubsection{Application scenario 2}

The goal of application scenario VER2 is to detect ``abnormal states''.  The output from AMIDST is  a normality-index  at each time step, which is to be combined into intervals of time where the system is seen as ``abnormal''. 
The length of the time intervals with an abnormal state will typically be in the range of hours. 
We note that the tags used for learning are clearly not well-defined (see \secref{VT:Dataset:1}); they are automatically generated using VT's existing software system and not evaluated by a domain expert.  
The supplied tags are nevertheless taken to be the ground truth during the evaluation of this application scenario, both because having a domain expert verify the tagging on all 100 logs would be very time consuming and because this application-scenario, as argued in Deliverable 7.1, is designed to be purely data-driven. 

The requirements for testing this scenario (Requirement D2, VER.U3.D3, VER.U3.D4, and VER.U3.D5) all point towards calculating false negatives and false positives, and the results of the AMIDST system should improve both random labelling and the output of an inexperienced  drilling engineer. 
Note that it is the \textit{detection} of an event that is highlighted, and the AMIDST system is not given extra credit for being able to locate an abnormal event very precisely in time. 
In order to meet these requirements it is important to define  what false positives and false negatives are.  
A positive tag (existence of a tag) is determined by looking through the log and compare  the calculated probability for that tag to a pre-set threshold. 
If the probability is larger than the threshold, the event is flagged. It is a true positive if that time-point is inside a labelling of the same event in the gold-standard labelling, and  a false positive otherwise. 
Similarly, events that are not detected are false negatives. 

%In practice we will randomly generate a set of negative tags with length approximately equal to the time windows of positive tags, making sure that the negative tags are taken far enough away from the the positive tags so that only negligible dependency is present. When comparing to the inexperienced drilling engineer, he  is instructed to tag regions which he believes are abnormal looking at three randomly selected logs. 

We note that the above measures are global and can be estimated after the logs have been processed.  The performance measures do not take concept drift  into account, even though this is clearly present in the data.  
A drilling operation will almost always start out as normal, but as the open hole gets longer, more unstable and more dirty, the probability of abnormality increases. Furthermore, the geology as such varies with the depth of the wellbore, and  often gets more challenging (e.g., due to higher pressures and temperatures).
Finally, equipment, which may have been well maintained at the  start of the operation, will degrade during drilling.  
It is therefore also of interest to compute the prequential AUC with a forgetting factor (see \equref{precAUCforget}) and plot this versus time.  
A proper discussion of this is believed to be of interest, and should be supplied for at least five logs.





\subsubsection{Application scenario 3}

In this application scenario it is of interest to both find the formation tops and correctly label them.  
The input to the learning is the prior knowledge in form of a lithology chart from the planning of the well as well as the drilling-related data captured during operation  (see the description of Dataset 2 in \secref{VT:Dataset:2}). 

From Delivery 1.2 we have  defined two important quantities:  MD\_prior is the mean deviation between the expert identified formation tops before drilling and after drilling, and  MD\_algorithm is the mean deviation between the formation tops that are detected by the algorithm and the formation tops that are identified by the experts after the well is drilled. 
Now, it is required that MD\_algorithm must be less than MD\_prior on  randomly chosen log (Requirement VER.U4.D2), and also that MD\_algorithm should be 10 percent less than MD\_prior when evaluated on on at least 3 randomly chosen  logs (Requirement VER.U4.D3). 

If we can assume that all formation shifts that are present in the true lithology is also found in lithology chart from the planning phase (albeit, maybe not always indicated at the correct depths), evaluation is straight forward. 
However, if this assumption is not met, a more complex evaluation function than outlined in the requirement must be employed to penalize formation changes that are erroneously added to or removed from the estimated lithology.

\todo{Sigve says:  In general, I do not have the answer for this.  Helge suggested looking at sequence alignment on wiki. I tried, but did not figure it out. Will think more.  A simple option would be to assume that sequences from planning are the same as the ground truth, by some manipulation of the data.}


\subsection{Runtime performance: test and evaluation}

The requirements related to runtime performance are
\bde
\item[VER.U1.O1:] Data import should take less than 10 minutes  on a single desktop computer or equivalent.
\item[VER.U5.D4:] Agents need to run fast enough for real-time implementation. In practice, this means that once probability tables are calculated  (after learning), the agent once provided a single new data point must be able to calculate probabilities, values or classifications and returning these in under 1 second on a single desktop computer or equivalent hardware.
\ede

The requirements are considered to be fairly straightforward to accomplish, but  will nevertheless be rigorously tested. 
The  requirements are considered fulfilled if the test is passed at least $99\%$ of the time.


%
%\subsubsection{Application scenario 1}
%
%\subsubsection{Application scenario 2}
%
%\subsubsection{Application scenario 3}










